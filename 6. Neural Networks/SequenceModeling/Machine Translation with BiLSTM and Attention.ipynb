{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs is a popular approach for machine translation, and is adopted by various applications including Google Translate. This project seeks to perform a basic machine translation task using an encoder-decoder-based seq2seq model combined with an attention mechanism. The encoder is a Bidirectional LSTM and the decoder is a simple LSTM. The attention is an additive attention mechanism, which uses attention weights ($\\alpha$) and parameterizes the context vector using feed-forward network layers. This mechanism is proposed by Bahdanau et al., in their paper [*Neural Machine Translation By Jointly Learning to Align and Translate*](https://arxiv.org/pdf/1409.0473.pdf) published in 2015.\n",
    "\n",
    "The model is trained on multiple data sources. The word feature vector data is retrieved from [*nlp.standford.edu.*](https://nlp.stanford.edu/data/glove.6B.zip) and is approximately 350MB in size. The data of English-Italian tab-delimited sentence pairs, which is originally collected for the Tatoeba Project, is retrieved from [manythings.org](http://www.manythings.org/anki/), and is approximately 50MB in size. The training time is within 2 hours (for 35 epochs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import basic libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import gdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set basic parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 128\n",
    "EPOCH = 35\n",
    "LATENT_DIM = 500\n",
    "LATENT_DIM_DECODER = LATENT_DIM\n",
    "SAMPLES = 21000  # use only these many lines from the datafile.\n",
    "MAX_WORD_NUM = SAMPLES\n",
    "MAX_SEQ_LEN = 100\n",
    "EMBEDDING = 100  # this should align with Glove embedding size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store data from textfile to usable arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data file for English to Italian translation\n",
    "DATAPATH = gdown.download(url=\"https://drive.google.com/file/d/16Qz4UOhyHZeq5NZBVwn3uV51xqT1XbJZ/view?usp=sharing\", fuzzy=True, output=\"Italian.txt\")\n",
    "# if you need the file for English to French translation\n",
    "#DATAPATH = gdown.download(url=\"https://drive.google.com/file/d/16h_8WHOp2mFCyIqbjNIZ5omayG-P42wU/view?usp=drive_link\", fuzzy=True, output=\"fra.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = []\n",
    "man = []\n",
    "man_inputs = []\n",
    "count = 0\n",
    "\n",
    "# preprocess the translation file\n",
    "for line in open(DATAPATH):\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "    \n",
    "    count += 1\n",
    "    if (count > SAMPLES):\n",
    "        break\n",
    "    \n",
    "    # split original and translation into lists\n",
    "    e, m, _ = line.rstrip().split('\\t')\n",
    "    eng.append(e)\n",
    "    man.append(m + ' <eos>')\n",
    "    man_inputs.append('<sos> ' + m)\n",
    "    \n",
    "    if (count % 5000 == 0):\n",
    "        print ('Sample Count: {}.'.format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the input and output sentences, and create maps that can be used by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# tokenize input and generate idx map\n",
    "tok_in = Tokenizer(num_words=MAX_WORD_NUM)\n",
    "tok_in.fit_on_texts(eng)\n",
    "eng_seq = tok_in.texts_to_sequences(eng)\n",
    "word2idx_in = tok_in.word_index\n",
    "max_in_len = max(len(s) for s in eng_seq)\n",
    "\n",
    "print(\"Number of input tokens: {}\".format(len(word2idx_in)))\n",
    "print(\"Maximum input sequence length: {}\".format(max_in_len))\n",
    "\n",
    "# tokenize output and generate idx map\n",
    "tok_out = Tokenizer(num_words=MAX_WORD_NUM, filters='')\n",
    "tok_out.fit_on_texts(man + man_inputs)\n",
    "man_seq = tok_out.texts_to_sequences(man)\n",
    "man_seq_inputs = tok_out.texts_to_sequences(man_inputs)\n",
    "word2idx_out = tok_out.word_index\n",
    "max_out_len = max(len(s) for s in man_seq)\n",
    "out_word_num = len(word2idx_out) + 1\n",
    "\n",
    "print(\"Number of output tokens: {}\".format(len(word2idx_out)))\n",
    "print(\"Maximum output sequence length: {}\".format(max_out_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad the input and output sequences to be the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "eng_seq_padded = pad_sequences(eng_seq, maxlen=max_in_len)\n",
    "man_seq_padded = pad_sequences(man_seq, maxlen=max_out_len, padding='post')\n",
    "man_seq_inputs_padded = pad_sequences(man_seq_inputs, maxlen=max_out_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in word vectors and use them to create word embeddings. [The dataset](https://nlp.stanford.edu/data/glove.6B.zip) of the word vectors is downloaded from *nlp.stanford.edu*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's download the Glove embeddings.\n",
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVec = {}\n",
    "\n",
    "print('Loading wordVec')\n",
    "\n",
    "# load in word vectors in a dict\n",
    "with open('glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        data = line.split()\n",
    "        word = data[0]\n",
    "        vec = np.asarray(data[1:], dtype='float32')\n",
    "        wordVec[word] = vec\n",
    "\n",
    "print('Finished loading wordVec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordNum = min(MAX_WORD_NUM, len(word2idx_in) + 1)\n",
    "word_embedding = np.zeros((wordNum, EMBEDDING))\n",
    "\n",
    "# create word embedding by fetching each word vector\n",
    "for tok, idx in word2idx_in.items():\n",
    "    if idx < MAX_WORD_NUM:\n",
    "        word_vector = wordVec.get(tok)\n",
    "        if word_vector is not None:\n",
    "            word_embedding[idx] = word_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create translated target matrix by loading the padded output target sequence using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_target_one_hot = np.zeros((len(eng), max_out_len, out_word_num), dtype='float32')\n",
    "\n",
    "for idx, tokVec in enumerate(man_seq_padded):\n",
    "    for tok_idx, tok in enumerate(tokVec):\n",
    "        if (tok > 0):\n",
    "            man_target_one_hot[idx, tok_idx, tok] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the encoder and decoder before attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Input, LSTM, GRU, Dense, Bidirectional, RepeatVector, Concatenate, Activation, Dot, Lambda\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Embedding\n",
    "embedding = Embedding(wordNum, EMBEDDING, weights=[word_embedding]) #, input_length=max_in_len)\n",
    "\n",
    "# Encoder\n",
    "input_layer_encoder = Input(shape=(max_in_len,))\n",
    "embed_encoder = embedding(input_layer_encoder)\n",
    "encoder = Bidirectional(LSTM(LATENT_DIM, return_sequences=True, dropout=0.2))\n",
    "encoder_out = encoder(embed_encoder)\n",
    "\n",
    "# Decoder input\n",
    "input_layer_decoder = Input(shape=(max_out_len,))\n",
    "embed_decoder = Embedding(out_word_num, EMBEDDING)\n",
    "decoder_input = embed_decoder(input_layer_decoder)\n",
    "\n",
    "# Decoder output, after attention\n",
    "decoder = LSTM(LATENT_DIM_DECODER, return_state=True)\n",
    "dense_decode = Dense(out_word_num, activation='softmax')\n",
    "s0 = Input(shape=(LATENT_DIM_DECODER,))\n",
    "c0 = Input(shape=(LATENT_DIM_DECODER,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Attention. \n",
    "\n",
    "In each attention iteration, the hidden states, encoded by the encoder, is concatenated with the predicted output of the previous word. During training and using teacher forcing, this predicted output is replaced by the groundtruth translation. The concatenated result is subsequently passed through dense layers (a feedfoward NN) to form the alpha attention weights. The weights are then used to compute the context vector by performing dot product: $context=\\sum_{t'=1}^{|T_{x}|} \\alpha(t')h(t')$. Eventually, the context vector is passed through a \"softmax over time\" layer to become the input to the decoder LSTM. The \"softmax over time\" activation function is implemented as follows: \n",
    "$\\alpha(t') = \\dfrac {exp(s(t'))}{\\sum_{x=1}^{|T_{x}|} exp(s(x)) }$. \n",
    "\n",
    "<img src=\"./attention.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of x is N x T x D.\n",
    "def softmax(x):\n",
    "    assert(K.ndim(x) > 2)\n",
    "    e = K.exp(x - K.max(x, axis=1, keepdims=True))\n",
    "    s = K.sum(e, axis=1, keepdims=True)\n",
    "    return e / s\n",
    "\n",
    "# Some of the common layers for attention\n",
    "repeat_attn = RepeatVector(max_in_len)\n",
    "concat_attn = Concatenate(axis=-1)\n",
    "dense1_attn = Dense(10, activation='tanh')  # over time dimension T\n",
    "dense2_attn = Dense(1, activation=softmax)\n",
    "dot_attn = Dot(axes=1)                      # over time dimension T\n",
    "\n",
    "def iterAttn(h, prevOut):\n",
    "    \"\"\"\n",
    "    h: encoder encoded hidden states at all time.\n",
    "    prevOut: output at the previous time (word).\n",
    "    An iteration of attention.\n",
    "    \"\"\"\n",
    "    prevOutRepeat = repeat_attn(prevOut) # Tx, LATENT_DIM_DECODE\n",
    "    total = concat_attn([h, prevOutRepeat]) # Tx, LATENT_DIM_DECODE + LATENT_DIM * 2\n",
    "    d = dense1_attn(total)\n",
    "    alphaLayer = dense2_attn(d)\n",
    "    context = dot_attn([alphaLayer, h])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute encoder-decoder and attention (with teacher forcing) inference for $Ty$ times, to get a list of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = s0\n",
    "c = c0\n",
    "\n",
    "# Iterate attention Ty times\n",
    "all_out = []\n",
    "for t in range(max_out_len):\n",
    "    # Get context vector with encoder and attention\n",
    "    context = iterAttn(encoder_out, s) \n",
    "    \n",
    "    # For teacher forcing, get the previous word\n",
    "    select_layer = Lambda(lambda x: x[:, t:t+1])\n",
    "    prevWord = select_layer(decoder_input)\n",
    "    \n",
    "    # Concat context and previous word as decoder input\n",
    "    concat2 = Concatenate() #axis=2)\n",
    "    decoder_in_concat = concat2([context, prevWord])\n",
    "    \n",
    "    # pass into decoder, inference output\n",
    "    pred, s, c = decoder(decoder_in_concat, initial_state=[s, c])\n",
    "    pred = dense_decode(pred)\n",
    "    all_out.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output needs to be stacked to be considered as the network's output. Also, need batchsize N to be the first dimension, and thus a permutation of dimensions is required. Afterwards, the model can be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack(outputs):\n",
    "    outputs = K.stack(outputs)\n",
    "    return K.permute_dimensions(outputs, pattern=(1, 0, 2))\n",
    "\n",
    "stack_layer = Lambda(stack)\n",
    "all_out = stack_layer(all_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attnModel = Model(inputs=[input_layer_encoder, input_layer_decoder, s0, c0,],\n",
    "                 outputs=all_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attnModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define customized loss and accuracy metrics for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myLoss(y_train, pred):\n",
    "    mask = K.cast(y_train > 0, dtype='float32')\n",
    "    val = mask * y_train * K.log(pred)\n",
    "    return -K.sum(val) / K.sum(mask)\n",
    "\n",
    "def acc(y_train, pred):\n",
    "    targ = K.argmax(y_train, axis=-1)\n",
    "    pred = K.argmax(pred, axis=-1)\n",
    "    correct = K.cast(K.equal(targ, pred), dtype='float32')\n",
    "\n",
    "    mask = K.cast(K.greater(targ, 0), dtype='float32') # filter out padding value 0.\n",
    "    correctCount = K.sum(mask * correct)\n",
    "    totalCount = K.sum(mask)\n",
    "    return correctCount / totalCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model using Adam optimizer and defined loss and metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "attnModel.compile(optimizer='adam', loss=myLoss, metrics=[acc])\n",
    "\n",
    "# Define empty s0 and c0\n",
    "init_s = np.zeros((len(eng_seq_padded), LATENT_DIM_DECODER))\n",
    "init_c = np.zeros((len(eng_seq_padded), LATENT_DIM_DECODER))\n",
    "\n",
    "# Train\n",
    "history = attnModel.fit(\n",
    "    x=[eng_seq_padded, man_seq_padded, init_s, init_c],\n",
    "    y=man_target_one_hot,\n",
    "    batch_size=BATCHSIZE,\n",
    "    epochs=EPOCH,\n",
    "    validation_split=0.22\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['acc'], label='acc')\n",
    "plt.plot(history.history['val_acc'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attnModel.save('attention_model_35_man.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in the inference model teacher forcing is not available, thus the model needs to be modified to use the previous inference result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inf = Model(input_layer_encoder, encoder_out)\n",
    "encoder_out_inf = Input(shape=(max_in_len, LATENT_DIM * 2,))\n",
    "\n",
    "# Decoder\n",
    "decoder_in_inf = Input(shape=(1,))\n",
    "decoder_in_embed_inf = embed_decoder(decoder_in_inf)\n",
    "\n",
    "# Context, concat without teacher forcing.\n",
    "context_inf = iterAttn(encoder_out_inf, s0)\n",
    "decoder_in_concat_inf = concat2([context_inf, decoder_in_embed_inf])\n",
    "\n",
    "# Decoder inference\n",
    "pred, s, c = decoder(decoder_in_concat_inf, initial_state=[s0, c0])\n",
    "pred_out = dense_decode(pred)\n",
    "\n",
    "# Define model\n",
    "decoder_inf = Model(\n",
    "    inputs=[decoder_in_inf, encoder_out_inf, s0, c0],\n",
    "    outputs=[pred_out, s, c]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse the word-to-index maps to convert translated indices to words. Then use the inference encoder and decoder models to create predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_in = {b:a for a, b in word2idx_in.items()}\n",
    "idx2word_out = {b:a for a, b in word2idx_out.items()}\n",
    "\n",
    "def inference(eng_seq):\n",
    "    # Encode\n",
    "    encoder_output = encoder_inf.predict(eng_seq, verbose=0)\n",
    "    \n",
    "    # Create output seq matrix\n",
    "    target_output = np.zeros((1, 1))\n",
    "    target_output[0, 0] = word2idx_out['<sos>']\n",
    "    \n",
    "    # init\n",
    "    eos = word2idx_out['<eos>']\n",
    "    s0 = np.zeros((1, LATENT_DIM_DECODER))\n",
    "    c0 = np.zeros((1, LATENT_DIM_DECODER))\n",
    "    \n",
    "    output_seq = []\n",
    "    s = s0\n",
    "    c = c0\n",
    "    for _ in range(max_out_len):\n",
    "        # Decoder inference\n",
    "        pred, s, c = decoder_inf.predict([target_output, encoder_output, s, c], verbose=0)\n",
    "        \n",
    "        # update output seq\n",
    "        tok = np.argmax(pred.flatten())\n",
    "        if tok == eos:\n",
    "            break\n",
    "        if tok > 0:\n",
    "            word = idx2word_out[tok]\n",
    "            output_seq.append(word)\n",
    "\n",
    "        # Update decoder input\n",
    "        target_output[0, 0] = tok\n",
    "        \n",
    "    sentence = ' '.join(output_seq)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe some of the sample inference results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    i = np.random.choice(len(eng))\n",
    "    eng_sen = eng_seq_padded[i:i+1]\n",
    "    print(eng_sen)\n",
    "    man_pred = inference(eng_sen)\n",
    "\n",
    "    print('--------------------------------------')\n",
    "    print('English: {}'.format(eng[i]))\n",
    "    print('Prediction: {}'.format(man_pred))\n",
    "    print('Ground truth: {}'.format(man[i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with an arbitrary sentence.\n",
    "seq = tok_in.texts_to_sequences([\"let us go\"])\n",
    "seq_padded = pad_sequences(seq, maxlen=max_in_len, padding='post')\n",
    "print(seq_padded)\n",
    "inference([seq_padded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project implements an encoder-decoder-based seq2seq model using BiLSTM and attention mechanism. It is observed that the model achieves reasonable accuracy on validation data, although the translated sentences differ from the groundtruth to a considerable extent.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da5401",
   "language": "python",
   "name": "da5401"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
