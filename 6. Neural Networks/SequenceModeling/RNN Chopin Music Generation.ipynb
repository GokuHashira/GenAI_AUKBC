{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the advance of deep learning, computers can now compose music using trained models based on the sample pieces. This not only allows artists that are lack of the domain knowledge to accomplish their ideas, but also provides possible creative styles and patterns that may foster the composer's creative process. \n",
    "\n",
    "In the project, a Recurrent Neural Network (RNN) model is built using LSTM and dense layers. The model is then trained on 10 of Frédéric Chopin's Valse and Waltz pieces in MIDI format downloaded from [*kunstderfuge.com*](http://www.kunstderfuge.com/chopin.htm). The MIDI files are converted to numeric, sequential data and fed into the RNN for training. The trained RNN reads a pattern of notes, then generates new notes based on the sequence. \n",
    "\n",
    "Moreover, to explore different styles of piano pieces, a dataset of 8 Nocturne pieces by Frédéric Chopin downloaded from [*8notes.com*](https://www.8notes.com/school/search_fsm.asp?keyword=chopin+nocturne&x=0&y=0&pageA=1) are used to train a complete seperate RNN model. The generated sample pieces from the Valse model and the Nocturne model can be found [here](https://drive.google.com/drive/folders/1EtkZnCGtkbKOGfVaamLZjXdWjsgXqizi?usp=sharing).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries and collect data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 17:03:32.696571: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748000012.713823  312769 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748000012.719092  312769 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-23 17:03:32.738564: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# need python3.XX-dev, libasound2-dev deb packages\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "from timidity import Parser, play_notes\n",
    "\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, LSTM, Dropout, Flatten, Embedding\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nocturnes Dataset/Nocturne_Opus_72_No_1_in_E_Minor.mid',\n",
       " 'Nocturnes Dataset/chop32n2.mid',\n",
       " 'Nocturnes Dataset/chopin_Nocturne_b49.mid',\n",
       " 'Nocturnes Dataset/chopin_cminor_nocturne_b108_PNO.mid',\n",
       " 'Nocturnes Dataset/chopin_nocturne_9_2.mid',\n",
       " 'Nocturnes Dataset/chopin_nocturneop9nr1.mid',\n",
       " 'Nocturnes Dataset/chopin_op55_1.mid',\n",
       " 'Nocturnes Dataset/chopin_op55_2.mid']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valseMusic = glob.glob('Nocturnes Dataset/*.mid')\n",
    "valseMusic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the *music21* library, the MIDI files are first parsed into single instrument track of notes and chords, then chords are further broken down into groups of notes. The MIDI files are concatenated and are eventually converted to a sequence of notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedNotes = []\n",
    "\n",
    "for valse in valseMusic:\n",
    "    \n",
    "    # parse Midi\n",
    "    valse = converter.parse(valse)\n",
    "    \n",
    "    # Handle cases for multi-instruments\n",
    "    try:\n",
    "        parts = instrument.partitionByInstrument(valse)\n",
    "        notes = parts.parts[0].recurse()\n",
    "    except:\n",
    "        notes = valse.flat.notes\n",
    "    \n",
    "    # Handle chords\n",
    "    for singleNote in notes:\n",
    "        if isinstance(singleNote, chord.Chord):\n",
    "            processedNotes.append('.'.join(str(norm) for norm in singleNote.normalOrder))\n",
    "        elif isinstance(singleNote, note.Note):\n",
    "            processedNotes.append(str(singleNote.pitch))\n",
    "\n",
    "# Save a notes library\n",
    "with open('nocturnesNotesLibrary', 'wb') as file:\n",
    "    pickle.dump(processedNotes, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6331, ['E2', 'B2', 'G3', 'E3', 'C4', 'B3', 'E2', 'B2', 'G3', 'E3'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processedNotes), processedNotes[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, each unique note/pitch is assigned with a unique number. The sequence of notes is then divided into subsequences, each containing the numeric representation of 100 notes. The training data that is fed into the network is then sequences of 100 numbers, each representing a corresponding note. The label/output of each input sequence is the next note that is after the 100-note sequence, also represented by a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQSIZE = 100\n",
    "\n",
    "alphabetSize = len(set(processedNotes))\n",
    "\n",
    "# Find unique pitches\n",
    "pitches = sorted(set(processedNote for processedNote in processedNotes))\n",
    "\n",
    "# a dict to convert each unique note to a unique number.\n",
    "dictRef = dict((pitch, num) for num, pitch in enumerate(pitches))\n",
    "\n",
    "inputNotes = []\n",
    "outputNotes = []\n",
    "\n",
    "# Divide sequence into subsequences of 100 notes, then convert notes to numbers\n",
    "for i in range(0, len(processedNotes) - SEQSIZE):\n",
    "    batchInput = processedNotes[i: i+SEQSIZE]\n",
    "    batchOutput = processedNotes[i+SEQSIZE]\n",
    "    inputNotes.append([dictRef[n] for n in batchInput])\n",
    "    outputNotes.append([dictRef[batchOutput]])\n",
    "\n",
    "# reshape the input and normalize, convert output to categorical.\n",
    "batchNum = len(inputNotes)\n",
    "inputNotes = np.reshape(inputNotes, (batchNum, SEQSIZE, 1)) / float(alphabetSize)\n",
    "outputNotes = to_categorical(outputNotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.80363636],\n",
       "       [0.86909091],\n",
       "       [0.96727273],\n",
       "       [0.94181818],\n",
       "       [0.93818182]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputNotes[0][95:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('0', 0), ('0.1', 1), ('0.1.3', 2), ('0.1.5', 3), ('0.1.6', 4), ('0.2', 5), ('0.2.5', 6), ('0.2.5.6', 7), ('0.2.5.8', 8), ('0.3', 9), ('0.3.4', 10), ('0.3.5', 11), ('0.3.6', 12), ('0.3.7', 13), ('0.4', 14), ('0.4.6', 15), ('0.4.7', 16), ('0.5', 17), ('0.6', 18), ('1', 19), ('1.2', 20), ('1.2.4', 21), ('1.2.6', 22), ('1.2.7', 23), ('1.3', 24), ('1.3.4', 25), ('1.3.8', 26), ('1.4', 27), ('1.4.8', 28), ('1.5', 29), ('1.5.7', 30), ('1.5.8', 31), ('1.6', 32), ('1.7', 33), ('10', 34), ('10.0', 35), ('10.0.3', 36), ('10.0.4', 37), ('10.0.5', 38), ('10.1', 39), ('10.1.2', 40), ('10.1.5', 41), ('10.11', 42), ('10.11.0', 43), ('10.11.1', 44), ('10.2', 45), ('10.2.5', 46), ('10.3', 47), ('11', 48), ('11.0', 49), ('11.0.1', 50), ('11.0.2', 51), ('11.0.2.5', 52), ('11.0.2.5.7', 53), ('11.0.4', 54), ('11.0.5', 55), ('11.1', 56), ('11.1.2', 57), ('11.1.3', 58), ('11.1.4', 59), ('11.1.5', 60), ('11.1.6', 61), ('11.2', 62), ('11.2.4', 63), ('11.2.5', 64), ('11.2.5.7', 65), ('11.2.6', 66), ('11.3', 67), ('11.3.6', 68), ('11.4', 69), ('2', 70), ('2.3', 71), ('2.4', 72), ('2.4.6', 73), ('2.4.8', 74), ('2.5', 75), ('2.5.6.10', 76), ('2.5.8', 77), ('2.6', 78), ('2.6.8', 79), ('2.7', 80), ('2.7.8', 81), ('2.8', 82), ('3', 83), ('3.4', 84), ('3.4.5', 85), ('3.4.6', 86), ('3.4.6.8.9', 87), ('3.5', 88), ('3.5.10', 89), ('3.5.7', 90), ('3.6', 91), ('3.6.7', 92), ('3.6.8', 93), ('3.6.9', 94), ('3.7', 95), ('3.7.10', 96), ('3.8', 97), ('3.9', 98), ('4', 99), ('4.10', 100), ('4.5', 101), ('4.5.6', 102), ('4.5.8.0', 103), ('4.6', 104), ('4.6.11', 105), ('4.6.7.11', 106), ('4.6.8', 107), ('4.6.8.9', 108), ('4.6.9', 109), ('4.7', 110), ('4.7.10', 111), ('4.7.11', 112), ('4.7.9', 113), ('4.8', 114), ('4.8.10', 115), ('4.8.11', 116), ('4.9', 117), ('5', 118), ('5.10', 119), ('5.11', 120), ('5.6', 121), ('5.6.11', 122), ('5.6.8', 123), ('5.7', 124), ('5.7.0', 125), ('5.7.10', 126), ('5.7.10.1', 127), ('5.7.11', 128), ('5.7.8', 129), ('5.8', 130), ('5.8.0', 131), ('5.8.10', 132), ('5.8.11', 133), ('5.8.11.1', 134), ('5.9', 135), ('5.9.0', 136), ('6', 137), ('6.10', 138), ('6.10.0', 139), ('6.10.1', 140), ('6.11', 141), ('6.7', 142), ('6.7.11', 143), ('6.7.9.0.2', 144), ('6.8', 145), ('6.8.0', 146), ('6.8.10', 147), ('6.8.9', 148), ('6.8.9.10', 149), ('6.8.9.11.1', 150), ('6.9', 151), ('6.9.0', 152), ('6.9.0.2', 153), ('6.9.11', 154), ('7', 155), ('7.0', 156), ('7.10', 157), ('7.10.0', 158), ('7.10.0.3', 159), ('7.10.1.3', 160), ('7.10.2', 161), ('7.11', 162), ('7.11.0', 163), ('7.11.1', 164), ('7.8', 165), ('7.8.0', 166), ('7.8.1', 167), ('7.8.10', 168), ('7.8.9', 169), ('7.9', 170), ('7.9.1', 171), ('7.9.11', 172), ('8', 173), ('8.0', 174), ('8.0.2', 175), ('8.0.3', 176), ('8.1', 177), ('8.10', 178), ('8.10.0', 179), ('8.10.1', 180), ('8.10.2', 181), ('8.11', 182), ('8.11.0', 183), ('8.11.1', 184), ('8.11.3', 185), ('8.9', 186), ('8.9.11', 187), ('9', 188), ('9.0', 189), ('9.0.4', 190), ('9.1', 191), ('9.1.3', 192), ('9.1.4', 193), ('9.10', 194), ('9.10.0', 195), ('9.10.0.1', 196), ('9.10.0.1.3', 197), ('9.10.11.0', 198), ('9.10.11.0.3', 199), ('9.11', 200), ('9.11.1', 201), ('9.11.1.3', 202), ('9.11.1.3.4', 203), ('9.11.2', 204), ('9.11.3', 205), ('9.11.4', 206), ('9.2', 207), ('A2', 208), ('A3', 209), ('A4', 210), ('A5', 211), ('A6', 212), ('B-1', 213), ('B-2', 214), ('B-3', 215), ('B-4', 216), ('B-5', 217), ('B-6', 218), ('B1', 219), ('B2', 220), ('B3', 221), ('B4', 222), ('B5', 223), ('C#2', 224), ('C#3', 225), ('C#4', 226), ('C#5', 227), ('C#6', 228), ('C#7', 229), ('C2', 230), ('C3', 231), ('C4', 232), ('C5', 233), ('C6', 234), ('C7', 235), ('D2', 236), ('D3', 237), ('D4', 238), ('D5', 239), ('D6', 240), ('D7', 241), ('E-2', 242), ('E-3', 243), ('E-4', 244), ('E-5', 245), ('E-6', 246), ('E2', 247), ('E3', 248), ('E4', 249), ('E5', 250), ('E6', 251), ('F#1', 252), ('F#2', 253), ('F#3', 254), ('F#4', 255), ('F#5', 256), ('F#6', 257), ('F2', 258), ('F3', 259), ('F4', 260), ('F5', 261), ('F6', 262), ('F7', 263), ('G#1', 264), ('G#2', 265), ('G#3', 266), ('G#4', 267), ('G#5', 268), ('G#6', 269), ('G2', 270), ('G3', 271), ('G4', 272), ('G5', 273), ('G6', 274)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notes (pitch) to Index\n",
    "dictRef.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The design of the RNN model is traditional and is structured as follows. Two LSTM-and-Dropout groups are followed by two densely-connected layers, also accompanied by Dropout layers. The output activation function is softmax. Since this is a single-label classification task, categorical crossentropy loss is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputNotes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "def createRNNModel(inputNotes, outputNum):\n",
    "    \n",
    "    rnnModel = Sequential([\n",
    "        LSTM(128, \n",
    "             #input_shape=inputNotes.shape[1:],\n",
    "             return_sequences=True, \n",
    "             recurrent_initializer='glorot_uniform',\n",
    "             recurrent_activation='sigmoid'), \n",
    "        Dropout(0.2),\n",
    "\n",
    "        LSTM(128, \n",
    "             return_sequences=True,\n",
    "             recurrent_initializer='glorot_uniform',\n",
    "             recurrent_activation='sigmoid'), \n",
    "        Dropout(0.2),\n",
    "\n",
    "        Flatten(),\n",
    "        Dropout(0.2),\n",
    "        Dense(256),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(outputNum, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    rnnModel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return rnnModel\n",
    "    \n",
    "rnnModel = createRNNModel(inputNotes, alphabetSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the style is mapped, the model is trained with 500 epochs. It is worth mentioning that since there are such great varieties in the labels, validation and early stopping mechanisms are not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1748000317.370697  316338 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.9920\n",
      "Epoch 1: loss improved from inf to 4.86673, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 4.9901\n",
      "Epoch 2/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.7723\n",
      "Epoch 2: loss improved from 4.86673 to 4.75564, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 4.7721\n",
      "Epoch 3/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.6683\n",
      "Epoch 3: loss improved from 4.75564 to 4.68214, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 4.6684\n",
      "Epoch 4/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.5721\n",
      "Epoch 4: loss improved from 4.68214 to 4.58796, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 4.5723\n",
      "Epoch 5/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.4692\n",
      "Epoch 5: loss improved from 4.58796 to 4.50393, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 4.4696\n",
      "Epoch 6/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.3991\n",
      "Epoch 6: loss improved from 4.50393 to 4.42527, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 4.3995\n",
      "Epoch 7/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.2868\n",
      "Epoch 7: loss improved from 4.42527 to 4.34700, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 4.2880\n",
      "Epoch 8/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.1951\n",
      "Epoch 8: loss improved from 4.34700 to 4.23925, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 4.1955\n",
      "Epoch 9/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4.0571\n",
      "Epoch 9: loss improved from 4.23925 to 4.10793, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 4.0581\n",
      "Epoch 10/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.8891\n",
      "Epoch 10: loss improved from 4.10793 to 3.94682, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 3.8900\n",
      "Epoch 11/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.6558\n",
      "Epoch 11: loss improved from 3.94682 to 3.74122, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 3.6567\n",
      "Epoch 12/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.4426\n",
      "Epoch 12: loss improved from 3.74122 to 3.52748, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 3.4430\n",
      "Epoch 13/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3.1704\n",
      "Epoch 13: loss improved from 3.52748 to 3.31405, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 3.1726\n",
      "Epoch 14/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.9541\n",
      "Epoch 14: loss improved from 3.31405 to 3.06349, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 2.9558\n",
      "Epoch 15/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.6473\n",
      "Epoch 15: loss improved from 3.06349 to 2.80854, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 2.6489\n",
      "Epoch 16/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.3930\n",
      "Epoch 16: loss improved from 2.80854 to 2.54541, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 2.3946\n",
      "Epoch 17/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.0737\n",
      "Epoch 17: loss improved from 2.54541 to 2.26407, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 2.0766\n",
      "Epoch 18/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.8481\n",
      "Epoch 18: loss improved from 2.26407 to 1.96589, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 1.8510\n",
      "Epoch 19/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.5948\n",
      "Epoch 19: loss improved from 1.96589 to 1.73730, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 1.5970\n",
      "Epoch 20/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.3229\n",
      "Epoch 20: loss improved from 1.73730 to 1.45361, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 1.3249\n",
      "Epoch 21/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.1038\n",
      "Epoch 21: loss improved from 1.45361 to 1.21262, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 1.1054\n",
      "Epoch 22/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.9374\n",
      "Epoch 22: loss improved from 1.21262 to 1.03203, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.9379\n",
      "Epoch 23/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.7209\n",
      "Epoch 23: loss improved from 1.03203 to 0.82293, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.7214\n",
      "Epoch 24/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.6637\n",
      "Epoch 24: loss improved from 0.82293 to 0.73432, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.6652\n",
      "Epoch 25/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4974\n",
      "Epoch 25: loss improved from 0.73432 to 0.58339, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.4996\n",
      "Epoch 26/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4560\n",
      "Epoch 26: loss improved from 0.58339 to 0.49072, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.4569\n",
      "Epoch 27/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3454\n",
      "Epoch 27: loss improved from 0.49072 to 0.39677, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.3459\n",
      "Epoch 28/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3346\n",
      "Epoch 28: loss improved from 0.39677 to 0.39316, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.3355\n",
      "Epoch 29/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2769\n",
      "Epoch 29: loss improved from 0.39316 to 0.32119, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.2777\n",
      "Epoch 30/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2808\n",
      "Epoch 30: loss improved from 0.32119 to 0.31382, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.2813\n",
      "Epoch 31/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2486\n",
      "Epoch 31: loss improved from 0.31382 to 0.28076, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.2491\n",
      "Epoch 32/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2305\n",
      "Epoch 32: loss improved from 0.28076 to 0.26463, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.2309\n",
      "Epoch 33/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2037\n",
      "Epoch 33: loss improved from 0.26463 to 0.24960, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.2039\n",
      "Epoch 34/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2001\n",
      "Epoch 34: loss improved from 0.24960 to 0.20622, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.2001\n",
      "Epoch 35/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1862\n",
      "Epoch 35: loss improved from 0.20622 to 0.19874, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.1864\n",
      "Epoch 36/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1867\n",
      "Epoch 36: loss did not improve from 0.19874\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1874\n",
      "Epoch 37/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1685\n",
      "Epoch 37: loss did not improve from 0.19874\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1688\n",
      "Epoch 38/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1895\n",
      "Epoch 38: loss did not improve from 0.19874\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1899\n",
      "Epoch 39/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1801\n",
      "Epoch 39: loss improved from 0.19874 to 0.18552, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.1801\n",
      "Epoch 40/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1632\n",
      "Epoch 40: loss did not improve from 0.18552\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1639\n",
      "Epoch 41/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1523\n",
      "Epoch 41: loss improved from 0.18552 to 0.15883, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.1524\n",
      "Epoch 42/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1311\n",
      "Epoch 42: loss improved from 0.15883 to 0.14293, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.1313\n",
      "Epoch 43/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1031\n",
      "Epoch 43: loss improved from 0.14293 to 0.13083, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1035\n",
      "Epoch 44/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1237\n",
      "Epoch 44: loss did not improve from 0.13083\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.1239\n",
      "Epoch 45/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1445\n",
      "Epoch 45: loss did not improve from 0.13083\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1447\n",
      "Epoch 46/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1234\n",
      "Epoch 46: loss did not improve from 0.13083\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1240\n",
      "Epoch 47/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1329\n",
      "Epoch 47: loss did not improve from 0.13083\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1330\n",
      "Epoch 48/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1481\n",
      "Epoch 48: loss did not improve from 0.13083\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1482\n",
      "Epoch 49/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1388\n",
      "Epoch 49: loss did not improve from 0.13083\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1389\n",
      "Epoch 50/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1332\n",
      "Epoch 50: loss did not improve from 0.13083\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1337\n",
      "Epoch 51/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1305\n",
      "Epoch 51: loss did not improve from 0.13083\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1309\n",
      "Epoch 52/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1266\n",
      "Epoch 52: loss did not improve from 0.13083\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1270\n",
      "Epoch 53/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1117\n",
      "Epoch 53: loss improved from 0.13083 to 0.12548, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.1118\n",
      "Epoch 54/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1102\n",
      "Epoch 54: loss improved from 0.12548 to 0.11798, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.1103\n",
      "Epoch 55/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0883\n",
      "Epoch 55: loss improved from 0.11798 to 0.11186, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0889\n",
      "Epoch 56/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1029\n",
      "Epoch 56: loss did not improve from 0.11186\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1029\n",
      "Epoch 57/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1414\n",
      "Epoch 57: loss did not improve from 0.11186\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.1416\n",
      "Epoch 58/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1436\n",
      "Epoch 58: loss did not improve from 0.11186\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1437\n",
      "Epoch 59/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1065\n",
      "Epoch 59: loss did not improve from 0.11186\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1069\n",
      "Epoch 60/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1119\n",
      "Epoch 60: loss did not improve from 0.11186\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1121\n",
      "Epoch 61/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1110\n",
      "Epoch 61: loss did not improve from 0.11186\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1110\n",
      "Epoch 62/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0935\n",
      "Epoch 62: loss did not improve from 0.11186\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0938\n",
      "Epoch 63/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1083\n",
      "Epoch 63: loss did not improve from 0.11186\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1084\n",
      "Epoch 64/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1137\n",
      "Epoch 64: loss did not improve from 0.11186\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1138\n",
      "Epoch 65/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1127\n",
      "Epoch 65: loss did not improve from 0.11186\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1127\n",
      "Epoch 66/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1043\n",
      "Epoch 66: loss did not improve from 0.11186\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1044\n",
      "Epoch 67/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1014\n",
      "Epoch 67: loss did not improve from 0.11186\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1017\n",
      "Epoch 68/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1008\n",
      "Epoch 68: loss improved from 0.11186 to 0.10764, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.1009\n",
      "Epoch 69/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1106\n",
      "Epoch 69: loss did not improve from 0.10764\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1106\n",
      "Epoch 70/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1186\n",
      "Epoch 70: loss did not improve from 0.10764\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1188\n",
      "Epoch 71/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0902\n",
      "Epoch 71: loss improved from 0.10764 to 0.09762, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0903\n",
      "Epoch 72/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0709\n",
      "Epoch 72: loss improved from 0.09762 to 0.09594, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0714\n",
      "Epoch 73/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0976\n",
      "Epoch 73: loss did not improve from 0.09594\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0978\n",
      "Epoch 74/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0819\n",
      "Epoch 74: loss improved from 0.09594 to 0.08929, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0820\n",
      "Epoch 75/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0729\n",
      "Epoch 75: loss did not improve from 0.08929\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0732\n",
      "Epoch 76/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1027\n",
      "Epoch 76: loss did not improve from 0.08929\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1027\n",
      "Epoch 77/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0690\n",
      "Epoch 77: loss did not improve from 0.08929\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0693\n",
      "Epoch 78/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1265\n",
      "Epoch 78: loss did not improve from 0.08929\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1265\n",
      "Epoch 79/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0955\n",
      "Epoch 79: loss did not improve from 0.08929\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0958\n",
      "Epoch 80/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0852\n",
      "Epoch 80: loss did not improve from 0.08929\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0855\n",
      "Epoch 81/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0990\n",
      "Epoch 81: loss did not improve from 0.08929\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0992\n",
      "Epoch 82/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0656\n",
      "Epoch 82: loss improved from 0.08929 to 0.07515, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0657\n",
      "Epoch 83/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1038\n",
      "Epoch 83: loss did not improve from 0.07515\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1038\n",
      "Epoch 84/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0805\n",
      "Epoch 84: loss did not improve from 0.07515\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0806\n",
      "Epoch 85/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0521\n",
      "Epoch 85: loss did not improve from 0.07515\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0525\n",
      "Epoch 86/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1189\n",
      "Epoch 86: loss did not improve from 0.07515\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1189\n",
      "Epoch 87/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0816\n",
      "Epoch 87: loss did not improve from 0.07515\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0818\n",
      "Epoch 88/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1032\n",
      "Epoch 88: loss did not improve from 0.07515\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1033\n",
      "Epoch 89/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0855\n",
      "Epoch 89: loss did not improve from 0.07515\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0856\n",
      "Epoch 90/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0523\n",
      "Epoch 90: loss did not improve from 0.07515\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0529\n",
      "Epoch 91/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0926\n",
      "Epoch 91: loss did not improve from 0.07515\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0926\n",
      "Epoch 92/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1019\n",
      "Epoch 92: loss did not improve from 0.07515\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1019\n",
      "Epoch 93/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0898\n",
      "Epoch 93: loss did not improve from 0.07515\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0898\n",
      "Epoch 94/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0846\n",
      "Epoch 94: loss did not improve from 0.07515\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0847\n",
      "Epoch 95/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0765\n",
      "Epoch 95: loss did not improve from 0.07515\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0767\n",
      "Epoch 96/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0780\n",
      "Epoch 96: loss did not improve from 0.07515\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0781\n",
      "Epoch 97/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0755\n",
      "Epoch 97: loss did not improve from 0.07515\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0757\n",
      "Epoch 98/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0820\n",
      "Epoch 98: loss did not improve from 0.07515\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0820\n",
      "Epoch 99/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0818\n",
      "Epoch 99: loss did not improve from 0.07515\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0820\n",
      "Epoch 100/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0910\n",
      "Epoch 100: loss did not improve from 0.07515\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0912\n",
      "Epoch 101/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0737\n",
      "Epoch 101: loss did not improve from 0.07515\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0739\n",
      "Epoch 102/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0649\n",
      "Epoch 102: loss improved from 0.07515 to 0.06905, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0650\n",
      "Epoch 103/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0664\n",
      "Epoch 103: loss did not improve from 0.06905\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0665\n",
      "Epoch 104/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0587\n",
      "Epoch 104: loss did not improve from 0.06905\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0589\n",
      "Epoch 105/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0508\n",
      "Epoch 105: loss did not improve from 0.06905\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0509\n",
      "Epoch 106/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0760\n",
      "Epoch 106: loss did not improve from 0.06905\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0764\n",
      "Epoch 107/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0828\n",
      "Epoch 107: loss did not improve from 0.06905\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0829\n",
      "Epoch 108/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0669\n",
      "Epoch 108: loss did not improve from 0.06905\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0670\n",
      "Epoch 109/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0894\n",
      "Epoch 109: loss did not improve from 0.06905\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0896\n",
      "Epoch 110/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0691\n",
      "Epoch 110: loss did not improve from 0.06905\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0691\n",
      "Epoch 111/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0625\n",
      "Epoch 111: loss did not improve from 0.06905\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0627\n",
      "Epoch 112/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0597\n",
      "Epoch 112: loss did not improve from 0.06905\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0598\n",
      "Epoch 113/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0916\n",
      "Epoch 113: loss did not improve from 0.06905\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0915\n",
      "Epoch 114/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0757\n",
      "Epoch 114: loss did not improve from 0.06905\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0758\n",
      "Epoch 115/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0925\n",
      "Epoch 115: loss did not improve from 0.06905\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0922\n",
      "Epoch 116/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0664\n",
      "Epoch 116: loss did not improve from 0.06905\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0664\n",
      "Epoch 117/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0731\n",
      "Epoch 117: loss did not improve from 0.06905\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0734\n",
      "Epoch 118/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0803\n",
      "Epoch 118: loss did not improve from 0.06905\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0803\n",
      "Epoch 119/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0533\n",
      "Epoch 119: loss improved from 0.06905 to 0.06809, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0536\n",
      "Epoch 120/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1013\n",
      "Epoch 120: loss did not improve from 0.06809\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.1013\n",
      "Epoch 121/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0810\n",
      "Epoch 121: loss did not improve from 0.06809\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0812\n",
      "Epoch 122/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0711\n",
      "Epoch 122: loss did not improve from 0.06809\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0714\n",
      "Epoch 123/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0671\n",
      "Epoch 123: loss did not improve from 0.06809\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0674\n",
      "Epoch 124/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0825\n",
      "Epoch 124: loss did not improve from 0.06809\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0825\n",
      "Epoch 125/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0655\n",
      "Epoch 125: loss improved from 0.06809 to 0.06753, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0655\n",
      "Epoch 126/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0414\n",
      "Epoch 126: loss improved from 0.06753 to 0.05539, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0417\n",
      "Epoch 127/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0864\n",
      "Epoch 127: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0865\n",
      "Epoch 128/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1042\n",
      "Epoch 128: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.1039\n",
      "Epoch 129/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0686\n",
      "Epoch 129: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0689\n",
      "Epoch 130/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0868\n",
      "Epoch 130: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 0.0868\n",
      "Epoch 131/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0497\n",
      "Epoch 131: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0497\n",
      "Epoch 132/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0559\n",
      "Epoch 132: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0560\n",
      "Epoch 133/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0881\n",
      "Epoch 133: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0881\n",
      "Epoch 134/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0611\n",
      "Epoch 134: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0611\n",
      "Epoch 135/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0869\n",
      "Epoch 135: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0868\n",
      "Epoch 136/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0573\n",
      "Epoch 136: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0574\n",
      "Epoch 137/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0635\n",
      "Epoch 137: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0636\n",
      "Epoch 138/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0856\n",
      "Epoch 138: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0856\n",
      "Epoch 139/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0679\n",
      "Epoch 139: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0680\n",
      "Epoch 140/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0640\n",
      "Epoch 140: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0643\n",
      "Epoch 141/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0553\n",
      "Epoch 141: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0554\n",
      "Epoch 142/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0658\n",
      "Epoch 142: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0658\n",
      "Epoch 143/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0736\n",
      "Epoch 143: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0737\n",
      "Epoch 144/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0652\n",
      "Epoch 144: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0651\n",
      "Epoch 145/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0590\n",
      "Epoch 145: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0593\n",
      "Epoch 146/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0570\n",
      "Epoch 146: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0570\n",
      "Epoch 147/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0511\n",
      "Epoch 147: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0513\n",
      "Epoch 148/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0620\n",
      "Epoch 148: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0619\n",
      "Epoch 149/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0774\n",
      "Epoch 149: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0775\n",
      "Epoch 150/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0479\n",
      "Epoch 150: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0483\n",
      "Epoch 151/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0548\n",
      "Epoch 151: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0549\n",
      "Epoch 152/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0560\n",
      "Epoch 152: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0560\n",
      "Epoch 153/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0546\n",
      "Epoch 153: loss did not improve from 0.05539\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0547\n",
      "Epoch 154/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0628\n",
      "Epoch 154: loss improved from 0.05539 to 0.05104, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0626\n",
      "Epoch 155/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0565\n",
      "Epoch 155: loss did not improve from 0.05104\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0565\n",
      "Epoch 156/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0545\n",
      "Epoch 156: loss did not improve from 0.05104\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0544\n",
      "Epoch 157/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0901\n",
      "Epoch 157: loss did not improve from 0.05104\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0900\n",
      "Epoch 158/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0503\n",
      "Epoch 158: loss did not improve from 0.05104\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0504\n",
      "Epoch 159/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0619\n",
      "Epoch 159: loss did not improve from 0.05104\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0620\n",
      "Epoch 160/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0485\n",
      "Epoch 160: loss did not improve from 0.05104\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0488\n",
      "Epoch 161/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0574\n",
      "Epoch 161: loss did not improve from 0.05104\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0574\n",
      "Epoch 162/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0559\n",
      "Epoch 162: loss did not improve from 0.05104\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0559\n",
      "Epoch 163/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0567\n",
      "Epoch 163: loss did not improve from 0.05104\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0567\n",
      "Epoch 164/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0582\n",
      "Epoch 164: loss did not improve from 0.05104\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0585\n",
      "Epoch 165/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0627\n",
      "Epoch 165: loss did not improve from 0.05104\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0627\n",
      "Epoch 166/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0533\n",
      "Epoch 166: loss did not improve from 0.05104\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0535\n",
      "Epoch 167/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0512\n",
      "Epoch 167: loss improved from 0.05104 to 0.04576, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0511\n",
      "Epoch 168/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0803\n",
      "Epoch 168: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0802\n",
      "Epoch 169/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0646\n",
      "Epoch 169: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0648\n",
      "Epoch 170/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0798\n",
      "Epoch 170: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0799\n",
      "Epoch 171/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0652\n",
      "Epoch 171: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0653\n",
      "Epoch 172/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0779\n",
      "Epoch 172: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0778\n",
      "Epoch 173/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0428\n",
      "Epoch 173: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0430\n",
      "Epoch 174/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0467\n",
      "Epoch 174: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0467\n",
      "Epoch 175/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0786\n",
      "Epoch 175: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0784\n",
      "Epoch 176/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0497\n",
      "Epoch 176: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0497\n",
      "Epoch 177/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0494\n",
      "Epoch 177: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0496\n",
      "Epoch 178/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0499\n",
      "Epoch 178: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0499\n",
      "Epoch 179/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0672\n",
      "Epoch 179: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0673\n",
      "Epoch 180/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0598\n",
      "Epoch 180: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0598\n",
      "Epoch 181/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0350\n",
      "Epoch 181: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0351\n",
      "Epoch 182/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0659\n",
      "Epoch 182: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0662\n",
      "Epoch 183/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0497\n",
      "Epoch 183: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0501\n",
      "Epoch 184/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0519\n",
      "Epoch 184: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0520\n",
      "Epoch 185/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0825\n",
      "Epoch 185: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0825\n",
      "Epoch 186/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0539\n",
      "Epoch 186: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0540\n",
      "Epoch 187/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0563\n",
      "Epoch 187: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0565\n",
      "Epoch 188/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0564\n",
      "Epoch 188: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0565\n",
      "Epoch 189/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0461\n",
      "Epoch 189: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0462\n",
      "Epoch 190/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0496\n",
      "Epoch 190: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0497\n",
      "Epoch 191/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0831\n",
      "Epoch 191: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0830\n",
      "Epoch 192/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0563\n",
      "Epoch 192: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0564\n",
      "Epoch 193/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0829\n",
      "Epoch 193: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0828\n",
      "Epoch 194/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0752\n",
      "Epoch 194: loss did not improve from 0.04576\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0750\n",
      "Epoch 195/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0404\n",
      "Epoch 195: loss improved from 0.04576 to 0.04296, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0405\n",
      "Epoch 196/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0539\n",
      "Epoch 196: loss did not improve from 0.04296\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0540\n",
      "Epoch 197/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0515\n",
      "Epoch 197: loss did not improve from 0.04296\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0515\n",
      "Epoch 198/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0670\n",
      "Epoch 198: loss did not improve from 0.04296\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0670\n",
      "Epoch 199/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0625\n",
      "Epoch 199: loss did not improve from 0.04296\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0623\n",
      "Epoch 200/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0479\n",
      "Epoch 200: loss did not improve from 0.04296\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0481\n",
      "Epoch 201/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0468\n",
      "Epoch 201: loss did not improve from 0.04296\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0469\n",
      "Epoch 202/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0739\n",
      "Epoch 202: loss did not improve from 0.04296\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0735\n",
      "Epoch 203/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0836\n",
      "Epoch 203: loss did not improve from 0.04296\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0835\n",
      "Epoch 204/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0854\n",
      "Epoch 204: loss did not improve from 0.04296\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0849\n",
      "Epoch 205/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0483\n",
      "Epoch 205: loss did not improve from 0.04296\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0484\n",
      "Epoch 206/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0411\n",
      "Epoch 206: loss did not improve from 0.04296\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0412\n",
      "Epoch 207/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0327\n",
      "Epoch 207: loss improved from 0.04296 to 0.03983, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0328\n",
      "Epoch 208/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0483\n",
      "Epoch 208: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0484\n",
      "Epoch 209/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0462\n",
      "Epoch 209: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0464\n",
      "Epoch 210/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0597\n",
      "Epoch 210: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0596\n",
      "Epoch 211/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0402\n",
      "Epoch 211: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0403\n",
      "Epoch 212/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0679\n",
      "Epoch 212: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0680\n",
      "Epoch 213/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0480\n",
      "Epoch 213: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0481\n",
      "Epoch 214/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0436\n",
      "Epoch 214: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0437\n",
      "Epoch 215/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0450\n",
      "Epoch 215: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0451\n",
      "Epoch 216/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0623\n",
      "Epoch 216: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0623\n",
      "Epoch 217/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0422\n",
      "Epoch 217: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0423\n",
      "Epoch 218/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0551\n",
      "Epoch 218: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0551\n",
      "Epoch 219/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0507\n",
      "Epoch 219: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0507\n",
      "Epoch 220/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0682\n",
      "Epoch 220: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0680\n",
      "Epoch 221/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0410\n",
      "Epoch 221: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0412\n",
      "Epoch 222/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0505\n",
      "Epoch 222: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0506\n",
      "Epoch 223/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0412\n",
      "Epoch 223: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0413\n",
      "Epoch 224/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0491\n",
      "Epoch 224: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0490\n",
      "Epoch 225/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0404\n",
      "Epoch 225: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0405\n",
      "Epoch 226/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0650\n",
      "Epoch 226: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0649\n",
      "Epoch 227/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0796\n",
      "Epoch 227: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0795\n",
      "Epoch 228/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0443\n",
      "Epoch 228: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0444\n",
      "Epoch 229/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0358\n",
      "Epoch 229: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0359\n",
      "Epoch 230/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0573\n",
      "Epoch 230: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0572\n",
      "Epoch 231/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0475\n",
      "Epoch 231: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0476\n",
      "Epoch 232/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0515\n",
      "Epoch 232: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0516\n",
      "Epoch 233/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0517\n",
      "Epoch 233: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0517\n",
      "Epoch 234/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0327\n",
      "Epoch 234: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0330\n",
      "Epoch 235/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0597\n",
      "Epoch 235: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0598\n",
      "Epoch 236/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0723\n",
      "Epoch 236: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0722\n",
      "Epoch 237/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0347\n",
      "Epoch 237: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0349\n",
      "Epoch 238/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0815\n",
      "Epoch 238: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0814\n",
      "Epoch 239/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0585\n",
      "Epoch 239: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0585\n",
      "Epoch 240/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0463\n",
      "Epoch 240: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0463\n",
      "Epoch 241/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0769\n",
      "Epoch 241: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0769\n",
      "Epoch 242/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0410\n",
      "Epoch 242: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0410\n",
      "Epoch 243/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0444\n",
      "Epoch 243: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0444\n",
      "Epoch 244/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0683\n",
      "Epoch 244: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0683\n",
      "Epoch 245/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0600\n",
      "Epoch 245: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0601\n",
      "Epoch 246/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0597\n",
      "Epoch 246: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0598\n",
      "Epoch 247/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0511\n",
      "Epoch 247: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0511\n",
      "Epoch 248/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0399\n",
      "Epoch 248: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0401\n",
      "Epoch 249/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0305\n",
      "Epoch 249: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0309\n",
      "Epoch 250/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0450\n",
      "Epoch 250: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0451\n",
      "Epoch 251/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0687\n",
      "Epoch 251: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0687\n",
      "Epoch 252/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0479\n",
      "Epoch 252: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0481\n",
      "Epoch 253/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0787\n",
      "Epoch 253: loss did not improve from 0.03983\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0786\n",
      "Epoch 254/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0275\n",
      "Epoch 254: loss improved from 0.03983 to 0.03670, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0276\n",
      "Epoch 255/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0289\n",
      "Epoch 255: loss improved from 0.03670 to 0.03455, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0290\n",
      "Epoch 256/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0281\n",
      "Epoch 256: loss did not improve from 0.03455\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0282\n",
      "Epoch 257/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0258\n",
      "Epoch 257: loss improved from 0.03455 to 0.03208, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0259\n",
      "Epoch 258/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0682\n",
      "Epoch 258: loss did not improve from 0.03208\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0682\n",
      "Epoch 259/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0550\n",
      "Epoch 259: loss did not improve from 0.03208\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0550\n",
      "Epoch 260/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0480\n",
      "Epoch 260: loss did not improve from 0.03208\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0481\n",
      "Epoch 261/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0395\n",
      "Epoch 261: loss did not improve from 0.03208\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0396\n",
      "Epoch 262/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0553\n",
      "Epoch 262: loss did not improve from 0.03208\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0553\n",
      "Epoch 263/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0622\n",
      "Epoch 263: loss did not improve from 0.03208\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0622\n",
      "Epoch 264/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0245\n",
      "Epoch 264: loss improved from 0.03208 to 0.03201, saving model to ChopinNocturnesLSTMx2.keras\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0246\n",
      "Epoch 265/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0375\n",
      "Epoch 265: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0376\n",
      "Epoch 266/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0438\n",
      "Epoch 266: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0439\n",
      "Epoch 267/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0601\n",
      "Epoch 267: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0600\n",
      "Epoch 268/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0325\n",
      "Epoch 268: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0326\n",
      "Epoch 269/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0537\n",
      "Epoch 269: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0537\n",
      "Epoch 270/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0399\n",
      "Epoch 270: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0402\n",
      "Epoch 271/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0416\n",
      "Epoch 271: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0418\n",
      "Epoch 272/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0487\n",
      "Epoch 272: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0487\n",
      "Epoch 273/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0359\n",
      "Epoch 273: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0361\n",
      "Epoch 274/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0390\n",
      "Epoch 274: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0390\n",
      "Epoch 275/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0388\n",
      "Epoch 275: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0390\n",
      "Epoch 276/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0452\n",
      "Epoch 276: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0453\n",
      "Epoch 277/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0390\n",
      "Epoch 277: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0389\n",
      "Epoch 278/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0252\n",
      "Epoch 278: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0253\n",
      "Epoch 279/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0261\n",
      "Epoch 279: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0264\n",
      "Epoch 280/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0527\n",
      "Epoch 280: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0527\n",
      "Epoch 281/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0432\n",
      "Epoch 281: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0433\n",
      "Epoch 282/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0470\n",
      "Epoch 282: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0471\n",
      "Epoch 283/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0651\n",
      "Epoch 283: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0651\n",
      "Epoch 284/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0494\n",
      "Epoch 284: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0497\n",
      "Epoch 285/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0640\n",
      "Epoch 285: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0641\n",
      "Epoch 286/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0621\n",
      "Epoch 286: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0621\n",
      "Epoch 287/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0331\n",
      "Epoch 287: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0331\n",
      "Epoch 288/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0329\n",
      "Epoch 288: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0331\n",
      "Epoch 289/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0243\n",
      "Epoch 289: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0246\n",
      "Epoch 290/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0333\n",
      "Epoch 290: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0334\n",
      "Epoch 291/300\n",
      "\u001b[1m193/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0352\n",
      "Epoch 291: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0355\n",
      "Epoch 292/300\n",
      "\u001b[1m191/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0471\n",
      "Epoch 292: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0473\n",
      "Epoch 293/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0402\n",
      "Epoch 293: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0403\n",
      "Epoch 294/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0501\n",
      "Epoch 294: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0500\n",
      "Epoch 295/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0353\n",
      "Epoch 295: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0353\n",
      "Epoch 296/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0482\n",
      "Epoch 296: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0482\n",
      "Epoch 297/300\n",
      "\u001b[1m194/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0273\n",
      "Epoch 297: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0274\n",
      "Epoch 298/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0484\n",
      "Epoch 298: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0484\n",
      "Epoch 299/300\n",
      "\u001b[1m192/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0385\n",
      "Epoch 299: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0387\n",
      "Epoch 300/300\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0373\n",
      "Epoch 300: loss did not improve from 0.03201\n",
      "\u001b[1m195/195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0373\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,560</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,277,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">275</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">70,675</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m66,560\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m3,277,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m275\u001b[0m)            │        \u001b[38;5;34m70,675\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,637,627</span> (40.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,637,627\u001b[0m (40.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,545,875</span> (13.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,545,875\u001b[0m (13.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,091,752</span> (27.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m7,091,752\u001b[0m (27.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpointCallback = ModelCheckpoint('ChopinNocturnesLSTMx2.keras', monitor='loss', \n",
    "                             verbose=2, save_best_only=True)\n",
    "rnnModel.fit(inputNotes, outputNotes, epochs=300, batch_size=32, callbacks=[checkpointCallback])\n",
    "rnnModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABC0AAAM6CAYAAACl+LmmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAB7CAAAewgFu0HU+AACKMklEQVR4nOzdd3hb9fn+8VuSLe+9EztxlrMhG8giIRAKZVNWyyylQGkLLd2/byl00kFpoYMy0wkFSge7lBFCCAmZZJLlJJ7x3kOWdH5/mIhIsh0PSUey36/rynX5HElHjxPFiW49n+djMQzDEAAAAAAAQJixml0AAAAAAABATwgtAAAAAABAWCK0AAAAAAAAYYnQAgAAAAAAhCVCCwAAAAAAEJYILQAAAAAAQFgitAAAAAAAAGGJ0AIAAAAAAIQlQgsAAAAAABCWCC0AAAAAAEBYIrQAAAAAAABhidACAAAAAACEJUILAAAAAAAQlggtAAAAAABAWCK0AAAAAAAAYYnQAgAAAAAAhKUoswsIpo6ODm3fvl2SlJWVpaioYf3tAgAAAABgCqfTqerqaknSzJkzFRsbG5DrDut38du3b9eCBQvMLgMAAAAAgBFjw4YNmj9/fkCuxfIQAAAAAAAQloZ1p0VWVpbn6w0bNigvL8/EagAAAAAAGJ4qKio8Kx2Ofy8+VMM6tDh+hkVeXp7y8/NNrAYAAAAAgOEvkPMkWR4CAAAAAADCEqEFAAAAAAAIS4QWAAAAAAAgLAU1tLBYLP36tWzZsmCWAQAAAAAAIhCdFgAAAAAAICyFZPeQW2+9VV/4whd6vT0hISEUZQAAAAAAgAgSktAiOztbM2bMCMVTAQAAAACAYYLlIQAAAAAAICwRWgAAAAAAgLBEaAEAAAAAAMJSSGZaPPPMM3r66ad16NAh2Ww25ebmauHChbr++uu1fPnyQV+3tLS0z9srKioGfW0AAAAACHcdHR1qaGhQW1ubXC6X2eUggtlsNsXHxys1NVWxsbFml+NhMQzDCNrFLZYT3ueiiy7SqlWrlJKSEpTrH1NSUqL8/PwBPwcAAAAAhBvDMFRRUaHGxkazS8EwlJKSory8vAG95y4tLVVBQYGkwL7/DmqnRXx8vC644AKtWLFCU6ZMUWJioqqrq7V69Wo99NBDqq2t1b/+9S9deOGFeu211xQdHR3McgAAAABgWKitrfULLKKiQtJIj2HK6XR6vm5sbJTdbldmZqaJFXULaqdFQ0ODUlNTe7zt6NGjOuecc7RlyxZJ0q9//Wt9+ctfHtD1+7M8ZMGCBZLotAAAAAAwPDgcDh04cMBznJ2drdTUVNlsNhOrQqRzuVxqaGhQVVWV59yECRNkt9v79fiI7LToLbCQpJycHD377LOaMmWKurq69OCDDw44tCCEAAAAADDStLS0eL7OyMhQRkaGidVguLDZbMrIyJDL5VJtba2k7tdaenq6qXWZunvI+PHjddZZZ0mS9u/fr/LycjPLAQAAAICw19ra6vk6OTnZxEowHB3/mjr+tWYW07c8nTZtmufrsrIyEysBAAAAgPDncDgkdW9MEBMTY3I1GG5iYmI8AziPvdbMZHpoMZBppAAAAAAw0rndbknd7fy8n0KgWSwWz3yUY681M5keWuzatcvz9ahRo0ysBAAAAAAAhBNTQ4vi4mK99tprkrqnko4ePdrMcgAAAAAAQBgJWmjx/PPPe+3z6uvo0aO69NJLPWtkvvCFLwSrFAAAAAAAEIGCtuXpl770JXV1denSSy/VaaedpsLCQsXFxammpkZvvfWW/vCHP6impkaStHjxYt12223BKgUAAAAAAESgoIUWklReXq4HH3xQDz74YK/3ufTSS/Xoo48y9RYAAAAAAHgJWmjxxz/+UatXr9a6det08OBB1dTUqKmpSYmJiSooKNDChQt13XXX6bTTTgtWCQAAAAAADMmqVat0ww03SOqey1hYWGhuQSNM0EKL008/XaeffnqwLg8AAAAAAIY507c8BQAAAAAA6AmhBQAAAAAACEuEFgAAAAAAICwRWoShji6XtpY06MkNR2QYhtnlAAAAAABgiqBueYqBaWhz6MqH39O+qha53N1hxZJJmcpPize5MgAAAABAb6qrq/XrX/9aL774ooqLi9XR0aHc3FwtWbJEN998sxYvXtzn49944w098sgjeu+991RZWSmLxaKsrCzl5eVp8eLFOvfcc3XGGWf4Pa6hoUG//e1v9cILL2jPnj1qaWlRamqqsrKyNHnyZK1cuVKXXHKJcnJygvWtBx2hRRhJiYtWRWOHJ7CQpB1lTYQWAAAAABCm/vvf/+qyyy5TU1OT1/nDhw/r8OHD+stf/qLbbrtNDzzwgKxW/8UOX/nKV/SrX/3K7/yRI0d05MgRrV+/XqtWrVJNTY3X7bt379aZZ56p8vJyr/M1NTWqqanR7t279a9//Usul0tf/OIXh/6NmoTQIoxYLBbNGJ2stftrPed2ljfqEzNyTawKAAAAANCTrVu36vzzz5fD4VB0dLS++MUv6oILLlBCQoK2bNmie++9V8XFxfrtb3+rhIQE/fSnP/V6/AsvvOAJLE466STdeuutmjp1qlJSUtTQ0KCdO3fqf//7nzZs2OD33Ndcc43Ky8sVHR2tm266Seecc45yc3PldrtVWlqq9957T//85z9D8dsQVIQWYWbGqBSv0GJHWaOJ1QAAAACIRG63ofo2h9llhExavF1WqyXkz/v5z39eDodDNptNL7zwglauXOm5bf78+brsssu0ePFi7dq1S7/4xS907bXXavr06Z77PP3005KksWPHau3atUpMTPS6/rJly3Tbbbeprq7O6/zBgwe1adMmSdIvf/lLv06KBQsW6JJLLtFPf/pTNTQ0BPJbDjlCizAzfXSK1/GO8qZe7gkAAAAAPatvc2juD/9ndhkhs+n/zlRGYkxIn3PDhg16//33JUk33XSTV2BxTFpamh5++GEtXrxYbrdbv/vd7/Tb3/7Wc3tlZaUkac6cOX6BxfHS09O9jo89TpKWLl3a6+MsFovS0tL69w2FKXYPCTMzRiV7HVc3d6qqqcOkagAAAAAAPfnf/z4OhW688cZe77do0SJNnTrV7zGSlJeXJ0l6++23deDAgX4/97HHSdKqVav6/bhIRGgRZgozEpRgt3md21HOEhEAAAAACCc7duyQJNntds2aNavP+55yyimSpH379snh+HjZzrXXXitJqq2t1YwZM3TllVfqiSee0P79+/u83rhx47RkyRJJ0v3336/p06frrrvu0htvvKG2trbBfkthidAizFitFk0f5bNEpIwlIgAAAAAQTo7NmUhPT1dUVN+TF3JzuzdXMAxD9fX1nvMrVqzQb37zG8XFxamjo0N///vf9dnPflaTJk1Sfn6+brnlFm3btq3Haz755JM67bTTJEm7du3SD37wA61YsUKpqalaunSpHnroIXV0RH7XPjMtwtD00cnacOjjQSsM4wQAAAAwEGnxdm36vzPNLiNk0uLtpj23xTK0AaC33XabLrvsMv3tb3/Ta6+9prVr16qxsVFlZWX6wx/+oIcffljf+c539MMf/tDrcaNHj9a7776r119/Xc8995xWr16tXbt2qaurS2vWrNGaNWv0i1/8Qi+99JKKioqGVKOZCC3C0AyfToudDOMEAAAAMABWqyXkgylHmmPDMWtra+V0Ovvstjg2OLO3wZjZ2dm64447dMcdd8jtdmvr1q365z//qd/85jdqaGjQj370I82fP18XXnih32NXrFihFStWeGr53//+p4cfflhvvPGGDhw4oCuuuEJbtmwJxLdsCpaHhKEZPjuIlDW0q6515GxXBAAAAADhbsaMGZIkh8OhrVu39nnfDRs2SJImTZoku73vrhCr1ao5c+boBz/4gV5//XXP+WPbo/YlIyNDV1xxhV5//XVdcMEFkqStW7dq3759J3xsuCK0CEMTshIUE+X9R7OTYZwAAAAAEDbOPPPj5TePP/54r/dbt26ddu3a5feY/pgzZ46nM6OmpmZAjz3WfTGYx4YTQoswFGWzamqe99anDOMEAAAAgPCxYMECzZs3T5L0yCOPeHVFHNPY2Kibb75ZUncHxa233up1+9///ne1t7f3+hwbN270DO4cN26c5/zWrVv77O4wDMOzvarFYlFhYWG/vqdwxEyLMDVjdLK2ljR4jtn2FAAAAADCyyOPPKJTTjlFDodD5557rr70pS/p/PPPV0JCgrZs2aJ7771XBw8elCR97Wtf8ywpOeab3/ymbrnlFl144YVaunSpioqKlJCQoNraWr3zzjt68MEHJUk2m02f+9znPI/bunWrbrjhBs2fP1/nn3++5syZo9zcXHV1dam4uFhPPPGEXnvtNUnSBRdcoLy8vBD9jgQeoUWYmukz12InO4gAAAAAQFiZNWuWnn/+eV122WVqamrSfffdp/vuu8/vfrfddpt+8pOf9HiNhoYG/fGPf9Qf//jHHm+PiYnRQw895OnqON7777+v999/v9f6Fi5cqMcee6yf3014IrQIU9N9dhA5VNumpo4uJcdGm1QRAAAAAMDXypUrtX//fv3qV7/SSy+9pIMHD6qzs1M5OTlasmSJbrnlFi1evLjHx7755pt6/vnn9fbbb2vv3r2qrKxUfX294uPjNWHCBK1YsUK33nqrxo8f7/W4q666Sjk5OXrttdf0/vvvq6ysTEePHpXT6VR2drbmzJmjK664QldeeaWs1sieCmExDMMwu4hgKS0tVUFBgSSppKRE+fn5JlfUfw6nW9O/94q6XB//8Tx506k6bUKGiVUBAAAAMNu+ffs8W2xOmjTJ7HIwDA3mNRas99+RHbkMY/YoqybnJnmdW3ew1qRqAAAAAAAIPUKLMHbaeO+uijf3VJlUCQAAAAAAoUdoEcbOmJLjdby9rFFHmzpMqgYAAAAAgNAitAhj8wrTlBTrPSuVbgsAAAAAwEhBaBHGom1WLS3K8jr3BqEFAAAAAGCEILQIc2dMzvY6fmd/jTqdLpOqAQAAAAAgdAgtwtyyyVmyWD4+bnO4tP5gnXkFAQAAAAAQIoQWYS4jMUazClK9zrFEBAAAAAAwEhBaRIAVU7yXiLy+56gMwzCpGgAAAAAAQoPQIgIs9wktSuradaC6xaRqAAAAAAAIDUKLCDAtL1m5ybFe59bsqzGpGgAAAAAAQoPQIgJYLBYtnpTpdW7j4XqTqgEAAABgJpvNJklyuVxyu90mV4Phxu12y+Xq3rHy2GvNTIQWEWLe2DSv402H6plrAQAAAIxAsbHdXdiGYailhWXjCKyWlhbPe824uDiTqyG0iBjzCr1Di8qmDpU1tJtUDQAAAACzJCcne76urKxUU1MTHRcYMrfbraamJlVWVnrOJSUlmVhRtyizC0D/jM9MVGp8tBraujznNh2uV35avIlVAQAAAAi1hIQExcXFqb29XS6XS2VlZbJYLGHRyo/I5XK5vLr54+LilJCQYGJF3QgtIoTVatHcMWl6fU+V59zGQ/W6cNZoE6sCAAAAEGoWi0VjxozRkSNH1N7e3X1tGIacTqfJlWG4iIuL05gxY2SxWMwuhdAikswt9AktGMYJAAAAjEhWq1Vjx45Va2urmpubPV0XwGDZbDbFxcUpKSlJCQkJYRFYSIQWEWXe2HSv4w8rm9Tc0aWk2GiTKgIAAABgFovFosTERCUmJppdChA0DOKMICflpyja9nHa5TakLUcazCsIAAAAAIAgIrSIILHRNs0YneJ1jiUiAAAAAIDhitAiwswb67316abDdSZVAgAAAABAcBFaRJi5PnMtthxpkNPFnswAAAAAgOGH0CLCzPXptGhzuLSnstmkagAAAAAACB5CiwiTlRSjwox4r3Pri1kiAgAAAAAYfggtItD8Qu8lIusO1JpUCQAAAAAAwUNoEYEWTszwOl5/sJa5FgAAAACAYYfQIgItnJDpddzc6dTO8iaTqgEAAAAAIDgILSJQTnKsJmQleJ17lyUiAAAAAIBhhtAiQvl2W7x7oMakSgAAAAAACA5Ciwi1cIL3XIv3D9XJ4WSuBQAAAABg+CC0iFCnjvcOLTq63Npa0mBOMQAAAAAABAGhRYRKS7BrWl6y17m1+1kiAgAAAAAYPggtIpjvEpF1DOMEAAAAAAwjhBYRbOFE79BiS0m92hxOk6oBAAAAACCwCC0i2PzCdNmsFs9xl8vQ+4fqTawIAAAAAIDAIbSIYEmx0To5P8XrHHMtAAAAAADDBaFFhFs8MdPr+O291SZVAgAAAABAYBFaRLilRVlex3sqm3W0qcOkagAAAAAACBxCiwg3qyBVSbFRXufotgAAAAAADAeEFhEuymbVogk+S0T2MdcCAAAAABD5CC2GgSVF3qHFO/uq5XYbJlUDAAAAAEBgEFoMA0snec+1qG/r0o7yRpOqAQAAAAAgMAgthoGC9HiNz0zwOsdcCwAAAABApCO0GCZ8dxF5ey9zLQAAAAAAkY3QYphY6jPXYvORejV3dJlUDQAAAAAAQ0doMUycMi5D0TaL59jpNrTuQK2JFQEAAAAAMDSEFsNEQkyU5o1N9zr39j7mWgAAAAAAIhehxTDCXAsAAAAAwHBCaDGM+M61OFLXpkM1rSZVAwAAAADA0BBaDCNTc5OVmRjjdW4NS0QAAAAAABGK0GIYsVotWjrJu9tiNUtEAAAAAAARitBimFnis0Rk3YEaOZxuk6oBAAAAAGDwCC2GmSWTvIdxtjpc2nyk3qRqAAAAAAAYPEKLYSYzMUbTRyV7nXt7L3MtAAAAAACRh9BiGPLd+nTNPuZaAAAAAAAiD6HFMLTUZ4nIjvJG1bZ0mlQNAAAAAACDQ2gxDM0dm6Z4u81zbBjSO/vptgAAAAAARBZCi2HIHmXVqeMzvM5tOdJgTjEAAAAAAAwSocUwNWdMqtfx9rJGcwoBAAAAAGCQCC2GqRmjU7yOd5Y3yulym1QNAAAAAAADR2gxTM30CS06utw6UN1qUjUAAAAAAAwcocUwlZEYo9GpcV7nPihtMKcYAAAAAAAGgdBiGPPtttjBXAsAAAAAQAQhtBjGZuZ7hxYfEFoAAAAAACIIocUw5ttpsau8iWGcAAAAAICIQWgxjPmGFp1Ot/ZVtZhUDQAAAAAAA0NoMYylJdiVn+Y9jHN7KUtEAAAAAACRgdBimPPtttjOXAsAAAAAQIQgtBjmGMYJAAAAAIhUhBbDnG+nxe6KJnUxjBMAAAAAEAEILYY539DC4XRr79Fmk6oBAAAAAKD/CC2GudR4uwrSGcYJAAAAAIg8hBYjgG+3xYd0WgAAAAAAIgChxQhQlJPkdbzvaItJlQAAAAAA0H+EFiPApGyf0KKKTgsAAAAAQPgjtBgBJuUkeh0fbepUY3uXSdUAAAAAANA/hBYjQGFGgqKsFq9z++m2AAAAAACEOUKLEcAeZVVhZoLXOeZaAAAAAADCHaHFCDEp23uJyL4qQgsAAAAAQHgjtBghJvnsILKXbU8BAAAAAGGO0GKE8O202E+nBQAAAAAgzBFajBC+O4hUNHaouYMdRAAAAAAA4YvQYoQYl5kgm98OInRbAAAAAADCF6HFCBETZdPYjHivc+wgAgAAAAAIZ4QWI4j/DiIM4wQAAAAAhC9CixGkyGcHEbY9BQAAAACEM0KLEWSib6cFy0MAAAAAAGGM0GIEmZTt3WlR1tCulk6nSdUAAAAAANA3QosRZHxWgnw2ENEBlogAAAAAAMIUocUIEhtt09iMBK9ze48yjBMAAAAAEJ4ILUYY37kWB2taTaoEAAAAAIC+EVqMMOMyvTstjtS1mVQJAAAAAAB9I7QYYQrS472Oj9QSWgAAAAAAwhOhxQgz1je0oNMCAAAAABCmCC1GmDE+oUVje5ca27pMqgYAAAAAgN4RWowwo9Pi/LY9pdsCAAAAABCOTAstvvnNb8pisXh+vfXWW2aVMqJE26walRrnde5wHTuIAAAAAADCjymhxdatW/XLX/7SjKeGpLEZzLUAAAAAAIS/kIcWbrdbn//85+V0OpWdnR3qp4f851qwgwgAAAAAIByFPLR44IEH9P7772vKlCm68cYbQ/30UA/bntJpAQAAAAAIQyENLY4cOaLvfve7kqSHHnpIdrs9lE+Pj4xNT/A6PkynBQAAAAAgDIU0tLjtttvU0tKi6667TqeffnoonxrH8V0eUtHYLofTbVI1AAAAAAD0LGShxdNPP60XXnhB6enp+sUvfhGqp0UPxvgM4nQbUllDu0nVAAAAAADQs6hQPElDQ4Nuv/12SdJPf/pTZWZmBuS6paWlfd5eUVERkOcZblLiopUSF63G9i7PuSN1bRqXmdDHowAAAAAACK2QhBbf+MY3VFlZqUWLFgV0+GZBQUHArjXSjM2I1weljZ7jI7WtkrLMKwgAAAAAAB9BXx6yZs0aPfroo4qKitJDDz0ki8US7KdEP7CDCAAAAAAg3AW108LhcOjzn/+8DMPQV77yFc2YMSOg1y8pKenz9oqKCi1YsCCgzzlcjCW0AAAAAACEuaCGFj/+8Y+1Z88ejRkzRt/73vcCfv38/PyAX3Ok8N1BhG1PAQAAAADhJmjLQ/bs2aOf/OQnkqQHH3xQCQkMeQwnvjuIlNS1yTAMk6oBAAAAAMBf0Dot7r//fjkcDo0fP15tbW166qmn/O6zY8cOz9dvvPGGKisrJUnnn38+IUeQ+XZatDpcqm11KDMxxqSKAAAAAADwFrTQorOzU5J08OBBXXXVVSe8/w9+8APP18XFxYQWQZaXEqdom0Vdro+7K47UtRFaAAAAAADCRtB3D0F4slktyk/zGcbJXAsAAAAAQBgJWmixatUqGYbR56/jh3O++eabnvOFhYXBKgvH8V0iwg4iAAAAAIBwQqfFCDY6Lc7ruKKxw6RKAAAAAADwR2gxguUmx3odH20itAAAAAAAhA9CixHMN7SopNMCAAAAABBGCC1GsJwUOi0AAAAAAOHL1NDi7rvv9gzfXLZsmZmljEi+nRa1rQ51Ol0mVQMAAAAAgDc6LUYw39BCkqqaOk2oBAAAAAAAf4QWI1hyXJRio71fAiwRAQAAAACEC0KLEcxisfgP4yS0AAAAAACECUKLES6HHUQAAAAAAGGK0GKEy2UHEQAAAABAmCK0GOH8l4cwiBMAAAAAEB4ILUY43+UhR1keAgAAAAAIE4QWI5zv8hAGcQIAAAAAwgWhxQjnN4izqUOGYZhUDQAAAAAAHyO0GOF8Oy0cTrca2rpMqgYAAAAAgI8RWoxw2Ukxsli8z7FEBAAAAAAQDggtRrhom1UZCTFe5wgtAAAAAADhgNACyk3xDi3YQQQAAAAAEA4ILaDcHoZxAgAAAABgNkIL+O0gcpTQAgAAAAAQBggt4N9pwfIQAAAAAEAYILSAcny2Pa0gtAAAAAAAhAFCC/h1WrA8BAAAAAAQDggtoFyfTov6ti51dLlMqgYAAAAAgG6EFvAbxClJVU2dJlQCAAAAAMDHCC2g5NgoxUXbvM6x7SkAAAAAwGyEFpDFYvFbIkJoAQAAAAAwG6EFJEk5yTFex0fZQQQAAAAAYDJCC0jy30GETgsAAAAAgNkILSBJyvFZHlLVzCBOAAAAAIC5CC0gScpJ8g4tjtJpAQAAAAAwGaEFJPlve1pFaAEAAAAAMBmhBST1MIizqVOGYZhUDQAAAAAAhBb4iG+nRXuXS82dTpOqAQAAAACA0AIfyUqK8TvHEhEAAAAAgJkILSBJio22KTU+2uvc0SZ2EAEAAAAAmIfQAh7sIAIAAAAACCeEFvDI7mEYJwAAAAAAZiG0gIfvME46LQAAAAAAZiK0gIfvtqdVzYQWAAAAAADzEFrAw7/TguUhAAAAAADzEFrAI5tBnAAAAACAMEJoAQ+/5SFNnTIMw6RqAAAAAAAjHaEFPHyXhzhcbjW0dZlUDQAAAABgpCO0gEdWUozfuaMM4wQAAAAAmITQAh7RNqsyE+1e5xjGCQAAAAAwC6EFvDCMEwAAAAAQLggt4MV/GCehBQAAAADAHIQW8OI7jJPlIQAAAAAAsxBawEu2X2hBpwUAAAAAwByEFvDiuzzkaDOdFgAAAAAAcxBawEuOzyBOZloAAAAAAMxCaAEvvjMtqpo75XYbJlUDAAAAABjJCC3gxXd5iMttqLbVYVI1AAAAAICRjNACXjISY2S1eJ9jGCcAAAAAwAyEFvBis1qUleTdbVHVTGgBAAAAAAg9Qgv48Z1rcbSJHUQAAAAAAKFHaAE/2Um+oQWdFgAAAACA0CO0gB/fYZx0WgAAAAAAzEBoAT9+257SaQEAAAAAMAGhBfz4dVowiBMAAAAAYAJCC/jJZhAnAAAAACAMEFrAT47PIM6alk45XW6TqgEAAAAAjFSEFvDjuzzEMKSaFodJ1QAAAAAARipCC/hJi7cr2mbxOse2pwAAAACAUCO0gB+r1aLsJN+5FoQWAAAAAIDQIrRAj7L9dhBhGCcAAAAAILQILdAj32GcVXRaAAAAAABCjNACPfIdxsnyEAAAAABAqBFaoEfZyb4zLVgeAgAAAAAILUIL9CjHL7Sg0wIAAAAAEFqEFuiR7/KQKgZxAgAAAABCjNACPfLttKhrdajT6TKpGgAAAADASERogR757h4iSdV0WwAAAAAAQojQAj1KjotSTJT3y4NhnAAAAACAUCK0QI8sFovfEpEqhnECAAAAAEKI0AK98h3GyQ4iAAAAAIBQIrRAr7J9tz1lpgUAAAAAIIQILdAr32GcdFoAAAAAAEKJ0AK98l0eUsUgTgAAAABACBFaoFe+gzjptAAAAAAAhBKhBXqV7dNpUUloAQAAAAAIIUIL9Mq306K5w6k2h9OkagAAAAAAIw2hBXrlG1pIzLUAAAAAAIQOoQV6lRgTpQS7zesccy0AAAAAAKFCaIE++Q3jbKbTAgAAAAAQGoQW6JPvMM4qOi0AAAAAACFCaIE+se0pAAAAAMAshBboU1aid6dFbYvDpEoAAAAAACMNoQX6lOETWtS0EloAAAAAAEKD0AJ9yki0ex3XtjCIEwAAAAAQGoQW6FOmX2hBpwUAAAAAIDQILdCnjASfmRatnTIMw6RqAAAAAAAjCaEF+uS7PKTLZaipw2lSNQAAAACAkYTQAn3y7bSQmGsBAAAAAAgNQgv0Kc5uU4Ld5nWulh1EAAAAAAAhQGiBE/Ld9pROCwAAAABAKBBa4IR851rUsIMIAAAAACAECC1wQn47iBBaAAAAAABCgNACJ5Tp02lR18ryEAAAAABA8BFa4IT8locwiBMAAAAAEAKEFjgh/+UhdFoAAAAAAIKP0AIn5NtpwUwLAAAAAEAoEFrghDJ9tzxleQgAAAAAIAQILXBCvp0W9W0OOV1uk6oBAAAAAIwUhBY4Id+ZFoYh1bd1mVQNAAAAAGCkILTACaXFR8ti8T5Xy7anAAAAAIAgI7TACUXZrEqLZxgnAAAAACC0CC3QLxkJ3qFFDdueAgAAAACCjNAC/cK2pwAAAACAUCO0QL9k+G17SqcFAAAAACC4CC3QL77LQ+i0AAAAAAAEG6EF+sV329MaQgsAAAAAQJARWqBf/GZasDwEAAAAABBkhBbol0wGcQIAAAAAQozQAv3iN4iTLU8BAAAAAEFGaIF+8R3E2epwqd3hMqkaAAAAAMBIQGiBfvHttJCYawEAAAAACC5CC/RLcmyUom0Wr3PMtQAAAAAABBOhBfrFYrH4bXtKpwUAAAAAIJgILdBvvtue1tBpAQAAAAAIIkIL9Jv/DiKEFgAAAACA4IkK1oWbmpr00ksv6f3339fGjRtVVlam6upqtbe3KzU1VdOmTdO5556rG2+8URkZGcEqAwGU6bODCNueAgAAAACCKWihxYYNG3TVVVf1eFt1dbVWr16t1atX6+c//7n+8pe/6Oyzzw5WKQgQ3+Uhta10WgAAAAAAgidooYUkFRQUaPny5Zo7d64KCgqUl5cnt9ut0tJSPfvss3ruuedUU1OjCy64QBs2bNDJJ58czHIwRL7LQ2rotAAAAAAABFHQQovly5fryJEjvd5++eWX61//+pcuvvhiORwO3XPPPXruueeCVQ4CIMNveQidFgAAAACA4AnaIE6bzXbC+1x00UWaPHmyJGnNmjXBKgUBkuk7iJMtTwEAAAAAQWT67iFJSUmSpI6ODpMrwYn4zbRoccgwDJOqAQAAAAAMd6aGFh9++KG2bt0qSZoyZYqZpaAffGdaON2GmtqdJlUDAAAAABjugjqIsydtbW0qKyvT888/r5/97GdyOrvf9N5xxx0DvlZpaWmft1dUVAymRPTCd6aFJNW0diolPtqEagAAAAAAw11IQotVq1bphhtu6PX2b33rW/r0pz894OsWFBQMpSwMUGy0TYkxUWrp/Li7oq7VoQlZJhYFAAAAABi2Qt5pcbxZs2bp4Ycf1vz5880sAwOQkWj3Ci1q2fYUAAAAABAkIQktLrroIs2bN0+S1N7ergMHDujpp5/WP//5T1111VX61a9+pfPOO2/A1y0pKenz9oqKCi1YsGBQNaNnGQl2Ha5t8xzXsO0pAAAAACBIQhJapKamKjU11XM8f/58XXnllfrzn/+s6667ThdeeKEee+wxXX/99QO6bn5+fmALxQn5DuOsJbQAAAAAAASJqbuHXHPNNbrsssvkdrv1xS9+UXV1dWaWg37I9N32tJXlIQAAAACA4DA1tJCkCy+8UJLU2tqqV155xeRqcCIZCXRaAAAAAABCw/TQIivr460nDh8+bGIl6I8Mn06LGgZxAgAAAACCxPTQoqyszPN1YmKiiZWgP/xmWrTSaQEAAAAACA7TQ4tnnnnG8/XMmTNNrAT9kZngM9OCTgsAAAAAQJAELbRYtWqVOjo6+rzP/fffr5deekmSNG7cOC1ZsiRY5SBAfDst6tu65HS5TaoGAAAAADCcBW3L07vvvlt33nmnLr30Ui1evFgTJkxQYmKimpubtX37dv31r3/V2rVrJUl2u10PP/ywbDZbsMpBgPjOtJCkujaHspNiTagGAAAAADCcBS20kKS6ujo98sgjeuSRR3q9T35+vh5//HGdeeaZwSwFAZIWb5fFIhnGx+dqWwgtAAAAAACBF7TQ4tVXX9WLL76otWvXav/+/Tp69Khqa2sVFxen7OxszZo1S+edd54uv/xyxcfHB6sMBJjNalFavF11xw3gZNtTAAAAAEAwBC20mDx5siZPnqyvfvWrwXoKmCQjwSe0aGUYJwAAAAAg8EzfPQSRx3euRQ2dFgAAAACAICC0wID57iDCtqcAAAAAgGAgtMCAZSZ4d1ow0wIAAAAAEAyEFhgwv04LZloAAAAAAIKA0AIDxkwLAAAAAEAoEFpgwDIS6LQAAAAAAAQfoQUGLDORmRYAAAAAgOAjtMCA+c60aHO41OZwmlQNAAAAAGC4IrTAgPnOtJDotgAAAAAABB6hBQYsKSZKdpv3S6e2ldACAAAAABBYhBYYMIvF4tdtUdvCME4AAAAAQGARWmBQ/EMLOi0AAAAAAIFFaIFB8d32tIZtTwEAAAAAAUZogUGh0wIAAAAAEGyEFhiUjATv0KKOQZwAAAAAgAAjtMCgpPsuD2EQJwAAAAAgwAgtMCh0WgAAAAAAgo3QAoPiO9OC0AIAAAAAEGiEFhiUdJ9Oi9pWhwzDMKkaAAAAAMBwRGiBQfHd8tThdKvV4TKpGgAAAADAcERogUFJ91keIkl1bHsKAAAAAAggQgsMSoLdJnuU98untpUdRAAAAAAAgUNogUGxWCx+O4jU0mkBAAAAAAggQgsMmu8wTnYQAQAAAAAEEqEFBq2nHUQAAAAAAAgUQgsMmu/ykDpmWgAAAAAAAojQAoOWkei97SmdFgAAAACAQCK0wKAx0wIAAAAAEEyEFhg0/+UhhBYAAAAAgMAhtMCg+Q3iZMtTAAAAAEAAEVpg0DISfXcPYRAnAAAAACBwCC0waOkJ3oM4O7rcanM4TaoGAAAAADDcEFpg0HyXh0gsEQEAAAAABA6hBQYtOTZK0TaL1zmGcQIAAAAAAoXQAoNmsViUFs8OIgAAAACA4CC0wJBkJHrPtagltAAAAAAABAihBYYkI8G304IdRAAAAAAAgUFogSHxHcZJpwUAAAAAIFAILTAkfqEFu4cAAAAAAAKE0AJD4r88hNACAAAAABAYhBYYkvRElocAAAAAAIKD0AJDwiBOAAAAAECwEFpgSNITvLc8rWOmBQAAAAAgQAgtMCS+gzhbHS51dLlMqgYAAAAAMJwQWmBIMn1mWkgM4wQAAAAABAahBYYkOTZaNqvF6xzbngIAAAAAAoHQAkNitVqUFu+7gwjDOAEAAAAAQ0dogSHz30GETgsAAAAAwNARWmDIfIdxEloAAAAAAAKB0AJDlp7ouzyE0AIAAAAAMHSEFhgyv+UhDOIEAAAAAAQAoQWGzG95SBuhBQAAAABg6AgtMGS+u4c0EFoAAAAAAAKA0AJDlsYgTgAAAABAEBBaYMjSfTot6tu6TKoEAAAAADCcEFpgyNISor2OG9occrsNk6oBAAAAAAwXhBYYMt+ZFm5Dauqg2wIAAAAAMDSEFhgy39BCYq4FAAAAAGDoCC0wZHF2m+KibV7n6tlBBAAAAAAwRIQWCIh0nx1E6ltZHgIAAAAAGBpCCwREarz3MM46Oi0AAAAAAENEaIGA8O+0ILQAAAAAAAwNoQUCwncYJ50WAAAAAIChIrRAQPh2WjQw0wIAAAAAMESEFggIZloAAAAAAAKN0AIBwUwLAAAAAECgEVogIJhpAQAAAAAINEILBITfTIs2ZloAAAAAAIaG0AIB4TvToqHNIZfbMKkaAAAAAMBwQGiBgPDttHAbUlM73RYAAAAAgMEjtEBA+M60kJhrAQAAAAAYGkILBERstE3xdpvXuQZCCwAAAADAEBBaIGD8dhBpZXkIAAAAAGDwCC0QMGkJ3sM461vptAAAAAAADB6hBQLGt9OinuUhAAAAAIAhILRAwPjuIMIgTgAAAADAUBBaIGD8Oi1YHgIAAAAAGAJCCwQMgzgBAAAAAIFEaIGASfcZxMmWpwAAAACAoSC0QMCkMdMCAAAAABBAhBYIGGZaAAAAAAACidACAeMbWjS0d8nlNkyqBgAAAAAQ6QgtEDC+W54ahtTUzjBOAAAAAMDgEFogYFLjo/3OMdcCAAAAADBYhBYImNhom+LtNq9zzLUAAAAAAAwWoQUCyneuRR2hBQAAAABgkAgtEFC+cy0a2phpAQAAAAAYHEILBFSaT2jBTAsAAAAAwGARWiCg0nyGcTLTAgAAAAAwWIQWCChmWgAAAAAAAoXQAgHlG1o0tDPTAgAAAAAwOIQWCKi0BO/lIQ3MtAAAAAAADBKhBQIqJc5npgW7hwAAAAAABonQAgHltzyE0AIAAAAAMEiEFggo/9DCIcMwTKoGAAAAABDJCC0QUKk+W5463YZaOp0mVQMAAAAAiGSEFggo39BCYokIAAAAAGBwCC0QUIkxUYqyWrzOEVoAAAAAAAaD0AIBZbFY/Lot6tn2FAAAAAAwCIQWCLhU32Gc7XRaAAAAAAAGjtACAZca591p0UCnBQAAAABgEAgtEHB+nRbMtAAAAAAADAKhBQIujZkWAAAAAIAAILRAwPkO4qTTAgAAAAAwGIQWCDj/5SF0WgAAAAAABo7QAgGX5hNa1NNpAQAAAAAYBEILBJz/8hA6LQAAAAAAA0dogYDzCy3a6bQAAAAAAAwcoQUCznd5SGN7l1xuw6RqAAAAAACRitACAefbaWEYUhPdFgAAAACAASK0QMD5dlpILBEBAAAAAAwcoQUCLjbappgo75dWPcM4AQAAAAADRGiBoPCba8G2pwAAAACAASK0QFD4zrWg0wIAAAAAMFCEFggK/9CCTgsAAAAAwMAQWiAo/JeH0GkBAAAAABgYQgsERapPaEGnBQAAAABgoIIaWmzcuFHf//73tXLlSuXn5ysmJkaJiYkqKirSDTfcoHfeeSeYTw8TMdMCAAAAADBUUcG68NKlS7VmzRq/8w6HQ/v27dO+ffu0atUqXXvttXrkkUdkt9t7uAoiVZpPaNHYTqcFAAAAAGBgghZalJeXS5JGjRqlyy67TEuWLNGYMWPkcrm0bt063XfffSorK9Of/vQndXV16W9/+1uwSoEJ/JeH0GkBAAAAABiYoIUWU6ZM0Y9//GNdeumlstlsXredeuqpuuaaa7Ro0SLt3btXTz75pG655RYtXbo0WOUgxFLjfJaHtNJpAQAAAAAYmKDNtHjhhRd0+eWX+wUWx2RmZuq+++7zHD/77LPBKgUmSEvw2T2E5SEAAAAAgAEydfeQ5cuXe74+cOCAiZUg0HxnWrR0OuVwuk2qBgAAAAAQiUwNLTo7Oz1f99aRgciUEuc/WJVuCwAAAADAQARtpkV/rF692vP11KlTB/z40tLSPm+vqKgY8DURGL5bnkpSQ5tDWUkxJlQDAAAAAIhEpoUWbrdb9957r+f48ssvH/A1CgoKAlkSAijaZlViTJRaOp2ec/VtdFoAAAAAAPrPtOUh999/vzZs2CBJuuSSSzR37lyzSkGQ+HZbNLDtKQAAAABgAEzptFi9erW+9a1vSZKys7P1+9//flDXKSkp6fP2iooKLViwYFDXxtClxdtVWt/uOW6g0wIAAAAAMAAhDy127typiy++WE6nU7GxsXrmmWeUnZ09qGvl5+cHuDoEkm+nRT2dFgAAAACAAQjp8pDi4mKtXLlS9fX1stlseuqpp7R06dJQloAQSo333kGkgd1DAAAAAAADELLQory8XGeeeabKy8tlsVj0+OOP68ILLwzV08MEacy0AAAAAAAMQUhCi5qaGp111lk6ePCgJOnBBx/UtddeG4qnhonSfDotalsILQAAAAAA/Rf00KKxsVFnn322du3aJUm69957ddtttwX7aREGspJivI6rmjtNqgQAAAAAEImCGlq0tbXpk5/8pDZv3ixJ+n//7//pm9/8ZjCfEmHEN7SoJrQAAAAAAAxA0EILh8Ohiy++WGvXrpUk3X777frhD38YrKdDGPILLVo6ZRiGSdUAAAAAACJN0LY8veqqq/Tf//5XknTGGWfoxhtv1I4dO3q9v91uV1FRUbDKgQmyfUILh9Otpg6nUuKie3kEAAAAAAAfC1po8dxzz3m+fuONN3TSSSf1ef+xY8fq0KFDwSoHJshMjPE7V93cQWgBAAAAAOiXkG15ipEnNtqm5FjvXIxhnAAAAACA/gpapwWzCyBJ2cmxaupo8RwzjBMAAAAA0F90WiCoshLZQQQAAAAAMDiEFggqtj0FAAAAAAwWoQWCitACAAAAADBYhBYIKt9tTxnECQAAAADoL0ILBBWdFgAAAACAwSK0QFD5hRYthBYAAAAAgP4htEBQZSfFeh3XtTrU5XKbVA0AAAAAIJIQWiCofDstJKmGbgsAAAAAQD8QWiCoUuOiFWW1eJ1jrgUAAAAAoD8ILRBUVqtFmYkM4wQAAAAADByhBYIuO5ltTwEAAAAAA0dogaDLotMCAAAAADAIhBYIOr9tTwktAAAAAAD9QGiBoMtO8l0e0mFSJQAAAACASEJogaCj0wIAAAAAMBiEFgg6v9CihdACAAAAAHBihBYIuqykWK/j6uZOGYZhUjUAAAAAgEhBaIGg851p0dHlVnOn06RqAAAAAACRgtACQZfps+WpxFwLAAAAAMCJEVog6OLsNiXFRHmdI7QAAAAAAJwIoQVCIivZd9tTQgsAAAAAQN8ILRASWYlsewoAAAAAGBhCC4SE37anhBYAAAAAgBMgtEBIZPtse1rV3GFSJQAAAACASEFogZDISLR7Hde3OkyqBAAAAAAQKQgtEBLpCd6hRV1bl0mVAAAAAAAiBaEFQiItPtrrmE4LAAAAAMCJEFogJNLifZaHtBFaAAAAAAD6RmiBkEjzWR7S3OFUl8ttUjUAAAAAgEhAaIGQ8O20kKQG5loAAAAAAPpAaIGQSPWZaSGxRAQAAAAA0DdCC4REtM2qpNgor3MM4wQAAAAA9IXQAiHju+0pnRYAAAAAgL4QWiBkUn3mWtS1MtMCAAAAANA7QguETLrPXAs6LQAAAAAAfSG0QMj47iDCTAsAAAAAQF8ILRAyaX4zLVgeAgAAAADoHaEFQoZBnAAAAACAgSC0QMikMtMCAAAAADAAhBYImXRmWgAAAAAABoDQAiHjv+UpoQUAAAAAoHeEFggZ35kWTR1OOV1uk6oBAAAAAIQ7QguETJrPTAtJamhnBxEAAAAAQM8ILRAyvstDJKmBYZwAAAAAgF4QWiBk7FFWJcVEeZ2ra6XTAgAAAADQM0ILhFRqAtueAgAAAAD6h9ACIcW2pwAAAACA/iK0QEj5bXtKpwUAAAAAoBeEFggp321PG9qYaQEAAAAA6BmhBUIq1Wfb0zqWhwAAAAAAekFogZDynWnBlqcAAAAAgN4QWiCk0nyWh9BpAQAAAADoDaEFQirNd/cQZloAAAAAAHpBaIGQSkvwnmlRz/IQAAAAAEAvCC0QUr6dFo3tXXK5DZOqAQAAAACEM0ILhJTvlqeG0R1cAAAAAADgi9ACIeW75anEME4AAAAAQM8ILRBSMVE2JdhtXufY9hQAAAAA0BNCC4Qc254CAAAAAPqD0AIh57/tKaEFAAAAAMAfoQVCzrfTor6NQZwAAAAAAH+EFgi5NJ9hnPUsDwEAAAAA9IDQAiHH8hAAAAAAQH8QWiDk0hnECQAAAADoB0ILhFxGondoUd1CaAEAAAAA8EdogZDLTIzxOq5p7jSpEgAAAABAOCO0QMj5hRYtnTIMw6RqAAAAAADhitACIZflE1p0Ot1q6XSaVA0AAAAAIFwRWiDkMpPsfudqmGsBAAAAAPBBaIGQi7dHKcFu8zpX08JcCwAAAACAN0ILmCIzyXuJSDXDOAEAAAAAPggtYIqehnECAAAAAHA8QguYIjPRe64F254CAAAAAHwRWsAUvp0W1QziBAAAAAD4ILSAKVgeAgAAAAA4EUILmMJ3ECehBQAAAADAF6EFTJHlO9OC0AIAAAAA4IPQAqbwWx7SzEwLAAAAAIA3QguYwje0aO9yqbXTaVI1AAAAAIBwRGgBU/jOtJBYIgIAAAAA8EZoAVMk2G2KjfZ++RFaAAAAAACOR2gBU1gsFr8lItXMtQAAAAAAHIfQAqbxG8ZJpwUAAAAA4DiEFjANoQUAAAAAoC+EFjBNVpLd65jQAgAAAABwPEILmMav04KZFgAAAACA4xBawDQsDwEAAAAA9IXQAqYhtAAAAAAA9IXQAqbJTPSdacHyEAAAAADAxwgtYJrMJO9Oi5ZOpzq6XCZVAwAAAAAIN4QWME2WT2ghSdXNLBEBAAAAAHQjtIBpkmKiZI/yfgky1wIAAAAAcAyhBUxjsViU5TOMk04LAAAAAMAxhBYwFcM4AQAAAAC9IbSAqdj2FAAAAADQG0ILmIrQAgAAAADQG0ILmCozyXt5CDMtAAAAAADHEFrAVDnJsV7HFY0dJlUCAAAAAAg3hBYw1ejUOK/jsoZ2kyoBAAAAAIQbQguYanSad2hR3dypji6XSdUAAAAAAMIJoQVM5dtpIbFEBAAAAADQjdACpkqKjVZSbJTXuXKWiAAAAAAARGiBMOA316Ke0AIAAAAAQGiBMJDvM9eilE4LAAAAAIAILRAG6LQAAAAAAPSE0AKmG+W37WmbSZUAAAAAAMIJoQVM57vtaXkDu4cAAAAAAAgtEAZ8l4dUNLbL7TZMqgYAAAAAEC4ILWA6306LLpehquZOk6oBAAAAAIQLQguYLjMhRvYo75cicy0AAAAAAIQWMJ3VatGolFivc2XMtQAAAACAEY/QAmHBd4kI254CAAAAAAgtEBZ8h3GyPAQAAAAAQGiBsDA6Nd7rmE4LAAAAAAChBcLCqFTvmRblzLQAAAAAgBGP0AJhwW+mRUO7DMMwqRoAAAAAQDggtEBYyPdZHtLS6VRTu9OkagAAAAAA4SCooUVVVZVeeOEF3XXXXTrnnHOUmZkpi8Uii8Wi66+/PphPjQiTmxIri8X7XCnDOAEAAABgRIsK5sVzcnKCeXkMI/Yoq3KSYlXZ9PEsi7L6dk0flWJiVQAAAAAAM4VseciYMWO0cuXKUD0dIpD/ME52EAEAAACAkSyonRZ33XWX5s+fr/nz5ysnJ0eHDh3SuHHjgvmUiGCj0+K1+UiD57iM0AIAAAAARrSghhb33HNPMC+PYWZ0qv8OIgAAAACAkYvdQxA2fJeHVDZ29HJPAAAAAMBIQGiBsJGT7B1aHG3qNKkSAAAAAEA4COrykGArLS3t8/aKiooQVYJAyPUJLaqaO+R2G7JaLb08AgAAAAAwnEV0aFFQUGB2CQgg306LLpehujaHMhNjTKoIAAAAAGAmlocgbGQm2uXbVMFcCwAAAAAYuSK606KkpKTP2ysqKrRgwYIQVYOhirJZlZkYo6rmj2dZVDV3SEoxrygAAAAAgGkiOrTIz883uwQEWG5KrFdoUdnIME4AAAAAGKlYHoKwkp3ku4MIy0MAAAAAYKQitEBYyU3xHrpJaAEAAAAAIxehBcJKjk+nRSWhBQAAAACMWIQWCCs5Kb7LQ5hpAQAAAAAjFaEFwkpOMjMtAAAAAADdCC0QVnJ9Qou6Voc6nS6TqgEAAAAAmCmoW56+88472r9/v+e4pqbG8/X+/fu1atUqr/tff/31wSwHESAnOcbvXFVTpwrS402oBgAAAABgpqCGFo8++qj++Mc/9njb2rVrtXbtWq9zhBZIiYtWTJRVnU6351xVcwehBQAAAACMQCwPQVixWCzK9RnGWdnIME4AAAAAGImCGlqsWrVKhmH0+xcgse0pAAAAAKAbnRYIO77bnlYRWgAAAADAiERogbCTk+Q9jJNOCwAAAAAYmQgtEHZ8Z1ocJbQAAAAAgBGJ0AJhJzvZN7RgECcAAAAAjESEFgg7uX6hRQeDWgEAAABgBCK0QNjJSfaeadHmcKm502lSNQAAAAAAsxBaIOzk+HRaSNLRRuZaAAAAAMBIQ2iBsBMbbVNKXLTXOeZaAAAAAMDIQ2iBsOQ714JtTwEAAABg5CG0QFjKYdtTAAAAABjxCC0QlnKSvIdxEloAAAAAwMhDaIGwlOvTaVHeQGgBAAAAACMNoQXCUkF6vNfxttIGGYZhUjUAAAAAADMQWiAszS9M9zqubu5UcU2rSdUAAAAAAMxAaIGwVJgRryyfuRYbiutMqgYAAAAAYAZCC4Qli8WiU8Z5d1usJ7QAAAAAgBGF0AJh65TxGV7H6w/WMtcCAAAAAEYQQguELd9Oi/LGDpXWt5tUDQAAAAAg1AgtELYmZScqPcHudY4lIgAAAAAwchBaIGxZLBYt8NlFZP3BWpOqAQAAAACEGqEFwtop471Diw2H6LQAAAAAgJGC0AJhbYHPXIvDtW2qbOwwqRoAAAAAQCgRWiCsTclNVnJslNe59cUsEQEAAACAkYDQAmHNZrX4dVu8d5AlIgAAAAAwEhBaIOzN9xnGuauiyaRKAAAAAAChRGiBsFeUm+R1fKim1aRKAAAAAAChRGiBsDcuI8HruLG9Sw1tDpOqAQAAAACECqEFwl5+WpyirBavc8V0WwAAAADAsEdogbAXZbOqID3e69yhWkILAAAAABjuCC0QEcZmeIcWxTVtJlUCAAAAAAgVQgtEhEKfuRYM4wQAAACA4Y/QAhFhXKZ3aHGY5SEAAAAAMOwRWiAiFPqEFsU1rTIMw6RqAAAAAAChQGiBiFDoM9OiqcOp+rYuk6oBAAAAAIQCoQUiwuhUtj0FAAAAgJGG0AIRIcpm1RjfbU8JLQAAAABgWCO0QMTwnWvBME4AAAAAGN4ILRAxxvrMtSiubTOpEgAAAABAKBBaIGL4bnvK8hAAAAAAGN4ILRAxCjP8Qwu2PQUAAACA4YvQAhHDt9OiudOp2laHSdUAAAAAAIKN0AIRY1RqnKJt3tueskQEAAAAAIYvQgtEDJvVogLfbU8ZxgkAAAAAwxahBSLKuB7mWgAAAAAAhidCC0SUQp+5FsW1hBYAAAAAMFwRWiCi+A7j3FXeZFIlAAAAAIBgI7RARJlVkOp1XFzTqurmTnOKAQAAAAAEFaEFIsqU3CTF221e5zYdrjepGgAAAABAMBFaIKJE2ayaMybN69zGQ3UmVQMAAAAACCZCC0ScuWO9Q4v36bQAAAAAgGGJ0AIRZ35hutfxzrJGtTtcJlUDAAAAAAgWQgtEnFljUmWzWjzHTrehrSUN5hUEAAAAAAgKQgtEnMSYKE3NS/I6t+kwcy0AAAAAYLghtEBEmjfWe4nI+4eYawEAAAAAww2hBSLSvELvYZybj9TL5TZMqgYAAAAAEAyEFohIvp0WzR1O7T3abFI1AAAAAIBgILRARMpNiVV+WpzXuY1sfQoAAAAAwwqhBSKW79anGw8xjBMAAAAAhhNCC0SsuWO951psZBgnAAAAAAwrhBaIWL6dFmUN7apobDepGgAAAABAoBFaIGJNyk5UcmyU1zm6LQAAAABg+CC0QMSyWi09LBFhrgUAAAAADBeEFoho83yHcbKDCAAAAAAMG4QWiGjzfDotdlc0qaXTaVI1AAAAAIBAIrRARDu5IFXRNovn2G1IW47QbQEAAAAAwwGhBSJabLRNM0aneJ17n2GcAAAAADAsEFog4vlufbrpMMM4AQAAAGA4ILRAxPOda7HlSIOcLrdJ1QAAAAAAAoXQAhHPd9vTNodLuyuaTaoGAAAAABAohBaIeBmJMRqfleB17v7/7dWTG46ovKHdpKoAAAAAAEMVZXYBQCDMG5umg9WtnuM39lTpjT1VirZZ9PA187R8SraJ1QEAAAAABoNOCwwLp4zL6PF8l8vQT1/ZE+JqAAAAAACBQGiBYeG8k/M0OSepx9v2VDartL4txBUBAAAAAIaK0ALDQkyUTS98ebEeunqubl8xSUmx3iufXt9dZVJlAAAAAIDBIrTAsBFts+oTM3L1lbOKdNbUHK/b/rf7qElVAQAAAAAGi9ACw9KZ07xDi/UH69TS6TSpGgAAAADAYBBaYFhaMilT0TaL59jhcmvN3moTKwIAAAAADBShBYalpNhonTree0eR11giAgAAAAARhdACw9aKKdlex299WC2X2zCpGgAAAADAQBFaYNha4TOMs67VoS1H6k2qBgAAAAAwUIQWGLYK0uM1OSfJ69xXnt6qX762V0dq20yqCgAAAADQX4QWGNZWTPVeIlJS164HXt+ns3/1tl7ZUWlSVQAAAACA/iC0wLB24azRslj8z7d3ufS1Z7aprKE99EUBAAAAAPqF0ALD2uTcJP3y8pM1OjXO77aWTqe+89x2GQbDOQEAAAAgHBFaYNi7eHa+3vnmcv3j1oVaNjnL67bVe6v1j81lJlUGAAAAAOgLoQVGBIvForlj0/TrK2YrOynG67bvP79TVU0dJlUGAAAAAOgNoQVGlJT4aP3o4ple55o6nLrk9+9qa0mDOUUBAAAAAHpEaIER56xpObrg5FFe50rr23XZQ+/q0TUHmXEBAAAAAGGC0AIj0t0XTNeolFivc10uQz98cbeufXyDKhtZLgIAAAAAZiO0wIiUnmDXP76wUPML0/xuW7OvRmf/6m09v63chMoAAAAAAMcQWmDEykuJ05M3narblk+QxeJ9W2N7l7705Bb9ad0hU2oDAAAAABBaYISLsln19bOn6E+fXaCc5Bi/23/68h7VtTpMqAwAAAAAQGgBSFoyKUuv3rFU5/sM6Gx1uPTomoMmVQUAAAAAIxuhBfCR1Hi7Hrxqti6c5R1c/PHdQ37dFo1tXVq1tlj/2VYup8sdyjIBAAAAYMQgtAB8fHnFJFmPm3Hh221RWt+mc379tu5+fpe+/OQWXfPYBjW2dUmSjjZ1aNXaYj23uZQwAwAAAACGKMrsAoBwMyErURfOGq1/binznPvju4f0uSXj5XS7dfWj61V+3Jao6w7W6uLfr9Wyomz9df1hdTq7w4rXd1fpN5+eLYvvlE8AAAAAQL8QWgA9+OIZE/XvrWVyG93HrQ6XPvnAGtmjrDpc2+Z3/4PVrTpYXex17sXtFZr3bppuWDQuFCUDAAAAwLBDaAH0oKdui4rjuiv668cv7daErES9d7BW/95arni7TfdeepLmjk0LZLkAAAAAMCwx0wLoxZfOmCi7rfe/ImMz4jUlN6nPa3S5DF37+Ab97q0DKmto176qFn3+TxvV2N4V6HIBAAAAYNghtAB6MT4rUb/7zBxNzE70uy03OVZ/ufEUPXvrQp07M1eSZLFIF88erU+fMqbP69a2OvTg6/uCUjMAAAAADCcsDwH6cOa0HK2Ymq09lc16flu5NhTXKSspRt85d6oK0uMlSb/7zFyV1rcpOS5aybHR6nK5taeiSZuPNPR63VXvHtJVp4zRhCz/QAQAAAAA0I3QAjgBi8WiqXnJmpqX3Ot98tPiPV9H26x68NNz9MkH1qjho61QLRbJarHI9dFkT6fb0I9e3K3Hr59/wudv7XTquc2l2lPZrMrGDlU2dSgt3q5vfGKyTspP9bu/y23oYHWLbFaLCjMSZLWyewkAAACAyERoAQTB6NQ4PXvLQj265qDi7VH69Clj9OymUj20+oDnPm/sqdKbH1Zp+eTsXq/jcLp13eMbtPFwvd9tWx9p0L9uW6iJ2Unq6HLpuc1levPDKq0/WKumDqckqSA9TpfMztelc/I1JiPe7xpD1dLp1LoDtWpzOHXWtBzF2/mRAgAAACBwLIZhGGYXESylpaUqKCiQJJWUlCg/P9/kijCStXQ6teznb6mmpdNzLt5u0z0XTNen5ubLYvHviLj35T1eQYevwox4/frK2frGsx/ow6PNfT7/iinZuvn0CZpfmNbjc/WX0+XWP7eU6YUPKrTuQK0cLrenlqdvPk3ZybGDvjYAAACAyBSs99+EFkAIPb2xRN949gO/8xfNGqV7LpyhlLhoz7l39tXo6sfWB7yGGaOTNTU3WXmpcZqSm6QVU7MVE2Xr9+O/9+8d+uO6wz3eNnN0iv5+86l0XAAAAAAjTLDef7N7CBBCn5qTr8UTM/3O/2truZb9/E2tWlusji6XPqxs1lee3up1n2ibRV8+Y6Kmj+p9tkZ/7Chr0jObSvXA6/v0hb9u1jm/WqMPK/vu0jhmW0mD/vRez4GFJG0va9SXn9zimd0BAAAAAENBpwUQYu0Ol37w4i79bf2RHm+3WKSe/lZ+97xpunHxOFU0tuv8B9d6LTM5ZnJOkm5ZNl6njs9QdXOnnt1Uqn9vLVdje1efNcVGW/XDi2bqU3N7/zvidhu65PfvamtJQ5/XkqRzZ+bqjjOLVJSTdML7SlJZQ7vue/VDHahu0SnjM/SZU8ZobEZCvx7ra3dFk0rq2rRgXLpS4+2DugYAAACAgWF5yCAQWiCcvfhBhb713Adq/mhoZl+WTc7SE9fP98yi2HS4Tlc+/J66XB//9b1w1ij95JKZfkszWjudenpjiR5dU6yyhvY+n2fu2DRdcPIonTszT1lJMV63PbOxRF/3Wdry+aXjdd5JebrmsQ09BiMLJ2To+oWFWjE1R7aPdjE52tSh6uZOjc2IV1JstF7ZUalv/uMDv8cvLcrShSeP0umTs5SZGKOalk5tPFSv6pZOLSvK8mw5K3UPLH1pe4WeePeQtn0UquQkx+ivnztFE7P7F5z0146yRq3ZV6N5hWmaX5ge0GsDAAAAkYrQYhAILRDuSura9KMXd+uVnZW93iczMUav3LFEmYneIcLru4/qrn/vlMtt6LYzJurqU8b0OWDT6XLr7X3V2lPZrIqGDq0vrtXeoy093tdqkWYVpOr0omzNGpOqji6X/t8/d3h1d4zLTNArdyxRTJRN6w/W6prHNniGcvrKT4vT6UVZ2nio3mtgaEF6nErq+g5SLBYpNzlWFY0dnnN2m1U3nz5en100Ts9tKdPDbx/Q0Sb/zpMpuUn6122LFBvd/5kdfXllR6Vu+9tmz/KXM6dm67vnTRt0V0hP/rzukP6xuUzTRiXraysnKz3h426RI7Vtyk2JlT2KlX0AAAAIL4QWg0BogUixobhOP3pxl7aVNnrOpSfYNasgVd/8xBRNzg1st4AkdXS59MMXd+kv7/W8TOVEnrh+vpZP+Xi71pe3V+irT29Te5crUCWeUG9LaY53/cJC3X3BdM+xYRjaUFynneVNWlqUpYnZiZKkLpdbv3/rgN49UKO0eLvGZiSoMCNeiyZmqiA9XttKGnTFw+vU0eUdzNijrLpx8Th9fsl4pSXY1dLp1N/WH9aG4jrNHZuum5eOl9Xav91aXvygQrf9bbPneHJOkv560ylqau/SV/6+VdtKG5UUG6VvfGLKCUOq4cQwDG08XC+X29Ap49JN+74dTrfau1xeA3MBAADQjdBiEAgtEEkMw9CRujZVN3dqXGaCMnw6K4Ll31vL9P3nd6m21dHvx6yYkq3Hrp/vd76ysUN/XX9Yf1t/ZEDXO2ZBYbr2VDapqR9LZgbi/itO1qKJmdpZ1qRfv77PM5fDapG+c+5UXbVgjL7w181avbfa77FWi/SJGbl6/1C9qpv9uzmOibfbdPb0XL2xp8prqcsXl0/U186efMIaO7pcWnHfar8lPIUZ8appcail0/v35MypOfrZp07y6sQ43lsfVumZjaVKiY/WsqIsLZ6UOaRdXdxuQ39Zf1h/WndYTpdbuSmxykuJ08zRKbpsXr6SYoPzRt4wDP3fv3borx/NgLlkzmjdd9nJIQ8u3j1Qo1v+vElNHU7NHZum6xYW6pwZuYq2jeyuF8Mw1Ol0KybKOmJCNAAA0DNCi0EgtAD6x+F0a82+av1nW7le23VUbY7euyVio6165falKszsfUlER5dLL35QoSfeLdaOsqYTPn9stFV3nz9dV8wvUEeXWy9ur9Bruyr1zr4atR5XS7zdpk6nu9fdSRaMS9f5J4/SD1/YpU5nz0tVepKeYFfdIEKW/rBZLfr3bYs0Y3SKnC63tpY0qNXhUrTVophom6blJSvObtNv39yvn7/64YCunRofrXNm5OmcGbk6dXyG7FFWOZxu/eTl3Xpi7SGv+8ZEWTWvME3T8pI1bVSyRqXEKd4epTi7VU0dTlU1daiquVOtnS51Ol3qdLqVkWDXzNEpSoqN1vf+s0PvH6rvsY7c5FjdfcF0fWJG7oB/fwzD0KbD9XIb3UuSfJe+vPBBub74ty1e5+6/4mRdPDt0P88b27u04r63VNPi/RpJi49WWoJdNotFuSmxumrBGJ07M29A127u6NL6g3X6oLRB20ob1eVya/aYVF196ljlpcQF8tsIuIY2h77y961688NqTR+VrEevmxf2NaP/ulzuER/KAQAGhtBiEAgtgIHr6HJpQ3GdVu+t1jv7alTd0qmEGJuSYqI1KjVOn186XgvG9W8ApWEY2nykXn9bX6LyhnbNHZum5VOyNC0vRfuqmrW9rFGtnU6dMyPPa7DmMZ1OlzZ9NHxzfGaipuYl6UB1q+769w6tL67z3O/0oix9ecVEzR3bXddf3jus//vXjsD8hvRg4YQMTclN1h/XHerX9q7T8pL19bMn61vPfeA3eyPebtNnF43TE2uLvQKagYqNtmrm6BS1OVzaWX7ioCgYzpqWox9dPEPZSbH9un9dq0NfenKz1u6vldQdApw7M08XzR6teWPT1NDWpbPuX+0XFmQk2PX6naf3ujvMm3uq9IMXd6mp3am5Y1O1tChL88amK9pmkcViUUaiXcnHdYbsr2rWL1/bq9oWh5ZMytSlc/O93nzf/Z+dWvXuoX59T1ctGKO7L5gmu82q4ppW1bc5dHJ+qqJ6ePP3/qE63fDE+35dNFJ32LWsKEsOl1uHalvldBm6cNZoff3syZ6htsfUtnTqnud3aWtJg86enqOvnz1lQHNPHM7u5yjMSOj345wut25Y9b7W7KvxnDtlXLqevOnUfi+HCjWnyy1DCrs34i9vr9Cj7xTLbRi6acn4AQdfA1Xd3KmWTqfG9RI8bzpcry/8dZNqWhy6YWGhvn3uVL/XHAAAPSG0GARCC2B4MgxDb++r0Z6KJi2ckKmZ+Sl+t9/yl016defRAV87LT5al87JV0l9m9bur/V7QzkhK0HP3bpIKfHR2l/Vot++uV//3lqmfmQX/WaxSHnJsSo/bvioJK2clqNNh+sHtfQmVHKSY/T7q+dqzpg0Sd1LhnaWN2pPZbMOVLUoJtqqBePSlZcSp689s02l9T0PYp07Nk1JsVF660P/JTuSdOX8At176UmeP5/EmO6lL6vWFuueF3b1OevEaukOWL5+9mTtLG/St/6x3WsWi8UiLZ6YqZuWjFdGol3nP/jOgP58J+ckqdXh9HxvE7IS9OBVczRtVLLnPm0Op8765dsn3NHH13fOnaLPL53gOS5raNc1j63XwepWz7lzZuTqwatmy2qx6PU9VdpW0qDTJmRo4YQMryUcnU6X/vreEf3mzf2qa3UoLT5at5w+QdeeVqgD1S16bnOZDta0KDMxRqeOz9Cp49OVn9YdLv7whV169J1iv/p+fPFMffqUMQP6ngzD0F/WH9E/NpVqck6SvvGJyQFdHmcYhn780m49sqZYucmx+umnTtLpRVmSurtoXvygQllJMVo2OSukgUa7w6V7nt+pp94v8Tr/mVPG6LvnTQvYAOHjPbe5VN/8xwfqchn65Mw8/fKKkxUT9fHz1LZ0auX9b3v9jLn2tLG654Lpfst/nK7uGS/BWhpmpmN/H1jyBAwvTpdbFouFIDaICC0GgdACGLlaOp36v39u11t7q9XQ9vGMifGZCfrC8olKio3S157epubjQon8tDj96bMLND6rezhnY3uX/rb+iJ5YW6yq5k7NHJ2i3356jsZkeHeFHKpp1cNrDur94jrNGJ2iGxYV6uvPfOC1U8pAXDY3X3eunKwbVr2v3RVNiou26bvnTdNVCwpU3dKp7zy3Q//bfeJAxh5l1cn5Kdp8pKFfHSH9MSY9Xrcum6DWTqfe+rBa7+yv8buP3WbVxbNHa/OReu2r6nmHmkAYnRrnedM/MTtRo1PjepxL0pv+DHKNt9u8lkvFRHV3tGw83PNSmd7Yo6z67ien6upTx8pisegnL+/WH1YfHNA1jj3/S7cv0YSsRO072qxrHtugyqYOv/udNS1HNS2d2nKkwXNuyaRMffe8aTKM7hkdj71T3GNoFBNl7XV5VW5yrCblJHp1WBwvKSZK/7vzdMXbbdpf1aLxmYlKie/7Te3PX92j3755wHM8Jj1ej18/3zMkd6j+tO6Q7vr3Ts+xPcqqJ66fr3i7TTf9aaOnk2dCVoLuvmC6lkzK6td1DcPQvqoWfVjZrJL6NpU3tCst3q4zp+bopPwUvze86w7U6t6Xd6umxaHEmCg1dXR57Yp0vGl5ybp12QQtmZTZa0fRMU6XW25DJ+yS2Xu0Wec98I7XLk/nzMjVbz49RzarRYZh6At/3ayXd/jvZvX1syfrtuUTPcf/3Vmpe57fpcqmDp1elKXvXzhd+WnxcrkNvbO/RiV1bTptQoYmZPX9Z1hc06q/rT+sg9WtykmJ1dS8ZE3LS9aU3CQlxAxuBk9jW5fueX6n1hfX6fTJWfrOuVM9oeaJvHewVnf/Z6f2VDZrUnaifnLJTM07bmtrh9OtKKslYN1Ez24q1dMbSzQpO1FfWzlZab3MKBppmjq6ZLNYBv0akLr/fq4vrlN9q0PLp2T3GgLurmhSRWO7puWlKDcl1vP3+rVdR9XQ5tB5J43SyQWpg67jeE6XW3WtDmUn968b8XjvHqjR5sP1OnNajqbkJp/4AfBwON16Y89RPb2xVGv2VSslzq4vnTFR15w6Nmw7AyMZocUgEFoAkLr/o9DQ3iW3YSgrMcbzZuJAdYvufHqbtpY0aNHEDN1/+awe/zNhGIbq27qUEhfd73R+a0mDLvnd2h4/oc/46D+mdW0OvzfN8Xab3vraMmUnx8rpcmtPZbPy0+L83rgcqmnVKzsr9fKOSm37aLDo8Qoz4vXbz8zR9FEpamzr0rqDtdpV3qhdFU368Gizmtqdane45HC5ZY+yKic5RtlJsUqJi1ZMlFU2q0XFNa36sLJZTrchq0W6fuE4fe3sIs9AT8Mw9J9t5QMe5DoYSTFRchvGkJbQBMJXzyrSl1dM0oHqFu072iK3Yai8oV2/+O+HfjvL9OSMKdm6fF6+bvvbFq8gqTAjXiun56q106nnNpf1uQvP3LFpunxevn780h6voa/hIjspRnWtDjndhmKirLr61LG6+fTxamp36qXtFTpQ3aL0BLvOmZGn9w7W6pev7fW7RnJslH72qZM1Z0yqMhJjZJHU4XSp3eHyfEoWbbP0OFzW4XR7lgLtr2rRJx9Y4xfCxEXb5P5oiKivM6fm6FNz87W0qPfhtdtKGvSjl3Zrw3HL1I5XmBGvS+bk64ZFhUqKjdau8iZd/Lu1A5q1I3V3Bc0ek6ZbTp+gM6dm+wUh/9xSqu/9e6c6nG7dvHS8vnJmkec/4R1dLnW53EqKjVaXy61Lfveutpc1+j3HVQvG6McXz9B/tpXr9qe29lrLNz4xWTcsHKdnN5fqe//e4fWzLTEmSp8+ZYz+u7NSh2rbPOdnFaTq0jmjtbQoS2PS42WxWNTR5dLmI/X6y3uH9fKOyh6DQ4tFGpser8m5SUqNsyvOblNKXLQ+MSNXU/M+fsPmcLpV2djh2Qr6aFOHrn1sg1dgPDknSY9eN6/HJYjHNLZ36d6Xd+vJDd5dLzarRV85c5Jio216emOJ9h5tUYLdpumjUjRjdIounTta00el9HLVvv35vcP67nHLGCdlJ+rPN56i3JTe39CWNbRr3YFajUmP1/zCtAF1ghiGoYrGDhmSRqXEDriL5GhThzYdrtfWkgbtLG9UYkyUbj59gqerLlD++O4h3fvyHrV3uXTq+HRde1qhzpqW49cFVVLXHRTG26OUEGNTarzd0x2z6XC9fvDCLs/Q7ZmjU/TU50/1CkFaO5265/mdenpjqedcQXqcoq1WHaxp9XquS2aP1jfPmaKcQYQNUvfsolVrD+mxtcVqaOvS4omZevjauf0ejv3CB+X60pNbZBjdfze+f8F0XXNaYa/3NwxDz20u09oDNZqck6QLZ43u83U1UA1tDj2zsVQbD9dpWl6KPru4cEgdV20Op+KibQHrbGp3uPS/3Ue1o6xRuyub9UFpg9eHV8fMHZumn146UxOzA79D30hGaDEIhBYATsQwDHW5DM+bnED60Yu79Miaj1vo7Tarvn3uFF2/sFAWi0W7K5r045d2e31q/X+fnKrPLRk/oOepb3Voa2mDthxp0P6qZo3LTNDNp0/wmtvQG6fLLZu19++9o8ul4ppWZSTYe/10yHc2xUDNHpOqu86bprX7a/TU+yU9fvr/k0tmqt3h0vdf2NXv615z6lg1tndp7f6afoUqp45P146yph5nTEjS2Ix4vXrH0h4/sdtV3qSb/7JRJXUDW+4hSdE2i16+famns6CxrUv/3lamA1UtykiM0b6qFj2/rXzA1w2VyTlJJ+wqirJa5Bxkt4/Vol6X5xSkx+nK+WN0+bwC7Sxv1Kp3D2nNvhqlJ9h1xbwCrd5b3eMb9f6IibJq+qhkpcRFKzE2WnHRVkXZrKpp7tR/d/Vv6dn4rAT96opZ+uLftuhIXVuv94u322QY6jOsWjIpU3edN02Tcrr/g+37pleSzjspT/dcMF2PrCnWn9YdUpvDpaVFWRqVEuu3DOV4hRnxqm7uPGEomBgT1evfjxNJT7ArLyXWE4QORrTNol9dMVufPKk78PrK37eqorFDybFROnt6rtYdrO3x50dafLTmFaZrd0WTGtq6NDk3SZ+am6+FEzL0j02l+tN7h3t8U3MiFot09/nTdd3CQknSjrJGPb+t3BPauQ1DeSlxmj4qWdNHJaswI0FWq0Vr99fo2sc3+HXA5afF6S83nuIZdG0YhqqaO/X+oTrPp8TH/te+ZFKmfnDhDBVmJqil06kPK5skWZSRYFd6ol12m1VOt6H6Vof+s61cz24qVfFHb8aTYqI0JS9JM0en6vTJWTplXHqvnQhVzR368Yu79e9t5T0GTJfNzdc3PjFFWUneS7raHS6V1reppsWh2tZO1TR3qrbVoZoWh5LjorR8crbmF6Z7fRDw1/WH9f/+6T+PKjc5Vt88Z7IumjVabkO69+XdevSdYr96EmOilJMcowPVrX7X+OTMPP3m07NlsVi0o6xRX35yi1840ZcEu023nD5BNywe12vnzv6qZv1na7le3F6hkvp2ZSbYlZ8Wr71VzX6vr2tOHasfXDTjhM/b2unUsl+85bd72ReXT9SdK4v8/u3udLr0jWc/0L+3fvxvxrEljzcvnaDFkzJ7fB6321BNS6cSY6N6DVP2V7XoD6sP6D/byr0C2MEO465u7tR3/7VDr+6q1KiUOP3uM3P61dVy7O9NTx8ivbqzUnf/Z2evXWy+7Dar7rv8ZJ1/8qgB1V5c06qm9i5lJNqVmRjT7+V8hmFod0WzHC63Tu6hI284iPjQ4vDhw3rggQf04osvqqSkRDExMZowYYIuv/xy3XbbbYqP7z0BHyxCCwBm6uhy6Vv/+EAv76jUyfmpuufC6V6fEkrd/4C9e6BWb31YpRmjU3TByaMi8h8xp8utn76yxyukOebYf5CLcpJU1dyp9w7WqvmjbW2vnF+gey6c7llX73C69cymEj34+n7PsoezpuXo4Wvmym1IVz68rtddTI6Jtln0s0+d5LXDSJfLLcPo/kTnsXeK9eiaYs8bxJgoq3540QxdNq9A7Q6X/rW1TL95Y7/fvIlVN8zXssnZvT5vQ5tDv31zv47Utemk/FSdXpSlf20p63H2w/G+vGKSvnpWUa+3tztc+sSv39bh2t7f9M4ek6obF4/TV5/eJofPp/l9vemXpHlj03TtwkL9Y1Op1/Ka0alxOmdGrsoa2vXewVrV9/Cm7rrTxuqOM4t05i9Xh/WslXA2Y3SyHrhytlzu7uUZfS2pslktOnV8ugozEjzbAPd0n0AsB/vkzDy9uL1iyNcJNKtFunLBGD2zsURdrvD43O1XV8xScU2rfv36vj7vl55g19JJmXrzw+peu6SibRZlJsYoKTZKFY0dnp+VPbFHWTUxK1F7KpuGNFcpNtqqxRMzde7MPJ01LUdJsdGqbenUS9sr9LNXP+yzBqn7Z/xXzirStaeNVYfTrftf26u/rj98wu6znOQYnTMjT6dNyFBzh1Nff3Zbn0v2zj95lJo7unqddXQiXz5jopo7nfrLe4cH/drJSLDr1mUTNHtMmlLiolXd3Km39lbprT3VA14S+sfPLvDM1+nN/a/t7fV1NTo1TjZrd+fZ1LwknTYhU89vLdeGQz13gEnSjy6eoc+cMlZS95v/9w/V6aXtFXp5R6UnGMlMjFFhRrxWTM3RDYsKFRtt0ys7KvTlp7b6/ftyvJPyUzQpO0lj0uM1bVSy5hem9bq87fXdR/WNZz/w+ncjLT5a/7h1oWeJ7jHHluy8uadKb35YpU2H69XlMmS3WRUbbVVmUozGpMfL4XTr3QMD//DEbrPq6VtO06x+BCadTpe+/dx2Pbe5zOt8ZqJdJ+Wn6uT8VE3OTVRybLSS46KVlxLrmdF0tKlDX316q+cDngWF6frNp2crOzlWbrehbaUNqm7uVF5K3EcdtpE5VyeiQ4vnn39eV199tZqaep5oX1RUpBdffFETJ07s8fbBIrQAEA7cbmPErJt8eXuFHlp9QFarRYsmZGr5lGzNKkj1+kTE6XJrf3WL4qJtGpvR8w4GHV0uvb23Woak5ZOzPev12x0u/fm9Q6ptdWhSdpKmj0pWp9Ot9QdrtfFwvawW6ZbTu/9D2Zeq5g79Y1OZGtodunxegd/a+06nS0+/X6In1h5STUunbl02Ubcum9DL1fr2+u6j+uY/PvDbBUWSxmUm6OXbl5zwU5r1B2t1xcPv9XjbJ2fm6eeXnaR4e5Te3FOl25/aoqYOpxJjovTlFRN1+bwCPfZOsR5++6Dn07HRqXE65aMtgpdNzvL8x2hbSYM2Ha7X1LxknTIu3fO6dbsNHaxp0daSRm0tqVdlY6dOGZeuzy4eJ5vV0uPWtAP1/86dqo2H6wY1QLc/JmQlaGxGgt7YU+V1/tOnjNHM0Sn6+asfDmrr48xEu4pykpSbEqtNh+v7DJck6eSCVH12UaGaOpzKTLDrzONa3zu6XHp2U6le331U6w7W9mvJ0UBZLdIXz5ik37yxr9c3uuedlKfffHqOfvnfD/XAG/t7vdb8wrQeQ8TYaGu/a0+MidJFs0epucOpXeVNOljTGpDQpT8za3oSb7fp7Om5AR+wHCnsUVYlxUQNKoQsyklUc4ez359yh7Pc5Fi1djq95l4FWk5yjF69Y6nXG/vS+jY1tndpTHq82hwuLfv5W312YA2U1SI9dt18pcRH65vPfnDCuVPjsxJ03sw8/ebN/YP6+zAlN0nzC9O1YFy6ZoxO0daSer34QWWvc7nGpMfruS8sVEaCXSV17fr31jL9a2tZj90zAzW/ME1nTcvRX9474tf5lpcSq+e/tFjxdps2HqpXe5dLWUkxyk6KUU5yrKJtVjW2denzf97otXtdf5xckKpFEzL05IYjfuF/dlKMrvvoQwPfzp+0+GhdcPIoXX3qWE+HXSSI2NBiy5YtWrRokdrb25WYmKhvf/vbWr58udrb2/XUU0/pkUcekdQdXGzcuFFJSYH7QyG0AAAMlWEYQ/60o7mjS79764Aee6fY80mVxSL95cZTtGhiz+26vny3Xp2Ynai7zpumpT6f1DV1dGlPRbOmjUr2amOubenUjvImjc9M6HN9/2C9urNSr+6sVGqcXUuKMlWQFqfH3inWMxtLPcsBspNitLQoS9tKGjz/WbZYpO+cM1U3LR0vt9vQ42uL9df13f+pDNQA2SirRf/8wiJNyknUl5/cov/uOqqYKKu+fvZk3bh4nCwWixrbu/TPzaX6766jWl9cd8LnToqN0pfOmKhrTyv0hE6GYWhrSYO+/dx27an0/9Q1PcGuF760WKNS4/xu89XR5dLLOyr005c/7HHY6vHsUdY+PwE93heWTdA3PjFF/9hUqm8/t91rMKfUveTmP7ct9gyFXLu/Rg+/fdCrC8dqkX508UxdtWCMXt99VD9+abdK6tu1eGKmPrdknOYXpuudfTV6bkuZ3u1heZbVIk3JTdb5J4/SZ04d47WUraPLpb1Hm7W7okmHa9vU3uXSoZpWvTmAT9dnjE7Wbz89Rz94YZf+t7vqxA/4yMppObrr/GnKT4vXugO1uvPprSpv7JA9yqpzZ+Tq0rn56nK59dquKj25oedOl4E6c2q2Kps6tKPMnK2qByLebtO8wnQVZsTruc1lg14q1B+XzBmtQzWt2nzcMGFfFotks/S89Cw5NkqXzMnXH9cd6jO8OrkgVb+6YpY6ulzadLheHV0uzRmbpln5qaptdegXr36opzeVDCoA83VSfoo+KPVerjZjdLJmF6TJ6Tb07oEaT+gZG23VqNQ4r52hjmX//f2xGG+3yWqx+P05xURZ1fXRAN/BirZZNCErscefc0OVmWiXw+lW0wk6fE5k7tg0nTo+XZNzkzW7INXz7167w6W7/r1Dz2wq9bp/flqcalscfiGRzWrR2Ix4dXa5B7zjV6CcOj5d15xaqJXT/ee7hJuIDS2WLl2qNWvWKCoqSm+//bZOO+00r9t//vOf6xvf+IYk6Xvf+57uvvvugD03oQUAIJyU1rfp0TXFKq5p1ZXzC3TOzLx+P9bpcuv3bx3QttJGLZucpSvmF4T9f16kj4cHFmbEa86YNE/3xoeVzdpV0aiinKQehxm63YZqWjtV3dwpm9Wi+Ogoxdltsli625rLGtr1t/VH9J9t5Z437JNzkvSZU8do79FmPbe5zLPzy/fOn6YbFo2T1B0sHK5tU3JctNJ72a3h2PDa6pZONXd0qbnDqc4ut5xut7pcbk3IStQlc/J7fXxLp1N3PLXF6w2z1SL9eQAh1TGtnU79/q0DevSdgz12L3zzE1O0ZFKmblj1vte69zOn5mjhhAz9ad0hz3DMOWNS9eTnT/Usx2poc2hPZbOONnXoaFOH4qJtOndmXo9bzu6vatE/NpeqprlTVy4Yo7ljP+5mMgxDbqPnNeaGYaikrl1bSupV3+rQpJwknVyQ2u8dPY5d496X9+gPb/vvuPPZReOUlxKrFz4o1+G6Np0xJVv3XDBdSbHRcrkN/W3DEb13oFa5KbGalpesOLtNz20u05sfVsnlNjy7HX1uyTi/TzO7XG7tO9qigvQ4r0GDhmHonud3eYWIx9isFl0+L18pcXa53G7tPdqineVNqmnp9LvvtLxkPXvraXK6DX37H9v10o6KHt8cx0ZbNbsg7aMBsVn6/VsHtOrdYq83nVFWi6Jslh5fI4kxUTrvpDxdNi9feSlx+rCyWdtKG/T23mptLWno15vXS2aP1rfPneqZXVHV1KF7X96j57aU9fm4pNgoZSXGeNb+p8bbteVIfZ9vdj9zyhj98KIZslgsevGDCn37uQ/83sBmJNj1h2vmal5hujqdLlU1depIXZuO1LUpLT5ap47PUGq8XQ++vk/39TDs126z6qal43THmUUn/Dm6q7xJv3ztw34FYNlJMTrvpFFaMTVbzR1OldZ3/91bOCFTU/OSdN0T7+vtAexydbyrFozRGVOydftTW7x2tepJbnKsHr9+vsZlJuj+/+3Vwz383Rmsy+bm6+ufmKysxBj9Z1u5fvDCrh47CfsrKTbqhEuQBiIz0a7/++Q0XTir9+W2TpdbVz+2Xu8dHFjXhNkumjVKv7pyttll9CkiQ4sNGzbolFNOkSTdfPPNeuihh/zu43a7NWPGDO3evVupqamqqqpSdHRg9vwmtAAAYHira3Vo3YFa5aXGanZBquc/qY3tXdpQXKe8lFjNGD24HR6GwuU29Kvj3iwcm5syWC2dTr2zr1r/212ltftrZBjdu9lcPr/7mpWNHXr47YNqczh1/smjPOGIy21oQ3Gdmju6tLQoq98D48KNYRh64PX9uv9/3W9Ao6wW/eCiGbpqwZhBXa+6uVMfVjZral5SjyHNibjdhr769Fb967iBhxkJdj346dlaOME/mDpS26bV+6q1+sMq7atq0eScJP3w4hnKTvp4wHFVc4fKGzpU3+pQQ7tDqfF2Tczq3s7Zd4nh7oomPb+tXAkxUZo7Nk0n56cqNtqqNodLda3dO1PZbBZFWbuHc0b18sa8vtWhNz+s0osfVOjtfdVesx7sUVbNLkjV7SsmaWEvYdvGQ3W66987tavCu1Pk+EGhPdlf1ayXt1fqveJabT7c4Pl0+7K5+br30pO8ArDyhnZ95e9bPW35U3K7d4TJTztxx5jbbejWv27yLDuzWqRL5uTrjjMn9evxx9t8pF5/WH1A20oa1dje5al5+qhkLZ+crWWTszR7TFqfu4xVNnZo5f2rB9xFkGC36c2vL1N2Uqyqmzu1raRBLsOQ1WJRXWun3jtYp7X7a1TV3KnTxmfol1ecrLyU7o4uwzD01ae36Z+9BEwTsxN10axROnt6rgxJB6tb9fu39mubT1eIJH3pjIn66lneA0DbHS69e6BGxTWtOlLXpr1Hm7W1pOGES8QS7Db933nTtGJKti7+3bsn7GIYlRKr5VOytXxytsZmxKujy61Wh1MVje06XNu9m8yo1DjdsHDcCbfalqSalk6d/+A7g1rOlJ0Uo8evn6+CtHhVNXdod2WztpU06IPSBh1t6g67mzqcPXbsnT09R20OV49bh6fFR/f6OEl6/Pp5OmNKzoDrDaWIDC2+853v6Cc/+Ykk6b333vMEGL7uvfdeffvb35Ykvfrqq1q5cmVAnp/QAgAAmKnN4VS0zRoRXTGRYENxnbaW1OuMKdmmb1XY5XLrJy/t0cs7KnRSfoq+d/70fi39CVeNbV3aeLh7adSknO6Biv3Z5vtYR8uT648oOsqqzy4qHNBQ6S6XW3uPNstus/a6dt/tNvRBWaOa2ru0aGJmv7cfP3b9v79fopqWTp13Ul7AXjedTpcMQwMOAl/dWakv/HXzgJa/3XlWkb60YlKf9zm2G9qxGVDHczjduvZx784Cu82qO86apM8vGe8Xajldbj2yplj3v7ZXDpdbVov03fM+7lg7EYfTre1ljdpQXKcNxbXaeKhezZ1O2aOsWjopU5+YkaezpuZ4woX9Vc267KF1fjMf0uKj9cmT8nTx7NGaM2Zg2/z2x7aSBl32h3Vey+tsVotykmJU0+LwWz4ndYc8q26Yf8LQq8vl1roDtXplZ6Xe3lstq8WiLyyboCvmF8htdA9ZfXjNQTmcbi2ZlKnPLx2vxRMz5XIbOlzXpn9sKtXf3y/xLK/LT4vT6q8vH9Br3wwRGVocWxqSkJCghoYGRUX13Aq4bt06LVy4UJJ011136Z577gnI8xNaAAAAAAgnB6pbtO5AraqaOnS0qVPtXS5NH5WsxZMylZUUo/9sLdc/NpfpYHWLzp2Zp5996qQhB5+N7V268+mtWr23WvML03X3BdNVdIIBj1VNHXqvuE5zx6Zp9BACOddHW6qmxEX3vr1uU4de2l7x0QyJBI3NiFd+Wv+Cs6HYfKRev/7fPkXbLFo+JVufmJ6rjMQYGYahhrYuHapt1f6qFh2qbVVavF1XLhgzoOVtfWls75LFol63qO90uvTKjkr9ed1hrZiaM+iB4KEUkaFFVlaWampqdPLJJ2vr1q293q++vl7p6emSpMsuu0xPP/10v65fWlra5+0VFRVasGCBJEILAAAAAJEjEIOgQ3FNBF+k7EQXrNAiMDFRDzo6OlRT071W50TFpqWlKSEhQa2trSopKen3cxz7DQEAAACA4SQY4QKBRWSKhMAimIK2wLK5+eOpwImJiSe8f0JC96Celpa+9wsGAAAAAAAjQ1A7LY6x23veEux4MTHdk5vb2/u//+2JujKOXx4CAAAAAAAiS9BCi9jYj7dwcjhOvHdvZ2f3/tVxcf0f8sKMCgAAAAAAhq+gLQ9JSvp4Gm1/lny0trZK6t9SEgAAAAAAMPwFLbSIjY1VRkaGpBPv8lFfX+8JLRiuCQAAAAAApCCGFpI0bdo0SdL+/fvldDp7vd+ePXs8X0+dOjWYJQEAAAAAgAgR1NBi8eLFkrqXfmzatKnX+61evdrz9aJFi4JZEgAAAAAAiBBBDS0uuugiz9dPPPFEj/dxu93605/+JElKTU3V8uXLg1kSAAAAAACIEEENLRYsWKAlS5ZIkh577DGtW7fO7z733Xefdu/eLUm6/fbbFR0dHcySAAAAAABAhAjalqfH/PrXv9aiRYvU3t6ulStX6jvf+Y6WL1+u9vZ2PfXUU3r44YclSUVFRbrzzjuDXQ4AAAAAAIgQQQ8tZs+erb///e+6+uqr1dTUpO985zt+9ykqKtKLL77otU0qAAAAAAAY2YK6POSY888/Xx988IG+8pWvqKioSPHx8UpNTdW8efP005/+VFu2bNHEiRNDUQoAAAAAAIgQFsMwDLOLCJbS0lIVFBRIkkpKSpSfn29yRQAAAAAADD/Bev8dkk4LAAAAAACAgSK0AAAAAAAAYYnQAgAAAAAAhCVCCwAAAAAAEJYILQAAAAAAQFgitAAAAAAAAGGJ0AIAAAAAAIQlQgsAAAAAABCWCC0AAAAAAEBYIrQAAAAAAABhidACAAAAAACEJUILAAAAAAAQlggtAAAAAABAWCK0AAAAAAAAYYnQAgAAAAAAhKUoswsIJqfT6fm6oqLCxEoAAAAAABi+jn/Pffx78aEa1qFFdXW15+sFCxaYWAkAAAAAACNDdXW1CgsLA3ItlocAAAAAAICwZDEMwzC7iGDp6OjQ9u3bJUlZWVmKigr/xpKKigpPV8iGDRuUl5dnckUYLnhtIVh4bSFYeG0hWHhtIRh4XSFYIuW15XQ6PasdZs6cqdjY2IBcN/zfxQ9BbGys5s+fb3YZg5aXl6f8/Hyzy8AwxGsLwcJrC8HCawvBwmsLwcDrCsES7q+tQC0JOR7LQwAAAAAAQFgitAAAAAAAAGGJ0AIAAAAAAIQlQgsAAAAAABCWCC0AAAAAAEBYIrQAAAAAAABhidACAAAAAACEJYthGIbZRQAAAAAAAPii0wIAAAAAAIQlQgsAAAAAABCWCC0AAAAAAEBYIrQAAAAAAABhidACAAAAAACEJUILAAAAAAAQlggtAAAAAABAWCK0AAAAAAAAYYnQAgAAAAAAhCVCCwAAAAAAEJYILcLI4cOHdeedd2rKlClKSEhQenq65s+fr5///Odqa2szuzyEGYvF0q9fy5YtO+G1Xn75ZV188cXKz89XTEyM8vPzdfHFF+vll18O/jeCkKmqqtILL7ygu+66S+ecc44yMzM9r5Prr79+wNcLxOvG6XTqoYce0pIlS5SVlaW4uDhNmDBBN998s3bu3DngmmCOQLy2Vq1a1e+fa6tWrTrh9dra2vSzn/1M8+fPV3p6uhISEjRlyhTdeeedOnz48NC+YYTMxo0b9f3vf18rV670/KxJTExUUVGRbrjhBr3zzjsDuh4/t3BMIF5b/NyCr6amJj311FO68847dfrpp2vixIlKSUmR3W5Xdna2li1bpp/97Geqra3t1/XeffddXX311Ro7dqxiY2OVm5urs88+W08++eSA6nryySe1cuVK5ebmKjY2VmPHjtXVV1+tdevWDebbDD0DYeE///mPkZycbEjq8VdRUZGxb98+s8tEGOntteL76/TTT+/1Gi6Xy7jxxhv7fPznPvc5w+Vyhe4bQ9D09ed83XXX9fs6gXrdVFdXG/Pnz+/1GjExMcYjjzwyxO8aoRCI19YTTzzR759rTzzxRJ/X2rdvnzFp0qReH5+cnGw8//zzQ//GEVRLlizp1+vh2muvNTo7O/u8Fj+3cLxAvbb4uQVfr732Wr9eD5mZmcYrr7zS57W+973vGVartddrfPKTnzTa29v7vEZbW5tx7rnn9noNq9Vq3H333YH8LQgKQoswsHnzZiMuLs6QZCQmJho/+tGPjHfffdd4/fXXjZtuusnzoioqKjKamprMLhdh4tjr4tZbbzW2b9/e66+DBw/2eo1vfetbnuvMnj3bePLJJ40NGzYYTz75pDF79mzPbd/+9rdD+J0hWI7/R2rMmDHGypUrBxVaBOJ143Q6jcWLF3vue8kllxgvv/yysX79euOBBx4wsrOzPf+YvvTSSwH47hFMgXhtHf+f/1dffbXPn2v19fW9XqepqckoKiryXOumm24yXn/9dePdd981fvSjHxmJiYmGJCM+Pt7YsmVLQL5/BMeECRMMScaoUaOM22///+3de0yT1xsH8G+hq9wE1IkGRVGwXqKbZmWTyMZQcPOCCEQnJpO5TXQ6N82y6ObULItOSHQuy6a4MJ2Zd4YouixTAiwDFJ2auHnh6qIyXcEbN+Xi+f3BryeFvi0FCy3y/SQkL57znp6XPj5tn56efihSU1NFQUGByM/PF1u2bBGDBg2S93NcXJzFsZi3yJitYot5i1o7ceKE8PPzEwsWLBBff/21SEtLE/n5+SI3N1ccOHBAzJkzRzg7OwsAQqPRiAsXLiiOs337dhkPAQEBIiUlRRQUFIj09HQRFhZmde6bN2+e7BsWFibS09NFQUGBSElJkf8PAIjk5OTO+HPYDIsWDsBQ7VWr1SIvL8+kPSkpSQbU+vXru36C5JCeNCauXr0q1Gq1ACB0Op2ora1t0V5TUyN0Op2MTa706f7WrVsnMjIyxK1bt4QQQpSVlbX7haWt4iYlJUXe9tKlS03ai4qK5OqzwMBA0dDQ0L6LpS5li9gyfvJfVlbW4bmsXbtWjpOUlGTSnpubK2PY0ko0sr8ZM2aIAwcOiMbGRsV2vV7f4oVeTk6OYj/mLWrNVrHFvEWtmYspY4cPH5b3d3R0tEl7ZWWl8PLykm8E6PV6k9uIjIyUY2RlZSneTmZmpuwTGRlpMje9Xi+GDBkiAAhvb29x584d6y+0i7FoYWenT5+WwbR48WLFPk1NTWL06NEyoOrr67t4luSInrRo8d5778kx8vPzFfvk5+dbfIJG3VtHXljaKm4MOa1v376ipqZGsc+XX34pxzl48KBV8yPHYK+iRX19vXyiN3r0aLNL/RcvXixvq6CgoEO3RY4hIyND3pfLly9X7MO8RR1hTWwxb1FHjRw5UgDNHxNpLTExUd7X+/btUzz/+vXrcsXG9OnTFftMmzZNFmOvX7+u2Gffvn0WC2aOghtx2ll6ero8XrhwoWIfJycnLFiwAABw7949ZGVldcXU6CkmhMCRI0cAAKNGjcLEiRMV+02cOBEjR44EABw5cgRCiC6bIzkeW8VNYWEhLl++DACYO3cu3NzcFMcx3sDx8OHDTzp96gGysrJw//59AEB8fDycnJSf5jC2nh5hYWHyuKSkxKSdeYs6qq3YshXmrZ6pd+/eAICHDx+atBleH3p6eiImJkbx/MGDByM8PBwAkJmZiaqqqhbtVVVVyMzMBACEh4dj8ODBiuPExMTA09MTgGPHFYsWdmbYmdjd3R0vvPCC2X6hoaHyODc3t9PnRU+3srIylJeXA2gZW0oM7Tdv3sS1a9c6e2rkwGwVN8Y7slsaZ+DAgdBqtQCY98g61saWTqeTLzoZW93bo0eP5LGzs7NJO/MWdVRbsWUrzFs9z9WrV3HhwgUAzcVUY/X19SgoKAAABAcHQ6PRmB3HEC+PHj3C2bNnW7SdOXMG9fX1Lfop0Wg0sph75swZNDQ0tO9iugiLFnZmqNoHBgZCrVab7Wcc0IZziADg0KFDGDNmDNzc3NC7d2+MGDEC8fHxFlfkXLp0SR63TpatMfbIwFZx05Fxrl+/jpqaGqvnSt3bwoUL4evrC41Gg2effRYTJ07EZ599hps3b1o8z9rYUqvVCAwMBMC81t3l5OTI49GjR5u0M29RR7UVW60xb5EltbW1KCoqwpYtWxAaGorGxkYAwIoVK1r0KywsRFNTE4Cuz1mNjY0oKiqyfCF2wqKFHT18+BAVFRUAYHbJjkGfPn3g7u4OoPlBkMjg0qVLuHz5Murq6lBdXY3i4mLs3r0bkydPRnR0tFxyaOzGjRvyuK3Y8/Pzk8eMvZ7NVnHTkXGEEC3Oo6dbdnY2/v33XzQ0NKCyshKnT5/Ghg0bEBgYiOTkZLPnGWLE3d0d3t7eFm/DEFt6vb7FO6rUfTx+/BibNm2Sv8+dO9ekD/MWdYQ1sdUa8xa1tmvXLqhUKqhUKri7u0Or1eKjjz7C7du3AQCrV6/G/PnzW5xjz5ylNI6jMP/WPnU6488eeXh4tNnf3d0dNTU1qK6u7sxpUTfh5uaGWbNmYcqUKRg1ahQ8PDyg1+uRk5OD7du3o7KyEunp6YiKisKJEyfwzDPPyHPbE3uGYhkAxl4PZ6u4YfyROcOHD0dMTAyCg4Plk6jS0lL8/PPPSE1NxcOHD7FkyRKoVCokJCSYnG+ILWsfUw2qq6vRq1cvG10FdZWvvvpKLqOOiYlR/Jgt8xZ1hDWxZcC8Re01fvx47NixA0FBQSZtzFnKWLSwI+ONVyx9XsnAkJjq6uo6bU7Ufdy8eVOxIh8REYHly5dj2rRpOH/+PHJycrBt2zZ88MEHsk97Ys/4AZGx17PZKm4Yf6QkOjoa8fHxUKlULf49KCgIb7zxBo4dO4aYmBg0NDRg5cqVmDVrFgYOHNiiryG22vOYCjC2uqOcnBysXr0aAODj44Nt27Yp9mPeovayNrYA5i2ybPbs2dDpdACa76+SkhIcPHgQhw8fRlxcHLZu3YqZM2e2OIc5Sxk/HmJHLi4u8tiwUYolhmVgrq6unTYn6j4sLSEcMGAAUlNT5eqKb775pkV7e2LPePkhY69ns1XcMP5IiZeXl8kTf2MzZ87EunXrADR/NjglJcWkjyG22vOYCjC2upu///4b0dHRaGxshIuLCw4dOgQfHx/Fvsxb1B7tiS2AeYss8/b2xtixYzF27FgEBQVh3rx5SEtLw+7du1FaWoqoqCjs2rWrxTnMWcpYtLAjw1fdANYtxTFs5mTN8jGi4cOHIyIiAgBQXFwsd08H2hd7xpuIMfZ6NlvFDeOPOiohIUG+QDDeJM/AEFvteUwFGFvdSVlZGaZOnYq7d+/C2dkZ+/fvxyuvvGK2P/MWWau9sWUt5i1q7c0338ScOXPw+PFjvP/++7hz545sY85SxqKFHbm4uKBfv34A0OZGTXfv3pUBZbxZCpElY8aMkcfGu1cbb8jTVuwZb8jD2OvZbBU3HRlHpVK1uZEUPf18fHzk46bSjvyGGKmpqcG9e/csjmWIrf79+/Nz4d1EeXk5wsPDUV5eDpVKhR9++AFRUVEWz2HeImt0JLasxbxFSgzxVVNTg19//VX+uz1zltI4joJFCzszvKgsLi6WX32j5MqVK/LYmq9dIgJgdsmicTHDOLaUMPbIwFZx05Fx/Pz8WmwURT2XpaXY1sZWY2MjSkpKADCvdRcVFRWIiIhAaWkpgOaPPS5YsKDN85i3qC0dja32YN6i1vr37y+P//nnH3ms1Wrh7OwMoOtzllqtxogRI9qYuX2waGFnISEhAJqrbH/++afZfsbLySZNmtTp86Kng/F3NPv6+srjYcOGyd+Vlioa+/333wEAgwYNgr+/v+0nSd2GreLGkPfaGufWrVsoLCwEwLxHzfR6vfyqcOOcZmBtbJ09e1auXmRsOb779+/jtddek49pmzZtwrJly6w6l3mLLHmS2LIW8xYpMV51Y/yRDI1GgxdffBEAkJ+fb3E/CkO89OrVS274aRAUFCQ34LQUV/X19Th16pQ8x/jbBh0JixZ2Nnv2bHm8c+dOxT6PHz/G7t27ATRv6BIWFtYVU6NurqysDCdOnAAABAQEYNCgQbJNpVLJZWlXrlyRyaq1U6dOyeprVFSUxXcK6Olnq7jRarXyHYGDBw+itrZWcRzjzamio6OfdPr0FNixYweEEACA0NBQk/ZXX30VXl5eAIAff/xR9m2NsdV91NbWYsaMGTh37hwAYM2aNVi1apXV5zNvkTlPGlvWYt4iJYcOHZLH48aNa9FmeH344MEDpKWlKZ5/48YNnDx5EgAwZcqUFntYAM17WkyZMgUAcPLkSbMfEUlLS8ODBw8AOHhcCbK7l19+WQAQarVa5OXlmbQnJSUJAAKAWL9+fddPkBzO0aNHRUNDg9n2W7duiQkTJsi42bx5s0mfq1evCmdnZwFA6HQ6UVtb26K9trZW6HQ6GZuFhYU2vw6yr7KyMhkj8fHxVp1jq7hJSUmRt71s2TKT9uLiYuHp6SkAiMDAQIvxTo6nvbFVVlYmzp07Z7FPRkaG0Gg0AoBwdXUVN27cUOy3du1aedtJSUkm7Xl5eUKtVgsAIjQ01JrLITt59OiRmDp1qrw/P/zwww6Nw7xFrdkitpi3SMnOnTtFXV2dxT5btmyR9/ewYcNEY2Nji/bKykrh5eUlAIihQ4eKioqKFu2NjY0iMjJSjpGVlaV4O5mZmbLPrFmzTG5Hr9eLIUOGCADC29tb3Llzp/0X3EVUQpgp51GXOX/+PCZNmoS6ujp4eHjg008/RVhYGOrq6rB//37s2LEDQHOV/+zZsyaVNOp5/P390dDQgNjYWAQHB8Pf3x+urq6oqKhAdnY2kpOT5VLEkJAQnDx5UnHDpk8++QSbNm0CAEyYMAGrVq1CQEAASkpKkJiYiPPnz8t+Gzdu7LoLpE7xxx9/oLi4WP5eUVGBjz/+GEDzUtN33323Rf+33npLcRxbxE1TUxNCQ0ORm5sLAIiNjcWiRYvQp08fFBQU4IsvvsB///0HJycnHDt2DNOmTXuia6fO9aSxlZ2djbCwMAQHByMyMhLPP/+8/JrB0tJSpKamIjU1Vb4D+e2332Lp0qWKc6mqqoJOp5NL9BMSEjBv3jy4uroiKysLGzduRHV1NVxdXZGXl4fx48fb4k9AnSA2Nla+yzh58mRs3brV4oo/jUYDrVar2Ma8RcZsEVvMW6TE398fVVVViI2NRUhICAICAuDh4YGqqipcvHgRe/bskTlEo9Hg+PHjCA8PNxknOTkZS5YsAdC8YnrNmjUYN24cysvLsXXrVmRlZQEA4uLisHfvXrPziYuLw/79+wEAYWFhWLFiBXx9fXHx4kVs2LBB7pOSnJyMhIQEm/4tbMq+NRMyOHr0qKzOK/1otVpRVFRk72mSgxg6dKjZWDH+iY2NFXfv3jU7TlNTk3j77bctjvHOO++Ipqamrrs46jTx8fFWxY3hxxxbxY1erxdBQUFmx+jVq5f4/vvvbf1noE7wpLGVlZVl1Xlubm4iOTm5zfkUFRWJESNGmB3H09NTZGRkdMafgmyoPTGF/78jaQ7zFhmzRWwxb5ESa5+jDx48WPz2228Wx1q3bp1QqVRmx5g+fXqbqzpqa2vF9OnTzY7h5OTULVbys2jhQK5duyZWrlwptFqtcHNzE97e3kKn04nExERRU1Nj7+mRA8nOzhaff/65eP3114VWqxV9+/YVarVaeHt7i3HjxonFixcrftTInOPHj4uoqCjh6+srNBqN8PX1FVFRUeKXX37pxKugrmarooWBLeKmoaFBfPfddyIkJET069dPuLi4iOHDh4tFixaJv/7660kul7rQk8bWgwcPxE8//SSWLVsmXnrpJTFkyBDh5uYmNBqNGDBggJg8ebLYsGGDuH37ttVzqq6uFomJiUKn0wlvb2/h5uYmRo4cKVauXCmuXbtmy8unTmLLooUB8xYJYZvYYt4iJVeuXBGbN28WMTEx4rnnnhMDBgwQarVa9O7dWwQEBIjY2Fixc+dOq1/b5ebmivnz5ws/Pz+h0WiEj4+PiIiIEHv37m3XvPbs2SMiIiKEj4+P0Gg0ws/PT8yfP79drxfsiR8PISIiIiIiIiKHxG8PISIiIiIiIiKHxKIFERERERERETkkFi2IiIiIiIiIyCGxaEFEREREREREDolFCyIiIiIiIiJySCxaEBEREREREZFDYtGCiIiIiIiIiBwSixZERERERERE5JBYtCAiIiIiIiIih8SiBRERERERERE5JBYtiIiIiIiIiMghsWhBRERERERERA6JRQsiIiIiIiIickgsWhARERERERGRQ2LRgoiIiIiIiIgcEosWREREREREROSQWLQgIiIiIiIiIofEogUREREREREROSQWLYiIiIiIiIjIIbFoQUREREREREQOiUULIiIiIiIiInJILFoQERERERERkUNi0YKIiIiIiIiIHBKLFkRERERERETkkP4HIYPQII9EkhcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 413,
       "width": 534
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelLoss = pd.DataFrame(rnnModel.history.history)\n",
    "modelLoss.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the weights are re-loaded into the model, five sequences of 100 numbers, each representing a 100-note sequence, are randomly drawn to be the input data for model prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "SEQSIZE = 100\n",
    "\n",
    "# Set up prediction input by mapping notes to integers.\n",
    "inputNotes = []\n",
    "\n",
    "for i in range(0, len(notes) - SEQSIZE):\n",
    "    batchInput = processedNotes[i: i+SEQSIZE]\n",
    "    inputNotes.append([dictRef[n] for n in batchInput])\n",
    "\n",
    "# reshape input and normalize\n",
    "batchNum = len(inputNotes)    \n",
    "inputNotes = np.reshape(inputNotes, (batchNum, SEQSIZE, 1)) / float(alphabetSize)\n",
    "\n",
    "# create model using weight.\n",
    "#rnnModel2 = createRNNModel(inputNotes, alphabetSize)\n",
    "#rnnModel2.load_weights('ChopinNocturnesLSTMx2.keras')\n",
    "\n",
    "# load the best model.\n",
    "rnnModel2 = load_model('ChopinNocturnesLSTMx2.keras')\n",
    "\n",
    "# inputs: 5 random sequences\n",
    "patterns = []\n",
    "for i in range(0, 5):\n",
    "    startNote = np.random.randint(0, len(inputNotes)-1)\n",
    "    patterns.append(list(inputNotes[startNote]))\n",
    "    \n",
    "dictRefReverse = dict((num, pitch) for num, pitch in enumerate(pitches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 predictions are made based on the 5 randomly selected input sequences. Each input is first reshaped and normalized before fed into the model. The prediction output is then converted to a note, and is used to generate the next note as part of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 song created.\n",
      "2 song created.\n",
      "3 song created.\n",
      "4 song created.\n",
      "5 song created.\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "songs = [[], [], [], [], []]\n",
    "\n",
    "# predict, generate output notes.\n",
    "# Generate 5 songs, each song has 800 notes.\n",
    "for j in range(5):\n",
    "    pat = deepcopy(patterns[j])\n",
    "\n",
    "    for _ in range(100):\n",
    "        # reshape input and predict\n",
    "        #print(pat[95:])\n",
    "        predIn = np.reshape(pat, (1, len(pat), 1)) #/ float(alphabetSize)\n",
    "        prediction = rnnModel2.predict(predIn, verbose=0)\n",
    "        \n",
    "        # Convert prediction back to notes\n",
    "        index = np.argmax(prediction)\n",
    "        prediction = dictRefReverse[index]\n",
    "        \n",
    "        # append to output, and use the new note for further generation.\n",
    "        songs[j].append(prediction)\n",
    "        #print(prediction, end=\" \")\n",
    "        pat.append(np.array([index/alphabetSize]))\n",
    "        pat = pat[1: len(pat)]\n",
    "    print('{} song created.'.format(j+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction outputs, each from a sequence of notes, are converted back to a stream of MIDI data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateMidi(outputNotes, fileName):\n",
    "    offset = 0\n",
    "    midiOutput = []\n",
    "\n",
    "    for notePat in outputNotes:\n",
    "\n",
    "        # It is a chord.\n",
    "        if ('.' in notePat) or notePat.isdigit():\n",
    "\n",
    "                # Split to notes\n",
    "                notes_in_chord = notePat.split('.')\n",
    "                notes = []\n",
    "\n",
    "                # chord as notes\n",
    "                for eachNote in notes_in_chord:\n",
    "                    new_note = note.Note(int(eachNote))\n",
    "                    new_note.storedInstrument = instrument.Piano()\n",
    "                    notes.append(new_note)\n",
    "\n",
    "                # Convert to midi chord and store\n",
    "                new_chord = chord.Chord(notes)\n",
    "                new_chord.offset = offset\n",
    "                midiOutput.append(new_chord)\n",
    "\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            # Convert to midi note and store\n",
    "            new_note = note.Note(notePat)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            midiOutput.append(new_note)\n",
    "\n",
    "        # increase offset to make sure notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    # Convert to midi stream\n",
    "    midiOutput = stream.Stream(midiOutput)\n",
    "    midiOutput.write('midi', fp=fileName)\n",
    "    \n",
    "for num, song in enumerate(songs):\n",
    "    generateMidi(song, 'ChopinNocturnesGenerated_{}.mid'.format(num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created MIDI parser on file ChopinNocturnesGenerated_4.mid\n",
      "BPM: 120\n",
      "Time Signature: 4 / 4\n",
      "Arranging notes... 132 / 132\n",
      "Playing song...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([    0,   147,   295, ..., -4170, -4164, -4158], dtype=int16),\n",
       " <simpleaudio.shiny.PlayObject at 0x77764e8675b0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listen to the midi music, uncomment the file wish to listen.\n",
    "# ps = Parser(\"ChopinValseGenerated_0.mid\")\n",
    "# ps = Parser(\"ChopinValseGenerated_1.mid\")\n",
    "# ps = Parser(\"ChopinValseGenerated_2.mid\")\n",
    "# ps = Parser(\"ChopinValseGenerated_3.mid\")\n",
    "# ps = Parser(\"ChopinValseGenerated_4.mid\")\n",
    "# ps = Parser(\"ChopinNocturnesGenerated_0.mid\")\n",
    "# ps = Parser(\"ChopinNocturnesGenerated_1.mid\")\n",
    "# ps = Parser(\"ChopinNocturnesGenerated_2.mid\")\n",
    "# ps = Parser(\"ChopinNocturnesGenerated_3.mid\")\n",
    "ps = Parser(\"ChopinNocturnesGenerated_4.mid\")\n",
    "\n",
    "play_notes(*ps.parse(), np.sin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Music generation is a new emerging field with recent advance in deep learning. As a crossroad of technology and art, it extends the ability of artists and brings new elements of style and tempo into music creation. In this music generation project, an LSTM-based RNN model is designed, implemented and trained with a set of 10 Chopin's Valse, and separately, a set of 8 Chopin's Nocturnes. While the generated music pieces are definitely of no match to Frédéric Chopin's original pieces, it provides an insight to the ability of deep learning in art generation, and has proven that music pieces generated by deep learning models are non-trivial and possess a considerable degree of artistic values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da5401",
   "language": "python",
   "name": "da5401"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
