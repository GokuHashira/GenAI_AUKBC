{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(4, use_bias=False, input_shape=(2,2)))\n",
    "model.add(Dense(3, use_bias=False, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdown.download(url=\"https://drive.google.com/file/d/1ddCfXSHR5LW2zHO52zIuQd9HP3ArNuLA/view?usp=sharing\", output=\"3Gram_love_data.txt\", fuzzy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['w1', 'w2', 'w3']\n",
    "trigrams = pd.read_csv('3Gram_love_data.txt', delimiter='\\t', names=column_names)\n",
    "print('shape of the data', trigrams.shape)\n",
    "print('random sample:\\n', trigrams.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = []\n",
    "for i in list(trigrams.columns.values):\n",
    "    for j in pd.unique(trigrams[i]):\n",
    "        unique_words.append(j)\n",
    "unique_words = np.unique(unique_words)\n",
    "\n",
    "print('count of unique words:', len(unique_words))\n",
    "print('unique word list:', unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indices = dict((w, i) for i, w in enumerate(unique_words))\n",
    "indices_words = dict((i, w) for i, w in enumerate(unique_words))\n",
    "\n",
    "print(\"word_indices dictionary\\n\", word_indices)\n",
    "print(\"word_indices.keys\\n\", word_indices.keys())\n",
    "print(\"word_indices.values\\n\", word_indices.values())\n",
    "print(\"\\n\" + \"#\"*50)\n",
    "print(\"indices_words dictionary\\n\", indices_words)\n",
    "print(\"indices_words.keys\\n\", indices_words.keys())\n",
    "print(\"indices_words.values\\n\", indices_words.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_w2 = trigrams[['w1','w2']]\n",
    "for i in list(w1_w2.columns.values):\n",
    "    w1_w2[i] = w1_w2[i].map(word_indices)\n",
    "w1_w2 = np.array(w1_w2)\n",
    "w1_w2 = np.reshape(w1_w2, (w1_w2.shape[0], 2, 1))\n",
    "w1_w2_hot = keras.utils.to_categorical(np.array(w1_w2), num_classes=len(word_indices))\n",
    "print(\"word1_word2_onehot shape\", w1_w2_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w3 = trigrams['w3'].map(word_indices)\n",
    "w3_hot = keras.utils.to_categorical(np.array(w3), num_classes=len(word_indices))\n",
    "print(\"word3_onehot shape is\", w3_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"time steps\", w1_w2_hot.shape[1])\n",
    "print(\"input nodes\", w1_w2_hot.shape[2])\n",
    "print(\"output nodes\", w3_hot.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = Sequential()\n",
    "model_rnn.add(SimpleRNN(30, input_shape=(w1_w2_hot.shape[1], w1_w2_hot.shape[2])))\n",
    "model_rnn.add(Dense(w3_hot.shape[1], activation='softmax'))\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_rnn.fit(w1_w2_hot, w3_hot, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_word_pred(in_text):\n",
    "    print(\"input is\", in_text)\n",
    "    encoded = [word_indices[i] for i in in_text]\n",
    "    encoded = np.array(encoded).reshape(1,2,1)\n",
    "    encoded = keras.utils.to_categorical(np.array(encoded), num_classes=len(word_indices))\n",
    "    yhat = np.argmax(model_rnn.predict(encoded, verbose=0))\n",
    "    print(\"output -->\", indices_words[yhat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_word_pred(['hate','you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data file.\n",
    "gdown.download(url=\"https://drive.google.com/file/d/1OURGEflZRYGUwCxL-uwgKHBmK6aVQF99/view?usp=drive_link\", output=\"Long_sequence_3gram.csv\", fuzzy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longseq = open('Long_sequence_3gram.csv').read().lower()\n",
    "print(longseq[495:801])\n",
    "print(longseq[30615:31000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longseq = longseq.replace(',',' ').replace('\\r','')\n",
    "print(longseq[495:801])\n",
    "print(longseq[30615:31000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(longseq)))\n",
    "print('unique chars \\n', chars)\n",
    "chars.remove('\\n')\n",
    "print('unique after removing newline \\n', chars)\n",
    "print('overal char count', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = dict((c,i) for i,c in enumerate(chars))\n",
    "print(\"chars to indices\\n\", char_indices)\n",
    "indices_char = dict((i,c) for i,c, in enumerate(chars))\n",
    "print(\"indices to chars\\n\", indices_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = longseq.splitlines()\n",
    "# add a white space at the end of every line.\n",
    "data = [i+' ' for i in data]\n",
    "# encoding the letters to numbers\n",
    "sentence = [[char_indices[j] for j in i] for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0], sentence[0])\n",
    "print(data[9000], sentence[9000])\n",
    "print(\"#data \", len(data))\n",
    "print(\"#sentences \", len(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 14\n",
    "X = []\n",
    "y = []\n",
    "for i in sentence:\n",
    "    for j in range(len(i)-seq_len):\n",
    "        X.append(i[j:j+seq_len])\n",
    "        y.append(i[j+seq_len])\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data[0:2]=\", data[0:2])\n",
    "print(\"sentence[0:2]=\", sentence[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,20):\n",
    "    print(\"X[\", i, \"]=\", X[i], \"y[\", i, \"]\", y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "X1 = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "X1 = keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n",
    "print(X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = np.array(y)\n",
    "y1 = keras.utils.to_categorical(np.array(y1), num_classes=len(char_indices))\n",
    "print(y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X1[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn2 = Sequential()\n",
    "# SimpleRNN(#hidden_nodes, input_shape=(timesteps, data_dim))\n",
    "model_rnn2.add(SimpleRNN(16,input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model_rnn2.add(Dense(len(char_indices)))\n",
    "model_rnn2.add(keras.layers.Activation('softmax'))\n",
    "model_rnn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_rnn2.fit(X_train, y_train, epochs=20, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn2.save_weights(\"char_rnn_model_weights_v1.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(in_text):\n",
    "    X1 = np.array([char_indices[i] for i in in_text]).reshape(1, 14, 1)\n",
    "    X1 = keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n",
    "    return (X1)\n",
    "\n",
    "def complete_pred(in_text):\n",
    "    completion = ' '\n",
    "    while True:\n",
    "        x = prepare_input(in_text)\n",
    "        pred = np.argmax(model_rnn2.predict(x, verbose=0))\n",
    "        next_char = indices_char[pred]\n",
    "        in_text = in_text[1:] + next_char\n",
    "        completion += next_char\n",
    "        \n",
    "        if (len(completion) > 20 or next_char == ' '):\n",
    "            return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_pred('of particular ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(128, activation='tanh', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model_lstm.add(Dense(len(char_indices)))\n",
    "model_lstm.add(Activation('softmax'))\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.fit(X_train, y_train, epochs=20, verbose=1, batch_size=256)\n",
    "model_lstm.save_weights(\"char_lstm_model_weights_v2.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input1(in_text):\n",
    "    X1 = np.array([char_indices[i] for i in in_text]).reshape(1, 14, 1)\n",
    "    X1 = keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n",
    "    return (X1)\n",
    "\n",
    "def complete_pred1(in_text):\n",
    "    completion = ' '\n",
    "    while True:\n",
    "        x = prepare_input1(in_text)\n",
    "        pred = np.argmax(model_lstm.predict(x, verbose=0))\n",
    "        next_char = indices_char[pred]\n",
    "        in_text = in_text[1:] + next_char\n",
    "        completion += next_char\n",
    "        \n",
    "        if (len(completion) > 20 or next_char == ' '):\n",
    "            return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_pred1(\"advice is for \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da5401",
   "language": "python",
   "name": "da5401"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
