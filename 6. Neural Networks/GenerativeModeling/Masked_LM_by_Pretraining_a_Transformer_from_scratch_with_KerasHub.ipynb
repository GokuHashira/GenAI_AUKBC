{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M4uLpSKiZxg"
      },
      "source": [
        "# Pretraining a Transformer from scratch with KerasHub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJoLbBtUiZxi"
      },
      "source": [
        "KerasHub aims to make it easy to build state-of-the-art text processing models. In this\n",
        "guide, we will show how library components simplify pretraining and fine-tuning a\n",
        "Transformer model from scratch.\n",
        "\n",
        "This guide is broken into three parts:\n",
        "\n",
        "1. *Setup*, task definition, and establishing a baseline.\n",
        "2. *Pretraining* a Transformer model.\n",
        "3. *Fine-tuning* the Transformer model on our classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyKkHGMoiZxj"
      },
      "source": [
        "## Setup\n",
        "\n",
        "The following guide uses Keras 3 to work in any of `tensorflow`, `jax` or\n",
        "`torch`. We select the `jax` backend below, which will give us a particularly\n",
        "fast train step below, but feel free to mix it up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrZo0Dv5iZxj"
      },
      "outputs": [],
      "source": [
        "#!pip install -q --upgrade keras-hub\n",
        "#!pip install -q --upgrade keras  # Upgrade to Keras 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "57i5PMPuiZxk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n",
        "\n",
        "\n",
        "import keras_hub\n",
        "import tensorflow as tf\n",
        "import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOjadAmeiZxk"
      },
      "source": [
        "Next up, we can download two datasets.\n",
        "\n",
        "- [SST-2](https://paperswithcode.com/sota/sentiment-analysis-on-sst-2-binary) a text\n",
        "classification dataset and our \"end goal\". This dataset is often used to benchmark\n",
        "language models.\n",
        "- [WikiText-103](https://paperswithcode.com/dataset/wikitext-103): A medium sized\n",
        "collection of featured articles from English Wikipedia, which we will use for\n",
        "pretraining.\n",
        "\n",
        "Finally, we will download a WordPiece vocabulary, to do sub-word tokenization later on in\n",
        "this guide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZrpsPUE6iZxl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f34dcb2-faf7-4410-af01-6f8641f201cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.keras/datasets/wikitext-103.tar.gz\n",
            "/root/.keras/datasets/SST-2.zip\n"
          ]
        }
      ],
      "source": [
        "## Download pretraining data.\n",
        "wtext = keras.utils.get_file(origin=\"https://dax-cdn.cdn.appdomain.cloud/dax-wikitext-103/1.0.1/wikitext-103.tar.gz\",\n",
        "    extract=True,\n",
        ")\n",
        "print(wtext)\n",
        "wiki_dir = wtext + \"/wikitext-103/\" #os.path.expanduser(\"~/.keras/datasets/wikitext-103-raw/\")\n",
        "\n",
        "# Download finetuning data.\n",
        "sst = keras.utils.get_file(\n",
        "    origin=\"https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "print(sst)\n",
        "\n",
        "sst_dir = sst + \"/SST-2/\" #os.path.expanduser(\"~/.keras/datasets/SST-2/\")\n",
        "\n",
        "# Download vocabulary data.\n",
        "vocab_file = keras.utils.get_file(\n",
        "    origin=\"https://storage.googleapis.com/tensorflow/keras-nlp/examples/bert/bert_vocab_uncased.txt\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA9PFU32iZxl"
      },
      "source": [
        "Next, we define some hyperparameters we will use during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PObLjgTPiZxl"
      },
      "outputs": [],
      "source": [
        "# Preprocessing params.\n",
        "PRETRAINING_BATCH_SIZE = 128\n",
        "FINETUNING_BATCH_SIZE = 32\n",
        "SEQ_LENGTH = 128\n",
        "MASK_RATE = 0.25\n",
        "PREDICTIONS_PER_SEQ = 32\n",
        "\n",
        "# Model params.\n",
        "NUM_LAYERS = 3\n",
        "MODEL_DIM = 256\n",
        "INTERMEDIATE_DIM = 512\n",
        "NUM_HEADS = 4\n",
        "DROPOUT = 0.1\n",
        "NORM_EPSILON = 1e-5\n",
        "\n",
        "# Training params.\n",
        "PRETRAINING_LEARNING_RATE = 5e-4\n",
        "PRETRAINING_EPOCHS = 8\n",
        "FINETUNING_LEARNING_RATE = 5e-5\n",
        "FINETUNING_EPOCHS = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrTE_a6yiZxm"
      },
      "source": [
        "### Load data\n",
        "\n",
        "We load our data with [tf.data](https://www.tensorflow.org/guide/data), which will allow\n",
        "us to define input pipelines for tokenizing and preprocessing text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JA0DAgWriZxm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d1485b3-4306-43f1-a72c-3db511bf3a90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(4,), dtype=string, numpy=\n",
            "array([b'hide new secretions from the parental units ',\n",
            "       b'contains no wit , only labored gags ',\n",
            "       b'that loves its characters and communicates something rather beautiful about human nature ',\n",
            "       b'remains utterly satisfied to remain the same throughout '],\n",
            "      dtype=object)>, <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 1, 0], dtype=int32)>)\n",
            "tf.Tensor(\n",
            "[b' Senj\\xc5\\x8d no Valkyria 3 : <unk> Chronicles ( Japanese : \\xe6\\x88\\xa6\\xe5\\xa0\\xb4\\xe3\\x81\\xae\\xe3\\x83\\xb4\\xe3\\x82\\xa1\\xe3\\x83\\xab\\xe3\\x82\\xad\\xe3\\x83\\xa5\\xe3\\x83\\xaa\\xe3\\x82\\xa23 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . '\n",
            " b\" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \"\n",
            " b\" It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \"\n",
            " b\" As with previous <unk> Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . \"], shape=(4,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "# Load SST-2.\n",
        "sst_train_ds = tf.data.experimental.CsvDataset(\n",
        "    sst_dir + \"train.tsv\", [tf.string, tf.int32], header=True, field_delim=\"\\t\"\n",
        ").batch(FINETUNING_BATCH_SIZE)\n",
        "sst_val_ds = tf.data.experimental.CsvDataset(\n",
        "    sst_dir + \"dev.tsv\", [tf.string, tf.int32], header=True, field_delim=\"\\t\"\n",
        ").batch(FINETUNING_BATCH_SIZE)\n",
        "\n",
        "# Load wikitext-103 and filter out short lines.\n",
        "wiki_train_ds = (\n",
        "    tf.data.TextLineDataset(wiki_dir + \"wiki.train.tokens\")\n",
        "    .filter(lambda x: tf.strings.length(x) > 100)\n",
        "    .batch(PRETRAINING_BATCH_SIZE)\n",
        ")\n",
        "wiki_val_ds = (\n",
        "    tf.data.TextLineDataset(wiki_dir + \"wiki.valid.tokens\")\n",
        "    .filter(lambda x: tf.strings.length(x) > 100)\n",
        "    .batch(PRETRAINING_BATCH_SIZE)\n",
        ")\n",
        "\n",
        "# Take a peak at the sst-2 dataset.\n",
        "print(sst_train_ds.unbatch().batch(4).take(1).get_single_element())\n",
        "print(wiki_train_ds.unbatch().batch(4).take(1).get_single_element())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RlDdy6piZxm"
      },
      "source": [
        "You can see that our `SST-2` dataset contains relatively short snippets of movie review\n",
        "text. Our goal is to predict the sentiment of the snippet. A label of 1 indicates\n",
        "positive sentiment, and a label of 0 negative sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNN8LpLhiZxm"
      },
      "source": [
        "### Establish a baseline\n",
        "\n",
        "As a first step, we will establish a baseline of good performance. We don't actually need\n",
        "KerasHub for this, we can just use core Keras layers.\n",
        "\n",
        "We will train a simple bag-of-words model, where we learn a positive or negative weight\n",
        "for each word in our vocabulary. A sample's score is simply the sum of the weights of all\n",
        "words that are present in the sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FErTHI9biZxm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae7b767a-4cc5-4784-ea1c-7e74d8432a1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "   2105/Unknown \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6421 - loss: 0.6476"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2105/2105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6421 - loss: 0.6476 - val_accuracy: 0.7557 - val_loss: 0.5374\n",
            "Epoch 2/5\n",
            "\u001b[1m2105/2105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7517 - loss: 0.5394 - val_accuracy: 0.7764 - val_loss: 0.4892\n",
            "Epoch 3/5\n",
            "\u001b[1m2105/2105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7829 - loss: 0.4873 - val_accuracy: 0.7959 - val_loss: 0.4686\n",
            "Epoch 4/5\n",
            "\u001b[1m2105/2105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7981 - loss: 0.4544 - val_accuracy: 0.8005 - val_loss: 0.4595\n",
            "Epoch 5/5\n",
            "\u001b[1m2105/2105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8098 - loss: 0.4314 - val_accuracy: 0.8005 - val_loss: 0.4564\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79fec0959950>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# This layer will turn our input sentence into a list of 1s and 0s the same size\n",
        "# our vocabulary, indicating whether a word is present in absent.\n",
        "multi_hot_layer = keras.layers.TextVectorization(\n",
        "    max_tokens=4000, output_mode=\"multi_hot\"\n",
        ")\n",
        "multi_hot_layer.adapt(sst_train_ds.map(lambda x, y: x))\n",
        "multi_hot_ds = sst_train_ds.map(lambda x, y: (multi_hot_layer(x), y))\n",
        "multi_hot_val_ds = sst_val_ds.map(lambda x, y: (multi_hot_layer(x), y))\n",
        "\n",
        "# We then learn a linear regression over that layer, and that's our entire\n",
        "# baseline model!\n",
        "\n",
        "inputs = keras.Input(shape=(4000,), dtype=\"int32\")\n",
        "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(inputs)\n",
        "baseline_model = keras.Model(inputs, outputs)\n",
        "baseline_model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "baseline_model.fit(multi_hot_ds, validation_data=multi_hot_val_ds, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKQR4pTLiZxm"
      },
      "source": [
        "A bag-of-words approach can be a fast and surprisingly powerful, especially when input\n",
        "examples contain a large number of words. With shorter sequences, it can hit a\n",
        "performance ceiling.\n",
        "\n",
        "To do better, we would like to build a model that can evaluate words *in context*. Instead\n",
        "of evaluating each word in a void, we need to use the information contained in the\n",
        "*entire ordered sequence* of our input.\n",
        "\n",
        "This runs us into a problem. `SST-2` is very small dataset, and there's simply not enough\n",
        "example text to attempt to build a larger, more parameterized model that can learn on a\n",
        "sequence. We would quickly start to overfit and memorize our training set, without any\n",
        "increase in our ability to generalize to unseen examples.\n",
        "\n",
        "Enter **pretraining**, which will allow us to learn on a larger corpus, and transfer our\n",
        "knowledge to the `SST-2` task. And enter **KerasHub**, which will allow us to pretrain a\n",
        "particularly powerful model, the Transformer, with ease."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNmVA_5-iZxm"
      },
      "source": [
        "## Pretraining\n",
        "\n",
        "To beat our baseline, we will leverage the `WikiText103` dataset, an unlabeled\n",
        "collection of Wikipedia articles that is much bigger than `SST-2`.\n",
        "\n",
        "We are going to train a *transformer*, a highly expressive model which will learn\n",
        "to embed each word in our input as a low dimensional vector. Our wikipedia dataset has no\n",
        "labels, so we will use an unsupervised training objective called the *Masked Language\n",
        "Modeling* (MaskedLM) objective.\n",
        "\n",
        "Essentially, we will be playing a big game of \"guess the missing word\". For each input\n",
        "sample we will obscure 25% of our input data, and train our model to predict the parts we\n",
        "covered up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-IS-ArriZxn"
      },
      "source": [
        "### Preprocess data for the MaskedLM task\n",
        "\n",
        "Our text preprocessing for the MaskedLM task will occur in two stages.\n",
        "\n",
        "1. Tokenize input text into integer sequences of token ids.\n",
        "2. Mask certain positions in our input to predict on.\n",
        "\n",
        "To tokenize, we can use a `keras_hub.tokenizers.Tokenizer` -- the KerasHub building block\n",
        "for transforming text into sequences of integer token ids.\n",
        "\n",
        "In particular, we will use `keras_hub.tokenizers.WordPieceTokenizer` which does\n",
        "*sub-word* tokenization. Sub-word tokenization is popular when training models on large\n",
        "text corpora. Essentially, it allows our model to learn from uncommon words, while not\n",
        "requiring a massive vocabulary of every word in our training set.\n",
        "\n",
        "The second thing we need to do is mask our input for the MaskedLM task. To do this, we can use\n",
        "`keras_hub.layers.MaskedLMMaskGenerator`, which will randomly select a set of tokens in each\n",
        "input and mask them out.\n",
        "\n",
        "The tokenizer and the masking layer can both be used inside a call to\n",
        "[tf.data.Dataset.map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map).\n",
        "We can use `tf.data` to efficiently pre-compute each batch on the CPU, while our GPU or TPU\n",
        "works on training with the batch that came before. Because our masking layer will\n",
        "choose new words to mask each time, each epoch over our dataset will give us a totally\n",
        "new set of labels to train on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "s_2W1C10iZxn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9676fa3-b1a3-472a-b187-10e9739b22b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'token_ids': <tf.Tensor: shape=(128, 128), dtype=int32, numpy=\n",
            "array([[ 7570,   103,  2271, ...,   103,  1012,   103],\n",
            "       [  103,  7849,  2271, ...,  1007,  1012,  2023],\n",
            "       [ 1996,  2034,  3940, ...,     0,     0,     0],\n",
            "       ...,\n",
            "       [ 2076,  1996,  2307, ...,     0,     0,     0],\n",
            "       [ 3216,  2225,  2083, ...,     0,     0,     0],\n",
            "       [  103,  2007, 29963, ...,     0,     0,     0]], dtype=int32)>, 'mask_positions': <tf.Tensor: shape=(128, 32), dtype=int64, numpy=\n",
            "array([[  0,   1,   4, ..., 124, 125, 127],\n",
            "       [  0,   3,   4, ..., 115, 123, 124],\n",
            "       [  4,   5,  13, ...,   0,   0,   0],\n",
            "       ...,\n",
            "       [  3,  20,  27, ..., 111, 118,   0],\n",
            "       [  6,   9,  11, ...,   0,   0,   0],\n",
            "       [  0,   2,   5, ...,   0,   0,   0]])>}, <tf.Tensor: shape=(128, 32), dtype=int32, numpy=\n",
            "array([[ 7570,  7849,  7946, ..., 25009,  9673,  7570],\n",
            "       [ 7570, 13091,  7946, ...,  1037, 11314,  2075],\n",
            "       [23976,  3695,  2004, ...,     0,     0,     0],\n",
            "       ...,\n",
            "       [ 6245,  2021,  3145, ...,  3005,  7511,     0],\n",
            "       [ 1010,  2379,  1010, ...,     0,     0,     0],\n",
            "       [ 9794,  1045,  1030, ...,     0,     0,     0]], dtype=int32)>, <tf.Tensor: shape=(128, 32), dtype=float32, numpy=\n",
            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 1., 1., 1.],\n",
            "       [1., 1., 1., ..., 0., 0., 0.],\n",
            "       ...,\n",
            "       [1., 1., 1., ..., 1., 1., 0.],\n",
            "       [1., 1., 1., ..., 0., 0., 0.],\n",
            "       [1., 1., 1., ..., 0., 0., 0.]], dtype=float32)>)\n"
          ]
        }
      ],
      "source": [
        "# Setting sequence_length will trim or pad the token outputs to shape\n",
        "# (batch_size, SEQ_LENGTH).\n",
        "tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab_file,\n",
        "    sequence_length=SEQ_LENGTH,\n",
        "    lowercase=True,\n",
        "    strip_accents=True,\n",
        ")\n",
        "# Setting mask_selection_length will trim or pad the mask outputs to shape\n",
        "# (batch_size, PREDICTIONS_PER_SEQ).\n",
        "masker = keras_hub.layers.MaskedLMMaskGenerator(\n",
        "    vocabulary_size=tokenizer.vocabulary_size(),\n",
        "    mask_selection_rate=MASK_RATE,\n",
        "    mask_selection_length=PREDICTIONS_PER_SEQ,\n",
        "    mask_token_id=tokenizer.token_to_id(\"[MASK]\"),\n",
        ")\n",
        "\n",
        "\n",
        "def preprocess(inputs):\n",
        "    inputs = tokenizer(inputs)\n",
        "    outputs = masker(inputs)\n",
        "    # Split the masking layer outputs into a (features, labels, and weights)\n",
        "    # tuple that we can use with keras.Model.fit().\n",
        "    features = {\n",
        "        \"token_ids\": outputs[\"token_ids\"],\n",
        "        \"mask_positions\": outputs[\"mask_positions\"],\n",
        "    }\n",
        "    labels = outputs[\"mask_ids\"]\n",
        "    weights = outputs[\"mask_weights\"]\n",
        "    return features, labels, weights\n",
        "\n",
        "\n",
        "# We use prefetch() to pre-compute preprocessed batches on the fly on the CPU.\n",
        "pretrain_ds = wiki_train_ds.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "pretrain_val_ds = wiki_val_ds.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Preview a single input example.\n",
        "# The masks will change each time you run the cell.\n",
        "print(pretrain_val_ds.take(1).get_single_element())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LwB-Y4piZxn"
      },
      "source": [
        "The above block sorts our dataset into a `(features, labels, weights)` tuple, which can be\n",
        "passed directly to `keras.Model.fit()`.\n",
        "\n",
        "We have two features:\n",
        "\n",
        "1. `\"token_ids\"`, where some tokens have been replaced with our mask token id.\n",
        "2. `\"mask_positions\"`, which keeps track of which tokens we masked out.\n",
        "\n",
        "Our labels are simply the ids we masked out.\n",
        "\n",
        "Because not all sequences will have the same number of masks, we also keep a\n",
        "`sample_weight` tensor, which removes padded labels from our loss function by giving them\n",
        "zero weight."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi2tlFCkiZxn"
      },
      "source": [
        "### Create the Transformer encoder\n",
        "\n",
        "KerasHub provides all the building blocks to quickly build a Transformer encoder.\n",
        "\n",
        "We use `keras_hub.layers.TokenAndPositionEmbedding` to first embed our input token ids.\n",
        "This layer simultaneously learns two embeddings -- one for words in a sentence and another\n",
        "for integer positions in a sentence. The output embedding is simply the sum of the two.\n",
        "\n",
        "Then we can add a series of `keras_hub.layers.TransformerEncoder` layers. These are the\n",
        "bread and butter of the Transformer model, using an attention mechanism to attend to\n",
        "different parts of the input sentence, followed by a multi-layer perceptron block.\n",
        "\n",
        "The output of this model will be a encoded vector per input token id. Unlike the\n",
        "bag-of-words model we used as a baseline, this model will embed each token accounting for\n",
        "the context in which it appeared."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "y3Q6c8j9iZxn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "0aaf6005-1872-4287-efab-b0933694c144"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ token_and_position_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m7,846,400\u001b[0m │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)     │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m527,104\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m527,104\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m527,104\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ token_and_position_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,846,400</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)     │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_encoder_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,428,224\u001b[0m (35.97 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,428,224</span> (35.97 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,428,224\u001b[0m (35.97 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,428,224</span> (35.97 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(SEQ_LENGTH,), dtype=\"int32\")\n",
        "\n",
        "# Embed our tokens with a positional embedding.\n",
        "embedding_layer = keras_hub.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=tokenizer.vocabulary_size(),\n",
        "    sequence_length=SEQ_LENGTH,\n",
        "    embedding_dim=MODEL_DIM,\n",
        ")\n",
        "outputs = embedding_layer(inputs)\n",
        "\n",
        "# Apply layer normalization and dropout to the embedding.\n",
        "outputs = keras.layers.LayerNormalization(epsilon=NORM_EPSILON)(outputs)\n",
        "outputs = keras.layers.Dropout(rate=DROPOUT)(outputs)\n",
        "\n",
        "# Add a number of encoder blocks\n",
        "for i in range(NUM_LAYERS):\n",
        "    outputs = keras_hub.layers.TransformerEncoder(\n",
        "        intermediate_dim=INTERMEDIATE_DIM,\n",
        "        num_heads=NUM_HEADS,\n",
        "        dropout=DROPOUT,\n",
        "        layer_norm_epsilon=NORM_EPSILON,\n",
        "    )(outputs)\n",
        "\n",
        "encoder_model = keras.Model(inputs, outputs)\n",
        "encoder_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqom7ywriZxn"
      },
      "source": [
        "### Pretrain the Transformer\n",
        "\n",
        "You can think of the `encoder_model` as it's own modular unit, it is the piece of our\n",
        "model that we are really interested in for our downstream task. However we still need to\n",
        "set up the encoder to train on the MaskedLM task; to do that we attach a\n",
        "`keras_hub.layers.MaskedLMHead`.\n",
        "\n",
        "This layer will take as one input the token encodings, and as another the positions we\n",
        "masked out in the original input. It will gather the token encodings we masked, and\n",
        "transform them back in predictions over our entire vocabulary.\n",
        "\n",
        "With that, we are ready to compile and run pretraining. If you are running this in a\n",
        "Colab, note that this will take about an hour. Training Transformer is famously compute\n",
        "intensive, so even this relatively small Transformer will take some time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "fs7qRr4JiZxo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4968d72-c817-4998-e1c9-e900b5203cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 16s/step - loss: 8.7373 - sparse_categorical_accuracy: 0.0015 - val_loss: 8.2526 - val_sparse_categorical_accuracy: 0.0480\n",
            "Epoch 2/8\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - loss: 8.5343 - sparse_categorical_accuracy: 0.0484 - val_loss: 8.0150 - val_sparse_categorical_accuracy: 0.0513\n",
            "Epoch 3/8\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 8.0048 - sparse_categorical_accuracy: 0.0585 - val_loss: 7.7771 - val_sparse_categorical_accuracy: 0.0533\n",
            "Epoch 4/8\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 8.6114 - sparse_categorical_accuracy: 0.0545 - val_loss: 7.5854 - val_sparse_categorical_accuracy: 0.0521\n",
            "Epoch 5/8\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 7.6198 - sparse_categorical_accuracy: 0.0522 - val_loss: 7.3937 - val_sparse_categorical_accuracy: 0.0497\n",
            "Epoch 6/8\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - loss: 7.6481 - sparse_categorical_accuracy: 0.0496 - val_loss: 7.2078 - val_sparse_categorical_accuracy: 0.0472\n",
            "Epoch 7/8\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - loss: 6.7380 - sparse_categorical_accuracy: 0.0463 - val_loss: 7.0276 - val_sparse_categorical_accuracy: 0.0446\n",
            "Epoch 8/8\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 5.6790 - sparse_categorical_accuracy: 0.0481 - val_loss: 6.8833 - val_sparse_categorical_accuracy: 0.0438\n"
          ]
        }
      ],
      "source": [
        "# Create the pretraining model by attaching a masked language model head.\n",
        "inputs = {\n",
        "    \"token_ids\": keras.Input(shape=(SEQ_LENGTH,), dtype=\"int32\", name=\"token_ids\"),\n",
        "    \"mask_positions\": keras.Input(\n",
        "        shape=(PREDICTIONS_PER_SEQ,), dtype=\"int32\", name=\"mask_positions\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "# Encode the tokens.\n",
        "encoded_tokens = encoder_model(inputs[\"token_ids\"])\n",
        "\n",
        "# Predict an output word for each masked input token.\n",
        "# We use the input token embedding to project from our encoded vectors to\n",
        "# vocabulary logits, which has been shown to improve training efficiency.\n",
        "outputs = keras_hub.layers.MaskedLMHead(\n",
        "    token_embedding=embedding_layer.token_embedding,\n",
        "    activation=\"softmax\",\n",
        ")(encoded_tokens, mask_positions=inputs[\"mask_positions\"])\n",
        "\n",
        "# Define and compile our pretraining model.\n",
        "pretraining_model = keras.Model(inputs, outputs)\n",
        "pretraining_model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.AdamW(PRETRAINING_LEARNING_RATE),\n",
        "    weighted_metrics=[\"sparse_categorical_accuracy\"],\n",
        "    jit_compile=True,\n",
        ")\n",
        "\n",
        "# Pretrain the model on our wiki text dataset.\n",
        "pretraining_model.fit(\n",
        "    pretrain_ds,\n",
        "    validation_data=pretrain_val_ds,\n",
        "    epochs=PRETRAINING_EPOCHS,\n",
        "    steps_per_epoch=2,\n",
        ")\n",
        "\n",
        "# Save this base model for further finetuning.\n",
        "encoder_model.save(\"encoder_model.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rewxp-0EiZxo"
      },
      "source": [
        "## Fine-tuning\n",
        "\n",
        "After pretraining, we can now fine-tune our model on the `SST-2` dataset. We can\n",
        "leverage the ability of the encoder we build to predict on words in context to boost\n",
        "our performance on the downstream task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNAOcqOjiZxo"
      },
      "source": [
        "### Preprocess data for classification\n",
        "\n",
        "Preprocessing for fine-tuning is much simpler than for our pretraining MaskedLM task. We just\n",
        "tokenize our input sentences and we are ready for training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "dlFatyA9iZxo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "007fe820-2fdd-43c5-9ff6-0c0ee302bb01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(32, 128), dtype=int32, numpy=\n",
            "array([[ 2009,  1005,  1055, ...,     0,     0,     0],\n",
            "       [ 4895, 10258,  2378, ...,     0,     0,     0],\n",
            "       [ 4473,  2149,  2000, ...,     0,     0,     0],\n",
            "       ...,\n",
            "       [ 1045,  2018,  2000, ...,     0,     0,     0],\n",
            "       [ 4283,  2000,  3660, ...,     0,     0,     0],\n",
            "       [ 1012,  1012,  1012, ...,     0,     0,     0]], dtype=int32)>, <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
            "array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
            "       0, 1, 1, 0, 0, 1, 0, 0, 1, 0], dtype=int32)>)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def preprocess(sentences, labels):\n",
        "    return tokenizer(sentences), labels\n",
        "\n",
        "\n",
        "# We use prefetch() to pre-compute preprocessed batches on the fly on our CPU.\n",
        "finetune_ds = sst_train_ds.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "finetune_val_ds = sst_val_ds.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Preview a single input example.\n",
        "print(finetune_val_ds.take(1).get_single_element())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3dRuEkiiZxo"
      },
      "source": [
        "### Fine-tune the Transformer\n",
        "\n",
        "To go from our encoded token output to a classification prediction, we need to attach\n",
        "another \"head\" to our Transformer model. We can afford to be simple here. We pool\n",
        "the encoded tokens together, and use a single dense layer to make a prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "hBocbLKmiZxo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b49ce9-9bc2-4192-e26a-3e74ae310a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<KerasTensor shape=(128, 256), dtype=float32, sparse=False, name=keras_tensor_128>\n",
            "Epoch 1/8\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11s/step - accuracy: 0.5521 - loss: 0.7235 - val_accuracy: 0.4908 - val_loss: 0.6992\n",
            "Epoch 2/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 366ms/step - accuracy: 0.4727 - loss: 0.7026 - val_accuracy: 0.5092 - val_loss: 0.7075\n",
            "Epoch 3/8\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - accuracy: 0.5625 - loss: 0.6913 - val_accuracy: 0.5092 - val_loss: 0.7404\n",
            "Epoch 4/8\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.4271 - loss: 0.7853 - val_accuracy: 0.5092 - val_loss: 0.7105\n",
            "Epoch 5/8\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step - accuracy: 0.5417 - loss: 0.6951 - val_accuracy: 0.5092 - val_loss: 0.6930\n",
            "Epoch 6/8\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 0.4896 - loss: 0.6918 - val_accuracy: 0.4908 - val_loss: 0.6942\n",
            "Epoch 7/8\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - accuracy: 0.5156 - loss: 0.6918 - val_accuracy: 0.4908 - val_loss: 0.6953\n",
            "Epoch 8/8\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 0.4583 - loss: 0.7012 - val_accuracy: 0.4908 - val_loss: 0.6945\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79fe8255e090>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# Reload the encoder model from disk so we can restart fine-tuning from scratch.\n",
        "encoder_model = keras.models.load_model(\"encoder_model.keras\", compile=False)\n",
        "\n",
        "# Take as input the tokenized input.\n",
        "inputs = keras.Input(shape=(SEQ_LENGTH,), dtype=\"int32\")\n",
        "\n",
        "# Encode and pool the tokens.\n",
        "encoded_tokens = encoder_model(inputs)\n",
        "print(encoded_tokens[0])\n",
        "pooled_tokens = keras.layers.GlobalAveragePooling1D()(encoded_tokens)\n",
        "\n",
        "# Predict an output label.\n",
        "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(pooled_tokens)\n",
        "\n",
        "# Define and compile our fine-tuning model.\n",
        "finetuning_model = keras.Model(inputs, outputs)\n",
        "finetuning_model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=keras.optimizers.AdamW(FINETUNING_LEARNING_RATE),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "# Finetune the model for the SST-2 task.\n",
        "finetuning_model.fit(\n",
        "    finetune_ds,\n",
        "    validation_data=finetune_val_ds,\n",
        "    epochs=FINETUNING_EPOCHS,\n",
        "    steps_per_epoch=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3DzIAAJiZxo"
      },
      "source": [
        "Pretraining was enough to boost our performance to 84%, and this is hardly the ceiling\n",
        "for Transformer models. You may have noticed during pretraining that our validation\n",
        "performance was still steadily increasing. Our model is still significantly undertrained.\n",
        "Training for more epochs, training a large Transformer, and training on more unlabeled\n",
        "text would all continue to boost performance significantly.\n",
        "\n",
        "One of the key goals of KerasHub is to provide a modular approach to NLP model building.\n",
        "We have shown one approach to building a Transformer here, but KerasHub supports an ever\n",
        "growing array of components for preprocessing text and building models. We hope it makes\n",
        "it easier to experiment on solutions to your natural language problems."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}