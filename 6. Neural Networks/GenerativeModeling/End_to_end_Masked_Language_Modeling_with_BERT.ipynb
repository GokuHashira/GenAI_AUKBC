{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYVnsH1uxWbr"
      },
      "source": [
        "# End-to-end Masked Language Modeling with BERT\n",
        "\n",
        "**Author:** [Ankur Singh](https://twitter.com/ankur310794)<br>\n",
        "**Date created:** 2020/09/18<br>\n",
        "**Last modified:** 2024/03/15<br>\n",
        "**Description:** Implement a Masked Language Model (MLM) with BERT and fine-tune it on the IMDB Reviews dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmvBAmlTxWbt"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Masked Language Modeling is a fill-in-the-blank task,\n",
        "where a model uses the context words surrounding a mask token to try to predict what the\n",
        "masked word should be.\n",
        "\n",
        "For an input that contains one or more mask tokens,\n",
        "the model will generate the most likely substitution for each.\n",
        "\n",
        "Example:\n",
        "\n",
        "- Input: \"I have watched this [MASK] and it was awesome.\"\n",
        "- Output: \"I have watched this movie and it was awesome.\"\n",
        "\n",
        "Masked language modeling is a great way to train a language\n",
        "model in a self-supervised setting (without human-annotated labels).\n",
        "Such a model can then be fine-tuned to accomplish various supervised\n",
        "NLP tasks.\n",
        "\n",
        "This example teaches you how to build a BERT model from scratch,\n",
        "train it with the masked language modeling task,\n",
        "and then fine-tune this model on a sentiment classification task.\n",
        "\n",
        "We will use the Keras `TextVectorization` and `MultiHeadAttention` layers\n",
        "to create a BERT Transformer-Encoder network architecture.\n",
        "\n",
        "Note: This example should be run with `tf-nightly`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBbA7JQZxWbu"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install `tf-nightly` via `pip install tf-nightly`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8ZRdQoSgxWbv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-29 20:19:06.026112: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1748530146.043300   68491 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1748530146.048652   68491 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1748530146.063059   68491 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1748530146.063086   68491 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1748530146.063088   68491 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1748530146.063089   68491 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-05-29 20:19:06.067978: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"  # or jax, or tensorflow\n",
        "\n",
        "import keras_hub\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.layers import TextVectorization\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import re\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjE0ydAFxWbv"
      },
      "source": [
        "## Set-up Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8g4mR4ADxWbw"
      },
      "outputs": [],
      "source": [
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    MAX_LEN = 256\n",
        "    BATCH_SIZE = 32\n",
        "    LR = 0.001\n",
        "    VOCAB_SIZE = 30000\n",
        "    EMBED_DIM = 128\n",
        "    NUM_HEAD = 8  # used in bert model\n",
        "    FF_DIM = 128  # used in bert model\n",
        "    NUM_LAYERS = 1\n",
        "\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buqdTy_zxWbw"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "We will first download the IMDB data and load into a Pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4ths5cIxWbx",
        "outputId": "faed3892-9ef3-48b0-9109-16da334f0da5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  2163k      0  0:00:37  0:00:37 --:--:-- 5292k\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "USPB2wZaxWbx"
      },
      "outputs": [],
      "source": [
        "def get_text_list_from_files(files):\n",
        "    text_list = []\n",
        "    for name in files:\n",
        "        with open(name) as f:\n",
        "            for line in f:\n",
        "                text_list.append(line)\n",
        "    return text_list\n",
        "\n",
        "\n",
        "def get_data_from_text_files(folder_name):\n",
        "    pos_files = glob.glob(\"aclImdb/\" + folder_name + \"/pos/*.txt\")\n",
        "    pos_texts = get_text_list_from_files(pos_files)\n",
        "    neg_files = glob.glob(\"aclImdb/\" + folder_name + \"/neg/*.txt\")\n",
        "    neg_texts = get_text_list_from_files(neg_files)\n",
        "    df = pd.DataFrame(\n",
        "        {\n",
        "            \"review\": pos_texts + neg_texts,\n",
        "            \"sentiment\": [0] * len(pos_texts) + [1] * len(neg_texts),\n",
        "        }\n",
        "    )\n",
        "    df = df.sample(len(df)).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "train_df = get_data_from_text_files(\"train\")\n",
        "test_df = get_data_from_text_files(\"test\")\n",
        "\n",
        "all_data = pd.concat([train_df, test_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62sSkYLExWbx"
      },
      "source": [
        "## Dataset preparation\n",
        "\n",
        "We will use the `TextVectorization` layer to vectorize the text into integer token ids.\n",
        "It transforms a batch of strings into either\n",
        "a sequence of token indices (one sample = 1D array of integer token indices, in order)\n",
        "or a dense representation (one sample = 1D array of float values encoding an unordered set of tokens).\n",
        "\n",
        "Below, we define 3 preprocessing functions.\n",
        "\n",
        "1.  The `get_vectorize_layer` function builds the `TextVectorization` layer.\n",
        "2.  The `encode` function encodes raw text into integer token ids.\n",
        "3.  The `get_masked_input_and_labels` function will mask input token ids.\n",
        "It masks 15% of all input tokens in each sequence at random."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QMZubKnMxWby"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-29 20:22:33.182004: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 182396128 exceeds 10% of free system memory.\n",
            "2025-05-29 20:22:33.182087: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 273594192 exceeds 10% of free system memory.\n",
            "2025-05-29 20:22:43.519923: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 173.95MiB (rounded to 182396160)requested by op StridedSlice\n",
            "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
            "Current allocation summary follows.\n",
            "Current allocation summary follows.\n",
            "2025-05-29 20:22:43.519958: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1058] BFCAllocator dump for GPU_0_bfc\n",
            "2025-05-29 20:22:43.519966: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (256): \tTotal Chunks: 2, Chunks in use: 2. 512B allocated for chunks. 512B in use in bin. 16B client-requested in use in bin.\n",
            "2025-05-29 20:22:43.519969: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2025-05-29 20:22:43.519974: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
            "2025-05-29 20:22:43.519977: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2025-05-29 20:22:43.519979: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2025-05-29 20:22:43.519982: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2025-05-29 20:22:43.519985: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2025-05-29 20:22:43.519987: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2025-05-29 20:22:43.519990: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2025-05-29 20:22:43.519992: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2025-05-29 20:22:43.519995: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2025-05-29 20:22:43.519998: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2025-05-29 20:22:43.520001: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (1048576): \tTotal Chunks: 1, Chunks in use: 0. 2.00MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2025-05-29 20:22:43.520003: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2025-05-29 20:22:43.520006: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2025-05-29 20:22:43.520008: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2025-05-29 20:22:43.520011: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2025-05-29 20:22:43.520013: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2025-05-29 20:22:43.520016: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2025-05-29 20:22:43.520021: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2025-05-29 20:22:43.520024: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
            "2025-05-29 20:22:43.520028: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1081] Bin for 173.95MiB was 128.00MiB, Chunk State: \n",
            "2025-05-29 20:22:43.520031: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1094] Next region of size 2097152\n",
            "2025-05-29 20:22:43.520038: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 771cac400000 of size 1280 next 1\n",
            "2025-05-29 20:22:43.520042: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 771cac400500 of size 256 next 2\n",
            "2025-05-29 20:22:43.520046: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 771cac400600 of size 256 next 3\n",
            "2025-05-29 20:22:43.520050: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] Free  at 771cac400700 of size 2095360 next 18446744073709551615\n",
            "2025-05-29 20:22:43.520052: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1119]      Summary of in-use Chunks by size: \n",
            "2025-05-29 20:22:43.520056: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1122] 2 Chunks of size 256 totalling 512B\n",
            "2025-05-29 20:22:43.520059: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1122] 1 Chunks of size 1280 totalling 1.2KiB\n",
            "2025-05-29 20:22:43.520062: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1126] Sum Total of in-use chunks: 1.8KiB\n",
            "2025-05-29 20:22:43.520066: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1128] Total bytes in pool: 2097152 memory_limit_: 136904704 available bytes: 134807552 curr_region_allocation_bytes_: 4194304\n",
            "2025-05-29 20:22:43.520072: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1133] Stats: \n",
            "Limit:                       136904704\n",
            "InUse:                            1792\n",
            "MaxInUse:                         1792\n",
            "NumAllocs:                           3\n",
            "MaxAllocSize:                     1280\n",
            "Reserved:                            0\n",
            "PeakReserved:                        0\n",
            "LargestFreeBlock:                    0\n",
            "\n",
            "2025-05-29 20:22:43.520076: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:512] *___________________________________________________________________________________________________\n"
          ]
        },
        {
          "ename": "InternalError",
          "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run StridedSlice: Dst tensor is not initialized. [Op:StridedSlice] name: strided_slice/",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mInternalError\u001b[39m                             Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     36\u001b[39m     vectorize_layer.set_vocabulary(vocab)\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m vectorize_layer\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m vectorize_layer = \u001b[43mget_vectorize_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreview\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVOCAB_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMAX_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m[mask]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Get mask token id for masked language model\u001b[39;00m\n\u001b[32m     48\u001b[39m mask_token_id = vectorize_layer([\u001b[33m\"\u001b[39m\u001b[33m[mask]\u001b[39m\u001b[33m\"\u001b[39m]).cpu().numpy()[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mget_vectorize_layer\u001b[39m\u001b[34m(texts, vocab_size, max_seq, special_tokens)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Build Text vectorization layer\u001b[39;00m\n\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m \u001b[33;03m    layers.Layer: Return TextVectorization Keras Layer\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     25\u001b[39m vectorize_layer = TextVectorization(\n\u001b[32m     26\u001b[39m     max_tokens=vocab_size,\n\u001b[32m     27\u001b[39m     output_mode=\u001b[33m\"\u001b[39m\u001b[33mint\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m     standardize=custom_standardization,\n\u001b[32m     29\u001b[39m     output_sequence_length=max_seq,\n\u001b[32m     30\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[43mvectorize_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Insert mask token in vocabulary\u001b[39;00m\n\u001b[32m     34\u001b[39m vocab = vectorize_layer.get_vocabulary()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/store/venv/torch/lib/python3.12/site-packages/keras/src/layers/preprocessing/text_vectorization.py:428\u001b[39m, in \u001b[36mTextVectorization.adapt\u001b[39m\u001b[34m(self, data, batch_size, steps)\u001b[39m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data.shape.rank == \u001b[32m1\u001b[39m:\n\u001b[32m    425\u001b[39m         \u001b[38;5;66;03m# A plain list of strings\u001b[39;00m\n\u001b[32m    426\u001b[39m         \u001b[38;5;66;03m# is treated as as many documents\u001b[39;00m\n\u001b[32m    427\u001b[39m         data = tf.expand_dims(data, -\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[38;5;28mself\u001b[39m.finalize_state()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/store/venv/torch/lib/python3.12/site-packages/keras/src/layers/preprocessing/text_vectorization.py:432\u001b[39m, in \u001b[36mTextVectorization.update_state\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28mself\u001b[39m._lookup_layer.update_state(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/store/venv/torch/lib/python3.12/site-packages/keras/src/layers/preprocessing/text_vectorization.py:553\u001b[39m, in \u001b[36mTextVectorization._preprocess\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    549\u001b[39m         inputs = tf.squeeze(inputs, axis=-\u001b[32m1\u001b[39m)\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._split == \u001b[33m\"\u001b[39m\u001b[33mwhitespace\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    551\u001b[39m     \u001b[38;5;66;03m# This treats multiple whitespaces as one whitespace, and strips\u001b[39;00m\n\u001b[32m    552\u001b[39m     \u001b[38;5;66;03m# leading and trailing whitespace.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m     inputs = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrings\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._split == \u001b[33m\"\u001b[39m\u001b[33mcharacter\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    555\u001b[39m     inputs = tf.strings.unicode_split(inputs, \u001b[33m\"\u001b[39m\u001b[33mUTF-8\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/store/venv/torch/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/store/venv/torch/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:6006\u001b[39m, in \u001b[36mraise_from_not_ok_status\u001b[39m\u001b[34m(e, name)\u001b[39m\n\u001b[32m   6004\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_from_not_ok_status\u001b[39m(e, name) -> NoReturn:\n\u001b[32m   6005\u001b[39m   e.message += (\u001b[33m\"\u001b[39m\u001b[33m name: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m-> \u001b[39m\u001b[32m6006\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m core._status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[31mInternalError\u001b[39m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run StridedSlice: Dst tensor is not initialized. [Op:StridedSlice] name: strided_slice/"
          ]
        }
      ],
      "source": [
        "# For data pre-processing and tf.data.Dataset\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@^_`{|}~\"), \"\"\n",
        "    )\n",
        "\n",
        "\n",
        "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
        "    \"\"\"Build Text vectorization layer\n",
        "\n",
        "    Args:\n",
        "      texts (list): List of string i.e input texts\n",
        "      vocab_size (int): vocab size\n",
        "      max_seq (int): Maximum sequence length.\n",
        "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
        "\n",
        "    Returns:\n",
        "        layers.Layer: Return TextVectorization Keras Layer\n",
        "    \"\"\"\n",
        "    vectorize_layer = TextVectorization(\n",
        "        max_tokens=vocab_size,\n",
        "        output_mode=\"int\",\n",
        "        standardize=custom_standardization,\n",
        "        output_sequence_length=max_seq,\n",
        "    )\n",
        "    vectorize_layer.adapt(texts)\n",
        "\n",
        "    # Insert mask token in vocabulary\n",
        "    vocab = vectorize_layer.get_vocabulary()\n",
        "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
        "    vectorize_layer.set_vocabulary(vocab)\n",
        "    return vectorize_layer\n",
        "\n",
        "\n",
        "vectorize_layer = get_vectorize_layer(\n",
        "    all_data.review.values.tolist(),\n",
        "    config.VOCAB_SIZE,\n",
        "    config.MAX_LEN,\n",
        "    special_tokens=[\"[mask]\"],\n",
        ")\n",
        "\n",
        "# Get mask token id for masked language model\n",
        "mask_token_id = vectorize_layer([\"[mask]\"]).cpu().numpy()[0][0]\n",
        "\n",
        "\n",
        "def encode(texts):\n",
        "    encoded_texts = vectorize_layer(texts)\n",
        "    return encoded_texts.cpu().numpy()\n",
        "\n",
        "\n",
        "def get_masked_input_and_labels(encoded_texts):\n",
        "    # 15% BERT masking\n",
        "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
        "    # Do not mask special tokens\n",
        "    inp_mask[encoded_texts <= 2] = False\n",
        "    # Set targets to -1 by default, it means ignore\n",
        "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
        "    # Set labels for masked tokens\n",
        "    labels[inp_mask] = encoded_texts[inp_mask]\n",
        "\n",
        "    # Prepare input\n",
        "    encoded_texts_masked = np.copy(encoded_texts)\n",
        "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
        "    # This means leaving 10% unchanged\n",
        "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
        "    encoded_texts_masked[inp_mask_2mask] = (\n",
        "        mask_token_id  # mask token is the last in the dict\n",
        "    )\n",
        "\n",
        "    # Set 10% to a random token\n",
        "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
        "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
        "        3, mask_token_id, inp_mask_2random.sum()\n",
        "    )\n",
        "\n",
        "    # Prepare sample_weights to pass to .fit() method\n",
        "    sample_weights = np.ones(labels.shape)\n",
        "    sample_weights[labels == -1] = 0\n",
        "\n",
        "    # y_labels would be same as encoded_texts i.e input tokens\n",
        "    y_labels = np.copy(encoded_texts)\n",
        "\n",
        "    return encoded_texts_masked, y_labels, sample_weights\n",
        "\n",
        "\n",
        "# We have 25000 examples for training\n",
        "x_train = encode(train_df.review.values)  # encode reviews with vectorizer\n",
        "y_train = train_df.sentiment.values\n",
        "train_classifier_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    .shuffle(1000)\n",
        "    .batch(config.BATCH_SIZE)\n",
        ")\n",
        "\n",
        "# We have 25000 examples for testing\n",
        "x_test = encode(test_df.review.values)\n",
        "y_test = test_df.sentiment.values\n",
        "test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(\n",
        "    config.BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Dataset for end to end model input (will be used at the end)\n",
        "test_raw_classifier_ds = test_df\n",
        "\n",
        "# Prepare data for masked language model\n",
        "x_all_review = encode(all_data.review.values)\n",
        "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
        "    x_all_review\n",
        ")\n",
        "\n",
        "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_masked_train, y_masked_labels, sample_weights)\n",
        ")\n",
        "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pc9BuVixWby"
      },
      "source": [
        "## Create BERT model (Pretraining Model) for masked language modeling\n",
        "\n",
        "We will create a BERT-like pretraining model architecture\n",
        "using the `MultiHeadAttention` layer.\n",
        "It will take token ids as inputs (including masked tokens)\n",
        "and it will predict the correct ids for the masked input tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oqJVjVJxWby"
      },
      "outputs": [],
      "source": [
        "def bert_module(query, key, value, i):\n",
        "    # Multi headed self-attention\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=config.NUM_HEAD,\n",
        "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
        "        name=\"encoder_{}_multiheadattention\".format(i),\n",
        "    )(query, key, value)\n",
        "    attention_output = layers.Dropout(0.1, name=\"encoder_{}_att_dropout\".format(i))(\n",
        "        attention_output\n",
        "    )\n",
        "    attention_output = layers.LayerNormalization(\n",
        "        epsilon=1e-6, name=\"encoder_{}_att_layernormalization\".format(i)\n",
        "    )(query + attention_output)\n",
        "\n",
        "    # Feed-forward layer\n",
        "    ffn = keras.Sequential(\n",
        "        [\n",
        "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
        "            layers.Dense(config.EMBED_DIM),\n",
        "        ],\n",
        "        name=\"encoder_{}_ffn\".format(i),\n",
        "    )\n",
        "    ffn_output = ffn(attention_output)\n",
        "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}_ffn_dropout\".format(i))(\n",
        "        ffn_output\n",
        "    )\n",
        "    sequence_output = layers.LayerNormalization(\n",
        "        epsilon=1e-6, name=\"encoder_{}_ffn_layernormalization\".format(i)\n",
        "    )(attention_output + ffn_output)\n",
        "    return sequence_output\n",
        "\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(reduction=None)\n",
        "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "\n",
        "class MaskedLanguageModel(keras.Model):\n",
        "\n",
        "    def compute_loss(self, x=None, y=None, y_pred=None, sample_weight=None):\n",
        "\n",
        "        loss = loss_fn(y, y_pred, sample_weight)\n",
        "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
        "        return keras.ops.sum(loss)\n",
        "\n",
        "    def compute_metrics(self, x, y, y_pred, sample_weight):\n",
        "\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {\"loss\": loss_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We list our `Metric` objects here so that `reset_states()` can be\n",
        "        # called automatically at the start of each epoch\n",
        "        # or at the start of `evaluate()`.\n",
        "        # If you don't implement this property, you have to call\n",
        "        # `reset_states()` yourself at the time of your choosing.\n",
        "        return [loss_tracker]\n",
        "\n",
        "\n",
        "def create_masked_language_bert_model():\n",
        "    inputs = layers.Input((config.MAX_LEN,), dtype=\"int64\")\n",
        "\n",
        "    word_embeddings = layers.Embedding(\n",
        "        config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n",
        "    )(inputs)\n",
        "    position_embeddings = keras_hub.layers.PositionEmbedding(\n",
        "        sequence_length=config.MAX_LEN\n",
        "    )(word_embeddings)\n",
        "    embeddings = word_embeddings + position_embeddings\n",
        "\n",
        "    encoder_output = embeddings\n",
        "    for i in range(config.NUM_LAYERS):\n",
        "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
        "\n",
        "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
        "        encoder_output\n",
        "    )\n",
        "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
        "    mlm_model.compile(optimizer=optimizer)\n",
        "    return mlm_model\n",
        "\n",
        "\n",
        "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
        "token2id = {y: x for x, y in id2token.items()}\n",
        "\n",
        "\n",
        "class MaskedTextGenerator(keras.callbacks.Callback):\n",
        "    def __init__(self, sample_tokens, top_k=5):\n",
        "        self.sample_tokens = sample_tokens\n",
        "        self.k = top_k\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
        "\n",
        "    def convert_ids_to_tokens(self, id):\n",
        "        return id2token[id]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        prediction = self.model.predict(self.sample_tokens)\n",
        "\n",
        "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
        "        masked_index = masked_index[1]\n",
        "        mask_prediction = prediction[0][masked_index]\n",
        "\n",
        "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
        "        values = mask_prediction[0][top_indices]\n",
        "\n",
        "        for i in range(len(top_indices)):\n",
        "            p = top_indices[i]\n",
        "            v = values[i]\n",
        "            tokens = np.copy(sample_tokens[0])\n",
        "            tokens[masked_index[0]] = p\n",
        "            result = {\n",
        "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
        "                \"prediction\": self.decode(tokens),\n",
        "                \"probability\": v,\n",
        "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
        "            }\n",
        "            pprint(result)\n",
        "\n",
        "\n",
        "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
        "generator_callback = MaskedTextGenerator(sample_tokens.cpu().numpy())\n",
        "\n",
        "bert_masked_model = create_masked_language_bert_model()\n",
        "bert_masked_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zil48jCyxWbz"
      },
      "source": [
        "## Train and Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3-ORQ73xWbz"
      },
      "outputs": [],
      "source": [
        "bert_masked_model.fit(mlm_ds, epochs=5, callbacks=[generator_callback])\n",
        "bert_masked_model.save(\"bert_mlm_imdb.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_T9dE8nxWbz"
      },
      "source": [
        "## Fine-tune a sentiment classification model\n",
        "\n",
        "We will fine-tune our self-supervised model on a downstream task of sentiment classification.\n",
        "To do this, let's create a classifier by adding a pooling layer and a `Dense` layer on top of the\n",
        "pretrained BERT features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa7ZivdxxWbz"
      },
      "outputs": [],
      "source": [
        "# Load pretrained bert model\n",
        "mlm_model = keras.models.load_model(\n",
        "    \"bert_mlm_imdb.keras\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel}\n",
        ")\n",
        "pretrained_bert_model = keras.Model(\n",
        "    mlm_model.input, mlm_model.get_layer(\"encoder_0_ffn_layernormalization\").output\n",
        ")\n",
        "\n",
        "# Freeze it\n",
        "pretrained_bert_model.trainable = False\n",
        "\n",
        "\n",
        "def create_classifier_bert_model():\n",
        "    inputs = layers.Input((config.MAX_LEN,), dtype=\"int64\")\n",
        "    sequence_output = pretrained_bert_model(inputs)\n",
        "    pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n",
        "    hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
        "    classifer_model = keras.Model(inputs, outputs, name=\"classification\")\n",
        "    optimizer = keras.optimizers.Adam()\n",
        "    classifer_model.compile(\n",
        "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return classifer_model\n",
        "\n",
        "\n",
        "classifer_model = create_classifier_bert_model()\n",
        "classifer_model.summary()\n",
        "\n",
        "# Train the classifier with frozen BERT stage\n",
        "classifer_model.fit(\n",
        "    train_classifier_ds,\n",
        "    epochs=5,\n",
        "    validation_data=test_classifier_ds,\n",
        ")\n",
        "\n",
        "# Unfreeze the BERT model for fine-tuning\n",
        "pretrained_bert_model.trainable = True\n",
        "optimizer = keras.optimizers.Adam()\n",
        "classifer_model.compile(\n",
        "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "classifer_model.fit(\n",
        "    train_classifier_ds,\n",
        "    epochs=5,\n",
        "    validation_data=test_classifier_ds,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBz2j70JxWbz"
      },
      "source": [
        "## Create an end-to-end model and evaluate it\n",
        "\n",
        "When you want to deploy a model, it's best if it already includes its preprocessing\n",
        "pipeline, so that you don't have to reimplement the preprocessing logic in your\n",
        "production environment. Let's create an end-to-end model that incorporates\n",
        "the `TextVectorization` layer inside evalaute method, and let's evaluate. We will pass raw strings as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e7FRxZ7xWbz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# We create a custom Model to override the evaluate method so\n",
        "# that it first pre-process text data\n",
        "class ModelEndtoEnd(keras.Model):\n",
        "\n",
        "    def evaluate(self, inputs):\n",
        "        features = encode(inputs.review.values)\n",
        "        labels = inputs.sentiment.values\n",
        "        test_classifier_ds = (\n",
        "            tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "            .shuffle(1000)\n",
        "            .batch(config.BATCH_SIZE)\n",
        "        )\n",
        "        return super().evaluate(test_classifier_ds)\n",
        "\n",
        "    # Build the model\n",
        "    def build(self, input_shape):\n",
        "        self.built = True\n",
        "\n",
        "\n",
        "def get_end_to_end(model):\n",
        "    inputs = classifer_model.inputs[0]\n",
        "    outputs = classifer_model.outputs\n",
        "    end_to_end_model = ModelEndtoEnd(inputs, outputs, name=\"end_to_end_model\")\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
        "    end_to_end_model.compile(\n",
        "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return end_to_end_model\n",
        "\n",
        "\n",
        "end_to_end_classification_model = get_end_to_end(classifer_model)\n",
        "# Pass raw text dataframe to the model\n",
        "end_to_end_classification_model.evaluate(test_raw_classifier_ds)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
