{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpAuMniFEqLM"
      },
      "source": [
        "# Image Caption Generator Using Cnn and LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SiK5ZIQEqLN"
      },
      "source": [
        "# What is Image Caption Generator?\n",
        "Image caption generator is a system that will predict some discription after see a image by using computer vision and deep learning. This project is required computer vision and npl operation. Cnn is used for image classification. There is a pretrain model called Xception Which is trained by imagenet dataset. Xception is resposible for image features extractions. We will use this pretrain model for extract the features from our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmmTbfYyEqLO"
      },
      "source": [
        "# Import Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuJ_ax-aEqLO"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-23T18:14:59.086904Z",
          "iopub.status.busy": "2024-01-23T18:14:59.086361Z",
          "iopub.status.idle": "2024-01-23T18:15:10.532699Z",
          "shell.execute_reply": "2024-01-23T18:15:10.531916Z",
          "shell.execute_reply.started": "2024-01-23T18:14:59.086879Z"
        },
        "id": "vq-Sx2NPEqLO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, LSTM, Embedding, Concatenate, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJZZzDbDEqLP"
      },
      "outputs": [],
      "source": [
        "# download the data from kagglehub\n",
        "import kagglehub\n",
        "\n",
        "# Alternate data sources are available!\n",
        "#coco_2017_dataset_path = kagglehub.dataset_download('awsaf49/coco-2017-dataset')\n",
        "#flickr30k_path = kagglehub.dataset_download('eeshawn/flickr30k')\n",
        "\n",
        "flickr8k_path = kagglehub.dataset_download('adityajn105/flickr8k')\n",
        "print(flickr8k_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-23T18:15:10.534619Z",
          "iopub.status.busy": "2024-01-23T18:15:10.534113Z",
          "iopub.status.idle": "2024-01-23T18:15:10.538922Z",
          "shell.execute_reply": "2024-01-23T18:15:10.537888Z",
          "shell.execute_reply.started": "2024-01-23T18:15:10.534592Z"
        },
        "id": "gLrWCSrtEqLQ"
      },
      "outputs": [],
      "source": [
        "# if you have the data locally, set the base path appropriately.\n",
        "local_flickr8k = \"/store/datasets/flickr8k\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iSVE786EqLR"
      },
      "outputs": [],
      "source": [
        "#file = '/store/datasets/flickr8k/Images/'\n",
        "BASE_PATH = flickr8k_path  # OR  local_flickr8k OR coco_2017_dataset_path OR flickr30k_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olWDBjcmEqLR"
      },
      "source": [
        "# Extract Image Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-23T18:15:10.540399Z",
          "iopub.status.busy": "2024-01-23T18:15:10.540057Z",
          "iopub.status.idle": "2024-01-23T18:15:13.739360Z",
          "shell.execute_reply": "2024-01-23T18:15:13.738386Z",
          "shell.execute_reply.started": "2024-01-23T18:15:10.540367Z"
        },
        "id": "3xmjvB-DEqLR"
      },
      "outputs": [],
      "source": [
        "modelx = Xception()\n",
        "modelx = Model(inputs=modelx.inputs, outputs=modelx.layers[-2].output)\n",
        "#modelx.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zgzu9lcEqLS"
      },
      "source": [
        "### run the following block if the pickle file doesn't exist.  TAKES LONG to RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-20T10:12:58.135635Z",
          "iopub.status.busy": "2024-01-20T10:12:58.135340Z",
          "iopub.status.idle": "2024-01-20T10:56:32.048534Z",
          "shell.execute_reply": "2024-01-20T10:56:32.047660Z",
          "shell.execute_reply.started": "2024-01-20T10:12:58.135607Z"
        },
        "id": "ocZV6-8nEqLS"
      },
      "outputs": [],
      "source": [
        "features = {}\n",
        "directory = os.path.join(BASE_PATH + \"/Images\")\n",
        "\n",
        "for img_name in tqdm(os.listdir(directory)):\n",
        "    # join the directory path and image name\n",
        "    img_path = os.path.join(directory, img_name)\n",
        "\n",
        "    # check if it's a file (not a subdirectory)\n",
        "    if os.path.isfile(img_path):\n",
        "        try:\n",
        "            # load image from file\n",
        "            image = load_img(img_path, target_size=(299, 299))\n",
        "            # convert image pixels to numpy array\n",
        "            image = img_to_array(image)\n",
        "            image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "            # preprocess image\n",
        "            image = preprocess_input(image)\n",
        "            feature = modelx.predict(image, verbose=0)\n",
        "            # get image id\n",
        "            image_id = img_name.split('.')[0]\n",
        "            # store feature\n",
        "            features[image_id] = feature\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_path}: {e}\")\n",
        "\n",
        "with open('features1.pkl','wb') as f:\n",
        "    pickle.dump(features,f)\n",
        "\n",
        "# Now 'features' should contain the extracted features for each valid image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the precomputed features file for Flick8k dataset"
      ],
      "metadata": {
        "id": "qXDpcMwYF7Be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if the features are available already, download it\n",
        "import gdown\n",
        "gdown.download(url=\"https://drive.google.com/file/d/1Xl5aS71ZP5UVi11QAOori4oT9HG5qaZy/view?usp=sharing\", fuzzy=True, output=\"features1.pkl\")"
      ],
      "metadata": {
        "id": "H2a2-GGBFi0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-23T18:15:13.742004Z",
          "iopub.status.busy": "2024-01-23T18:15:13.741026Z",
          "iopub.status.idle": "2024-01-23T18:15:17.058241Z",
          "shell.execute_reply": "2024-01-23T18:15:17.057171Z",
          "shell.execute_reply.started": "2024-01-23T18:15:13.741967Z"
        },
        "id": "v-hMaz8cEqLT"
      },
      "outputs": [],
      "source": [
        "# load features\n",
        "with open('features1.pkl','rb') as f:\n",
        "    features = pickle.load(f)\n",
        "\n",
        "print(f\"loaded {len(features)} data points..\")\n",
        "print(f\"embedding size:\", features[next(iter(features))].shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gze8Ue_UEqLT"
      },
      "source": [
        "# Perform Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-23T18:15:21.172353Z",
          "iopub.status.busy": "2024-01-23T18:15:21.171993Z",
          "iopub.status.idle": "2024-01-23T18:15:21.480226Z",
          "shell.execute_reply": "2024-01-23T18:15:21.479445Z",
          "shell.execute_reply.started": "2024-01-23T18:15:21.172324Z"
        },
        "id": "TVx6l0LxEqLT"
      },
      "outputs": [],
      "source": [
        "with open(BASE_PATH + '/captions.txt','r') as f:\n",
        "    next(f)\n",
        "    captions_doc = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdX0PZpnEqLT"
      },
      "source": [
        "# Cleaning and mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-23T18:15:21.920564Z",
          "iopub.status.busy": "2024-01-23T18:15:21.920240Z",
          "iopub.status.idle": "2024-01-23T18:15:22.571642Z",
          "shell.execute_reply": "2024-01-23T18:15:22.570726Z",
          "shell.execute_reply.started": "2024-01-23T18:15:21.920538Z"
        },
        "id": "M-gdKVDeEqLT"
      },
      "outputs": [],
      "source": [
        "# create mapping of image to captions\n",
        "mapping = {}\n",
        "# process lines\n",
        "for line in tqdm(captions_doc.split('\\n')):\n",
        "    # split the line by comma(,)\n",
        "    tokens = line.split(',')\n",
        "    if len(line) < 2:\n",
        "        continue\n",
        "    image_id, caption = tokens[0], tokens[1:]\n",
        "    # remove extension from image ID\n",
        "    image_id = image_id.split('.')[0]\n",
        "    # convert caption list to string\n",
        "    caption = \" \".join(caption)\n",
        "    # create list if needed\n",
        "    if image_id not in mapping:\n",
        "        mapping[image_id] = []\n",
        "    # store the caption\n",
        "    mapping[image_id].append(caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-23T18:15:22.573552Z",
          "iopub.status.busy": "2024-01-23T18:15:22.573253Z",
          "iopub.status.idle": "2024-01-23T18:15:22.579896Z",
          "shell.execute_reply": "2024-01-23T18:15:22.578991Z",
          "shell.execute_reply.started": "2024-01-23T18:15:22.573527Z"
        },
        "id": "EaRHvT5jEqLU"
      },
      "outputs": [],
      "source": [
        "len(mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s6oLyVPEqLU"
      },
      "source": [
        "# Preprocess Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-23T18:15:22.581790Z",
          "iopub.status.busy": "2024-01-23T18:15:22.581162Z",
          "iopub.status.idle": "2024-01-23T18:15:22.589001Z",
          "shell.execute_reply": "2024-01-23T18:15:22.588090Z",
          "shell.execute_reply.started": "2024-01-23T18:15:22.581757Z"
        },
        "id": "tP1SQF2KEqLU"
      },
      "outputs": [],
      "source": [
        "def clean(mapping):\n",
        "    for key, captions in mapping.items():\n",
        "        for i in range(len(captions)):\n",
        "            # take one caption at a time\n",
        "            caption = captions[i]\n",
        "            # preprocessing steps\n",
        "            # convert to lowercase\n",
        "            caption = caption.lower()\n",
        "            # delete digits, special chars, etc.,\n",
        "            caption = caption.replace('[^A-Za-z]', '')\n",
        "            # delete additional spaces\n",
        "            caption = caption.replace('\\s+', ' ')\n",
        "            # add start and end tags to the caption\n",
        "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n",
        "            captions[i] = caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-23T18:15:22.781906Z",
          "iopub.status.busy": "2024-01-23T18:15:22.781557Z",
          "iopub.status.idle": "2024-01-23T18:15:23.466462Z",
          "shell.execute_reply": "2024-01-23T18:15:23.465724Z",
          "shell.execute_reply.started": "2024-01-23T18:15:22.781881Z"
        },
        "id": "neSDkAJnEqLV"
      },
      "outputs": [],
      "source": [
        "# process text\n",
        "clean(mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-23T18:15:23.468148Z",
          "iopub.status.busy": "2024-01-23T18:15:23.467897Z",
          "iopub.status.idle": "2024-01-23T18:15:23.507342Z",
          "shell.execute_reply": "2024-01-23T18:15:23.506434Z",
          "shell.execute_reply.started": "2024-01-23T18:15:23.468126Z"
        },
        "id": "M_zUtBhMEqLV"
      },
      "outputs": [],
      "source": [
        "all_captions = []\n",
        "for key in mapping:\n",
        "    for caption in mapping[key]:\n",
        "        all_captions.append(caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-23T18:15:23.508779Z",
          "iopub.status.busy": "2024-01-23T18:15:23.508481Z",
          "iopub.status.idle": "2024-01-23T18:15:23.519439Z",
          "shell.execute_reply": "2024-01-23T18:15:23.518363Z",
          "shell.execute_reply.started": "2024-01-23T18:15:23.508755Z"
        },
        "id": "ZdlB4CfvEqLV"
      },
      "outputs": [],
      "source": [
        "len(all_captions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Tmi304MNIpY4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-23T18:15:23.522011Z",
          "iopub.status.busy": "2024-01-23T18:15:23.521454Z",
          "iopub.status.idle": "2024-01-23T18:15:23.528953Z",
          "shell.execute_reply": "2024-01-23T18:15:23.528143Z",
          "shell.execute_reply.started": "2024-01-23T18:15:23.521984Z"
        },
        "id": "um9bKF0pEqLV"
      },
      "outputs": [],
      "source": [
        "all_captions[33:56]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load some images with captions\n",
        "\n",
        "Here we need to map the images in the training set to their corresponding descriptions which are present in our descriptions variable. Create a list of names of all training images and then create an empty dictionary and map the images to their descriptions using image name as key and a list of descriptions as its value. while mapping the descriptions add unique words at the beginning and end to identify the start and end of the sentence."
      ],
      "metadata": {
        "id": "OkZ04tI0IqlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(mapping.keys())[:5]"
      ],
      "metadata": {
        "id": "eBCVJgbwJlxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualization(data, num_of_images):\n",
        "    count = 1\n",
        "    fig = plt.figure(figsize=(10,20))\n",
        "    for filename in list(mapping.keys()): #captions_dictionary\n",
        "        captions = mapping[filename]\n",
        "        image_load = load_img(BASE_PATH + \"/Images/\" + filename + \".jpg\", target_size=(199,199,3))\n",
        "\n",
        "        ax = fig.add_subplot(num_of_images,2,count,xticks=[],yticks=[])\n",
        "        ax.imshow(image_load)\n",
        "        count += 1\n",
        "\n",
        "        ax = fig.add_subplot(num_of_images,2,count)\n",
        "        plt.axis('off')\n",
        "        ax.plot()\n",
        "        ax.set_xlim(0,1)\n",
        "        ax.set_ylim(0,len(captions))\n",
        "        for i, caption in enumerate(captions):\n",
        "            ax.text(0,i,caption,fontsize=14)\n",
        "        count += 1\n",
        "        if count > num_of_images:\n",
        "          break\n",
        "    plt.show()\n",
        "\n",
        "visualization(list(mapping.keys()), 5)"
      ],
      "metadata": {
        "id": "N05AoSx3Ilyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the Caption Length Distribution\n",
        "\n",
        "We analyze the length of captions to determine an optimal sequence length."
      ],
      "metadata": {
        "id": "XzKV4xPzJvp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def captions_length(data):\n",
        "    plt.figure(figsize=(15, 7), dpi=100)\n",
        "    sns.set_style('darkgrid')\n",
        "    sns.histplot(x=[len(x.split(' ')) for x in data], kde=True, binwidth=1)\n",
        "    plt.title('Caption lengths histogram', fontsize=15, fontweight='bold')\n",
        "    plt.xticks(fontweight='bold')\n",
        "    plt.yticks(fontweight='bold')\n",
        "    plt.xlabel('Length', fontweight='bold')\n",
        "    plt.ylabel('Frequency', fontweight='bold')\n",
        "    plt.show()\n",
        "\n",
        "captions_length(all_captions)"
      ],
      "metadata": {
        "id": "dCtoYx7uJuu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l78Pf_gbEqLW"
      },
      "source": [
        "# Tokenize the content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-23T18:15:24.351365Z",
          "iopub.status.busy": "2024-01-23T18:15:24.350840Z",
          "iopub.status.idle": "2024-01-23T18:15:27.329724Z",
          "shell.execute_reply": "2024-01-23T18:15:27.328709Z",
          "shell.execute_reply.started": "2024-01-23T18:15:24.351339Z"
        },
        "id": "YL8Vr-jNEqLW"
      },
      "outputs": [],
      "source": [
        "# tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-23T18:15:27.331787Z",
          "iopub.status.busy": "2024-01-23T18:15:27.331449Z",
          "iopub.status.idle": "2024-01-23T18:15:27.349918Z",
          "shell.execute_reply": "2024-01-23T18:15:27.349188Z",
          "shell.execute_reply.started": "2024-01-23T18:15:27.331761Z"
        },
        "id": "rpx0Y75SEqLW"
      },
      "outputs": [],
      "source": [
        "with open('tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-23T18:15:27.351203Z",
          "iopub.status.busy": "2024-01-23T18:15:27.350934Z",
          "iopub.status.idle": "2024-01-23T18:15:27.356754Z",
          "shell.execute_reply": "2024-01-23T18:15:27.355859Z",
          "shell.execute_reply.started": "2024-01-23T18:15:27.351180Z"
        },
        "id": "YPNAjYpMEqLW"
      },
      "outputs": [],
      "source": [
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-23T18:15:27.358769Z",
          "iopub.status.busy": "2024-01-23T18:15:27.358416Z",
          "iopub.status.idle": "2024-01-23T18:15:27.497833Z",
          "shell.execute_reply": "2024-01-23T18:15:27.496994Z",
          "shell.execute_reply.started": "2024-01-23T18:15:27.358737Z"
        },
        "id": "kBHbXr6QEqLX"
      },
      "outputs": [],
      "source": [
        "# get maximum len of the captions available\n",
        "max_length = max(len(caption.split()) for caption in all_captions)\n",
        "max_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg3S3HtTEqLX"
      },
      "source": [
        "# Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-23T18:15:27.499608Z",
          "iopub.status.busy": "2024-01-23T18:15:27.498962Z",
          "iopub.status.idle": "2024-01-23T18:15:27.505415Z",
          "shell.execute_reply": "2024-01-23T18:15:27.504532Z",
          "shell.execute_reply.started": "2024-01-23T18:15:27.499577Z"
        },
        "id": "rOy8l3yHEqLX"
      },
      "outputs": [],
      "source": [
        "image_ids = list(mapping.keys())\n",
        "n_data = len(image_ids)\n",
        "split = int(len(image_ids) * 0.90)\n",
        "# only 2% of the data used for validation.\n",
        "v_split = split + int((n_data - split)/5)\n",
        "\n",
        "train = image_ids[:split]\n",
        "val = image_ids[split:v_split]\n",
        "test = image_ids[v_split:]\n",
        "\n",
        "print(f\"training: {len(train)}, val: {len(val)}, test: {len(test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVg__AvMEqLX"
      },
      "source": [
        "# Create Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-23T18:15:27.560062Z",
          "iopub.status.busy": "2024-01-23T18:15:27.559822Z",
          "iopub.status.idle": "2024-01-23T18:15:27.569766Z",
          "shell.execute_reply": "2024-01-23T18:15:27.568816Z",
          "shell.execute_reply.started": "2024-01-23T18:15:27.560041Z"
        },
        "id": "RLl26L6uEqLX"
      },
      "outputs": [],
      "source": [
        "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
        "    # Loop over images\n",
        "    x1, x2, y = list(), list(), list()\n",
        "    n = 0\n",
        "    while 1:\n",
        "        for key in data_keys:\n",
        "            n += 1\n",
        "            captions = mapping[key]\n",
        "            # process each caption\n",
        "            for caption in captions:\n",
        "                # encode the sequence\n",
        "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "                # Spllt the squences into x,y pairs\n",
        "                for i in range(1, len(seq)):\n",
        "                    # split into input and output pairs\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    # pad input sequence\n",
        "                    in_seq  = pad_sequences([in_seq], maxlen=max_length, padding='post', truncating='post')[0]\n",
        "                    # encode output sequence\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\n",
        "                    # store the sequeces\n",
        "                    x1.append(features[key][0])\n",
        "                    x2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "\n",
        "                if n == batch_size:\n",
        "                    x1,x2,y = np.array(x1), np.array(x2), np.array(y)\n",
        "                    yield (x1,x2), y\n",
        "                    x1, x2, y = list(), list(), list()\n",
        "                    n = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FT6GzINEqLY"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-23T18:15:48.012969Z",
          "iopub.status.busy": "2024-01-23T18:15:48.012091Z",
          "iopub.status.idle": "2024-01-23T18:15:49.971132Z",
          "shell.execute_reply": "2024-01-23T18:15:49.970316Z",
          "shell.execute_reply.started": "2024-01-23T18:15:48.012934Z"
        },
        "id": "Z9nzF6zxEqLY"
      },
      "outputs": [],
      "source": [
        "# Assuming you have defined vocab_size and max_length\n",
        "\n",
        "# Encoder model\n",
        "inputs1 = Input(shape=(2048,), name=\"image_input\")\n",
        "fe1 = BatchNormalization(name=\"image_batch_norm\")(inputs1)\n",
        "fe2 = Dense(512, activation='relu',name=\"image_mlp_layer\")(fe1)\n",
        "\n",
        "inputs2 = Input(shape=(max_length,), name=\"text_input\")\n",
        "se1 = Embedding(vocab_size, 512, mask_zero=True, name='text_embedding')(inputs2)\n",
        "se2 = BatchNormalization(name=\"text_batch_norm\")(se1)\n",
        "se3 = Bidirectional(LSTM(256), name=\"text_bi_lstm\")(se2)\n",
        "\n",
        "# Decoder\n",
        "decoder = Concatenate(name='conc_image_text')([fe2, se3])\n",
        "decoder2 = Dense(512, activation='relu', name=\"decoder\")(decoder)\n",
        "outputs = Dense(vocab_size, activation='softmax', name=\"output\")(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "optimizer = Adam(learning_rate=0.0001, clipvalue=5.0)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuFlav6TEqLY"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-23T18:15:58.000557Z",
          "iopub.status.busy": "2024-01-23T18:15:57.999845Z"
        },
        "id": "EMYuY1krEqLZ"
      },
      "outputs": [],
      "source": [
        "epochs = 5 # 10 works well\n",
        "batch_size = 128\n",
        "steps_per_epoch = len(train) // batch_size\n",
        "\n",
        "# Define a ModelCheckpoint callback\n",
        "checkpoint_filepath = 'model_checkpoint.keras'\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n",
        "#val_generator = data_generator(val, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n",
        "\n",
        "# Add the ModelCheckpoint callback to the list of callbacks\n",
        "history = model.fit(generator, epochs=epochs, verbose=1,\n",
        "                    steps_per_epoch=steps_per_epoch,)\n",
        "                    #validation_data = val_generator,\n",
        "                    #callbacks=[model_checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-21T17:12:28.236916Z",
          "iopub.status.idle": "2024-01-21T17:12:28.237315Z",
          "shell.execute_reply": "2024-01-21T17:12:28.237142Z",
          "shell.execute_reply.started": "2024-01-21T17:12:28.237124Z"
        },
        "id": "dq5M7YraEqLZ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-21T13:33:18.243181Z",
          "iopub.status.busy": "2024-01-21T13:33:18.242804Z",
          "iopub.status.idle": "2024-01-21T13:33:18.705413Z",
          "shell.execute_reply": "2024-01-21T13:33:18.704600Z",
          "shell.execute_reply.started": "2024-01-21T13:33:18.243150Z"
        },
        "id": "bVOsyeDmEqLZ"
      },
      "outputs": [],
      "source": [
        "model.save('caption-generator-model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdoqUr7bEqLf"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL0QkkJ_EqLf"
      },
      "source": [
        "# Generate Captions For Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15E-CH1WEqLf"
      },
      "outputs": [],
      "source": [
        "def idx_to_word(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "def predict_caption(model, image, tokenizer, max_length):\n",
        "    # add start tag for generation process\n",
        "    in_text = 'startseq'\n",
        "    # iterate over the max length of sequence\n",
        "    for i in range(max_length):\n",
        "        # encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # pad the sequence\n",
        "        sequence = pad_sequences([sequence], max_length, padding='post', truncating='post')\n",
        "        # predict next word\n",
        "        yhat = model.predict([image, sequence], verbose=0)\n",
        "        # get index with high probability\n",
        "        yhat = np.argmax(yhat)\n",
        "        # convert index to word\n",
        "        word = idx_to_word(yhat, tokenizer)\n",
        "        # stop if word not found\n",
        "        if word is None:\n",
        "            break\n",
        "        # append word as input for generating next word\n",
        "        in_text += \" \" + word\n",
        "        # stop if we reach end tag\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "\n",
        "    return in_text\n",
        "\n",
        "def beam_search_generator(model, image, tokenizer, max_caption_length, K_beams = 3, log = False):\n",
        "    start = [tokenizer.word_index['startseq']]\n",
        "    start_word = [[start, 0.0]]\n",
        "    for _ in range(max_caption_length):\n",
        "        temp = []\n",
        "        for s in start_word:\n",
        "            sequence  = pad_sequences([s[0]], maxlen=max_caption_length, padding='post', truncating='post').reshape((1,max_caption_length))\n",
        "            preds = model.predict([image, sequence], verbose=0)\n",
        "            word_preds = np.argsort(preds[0])[-K_beams:]\n",
        "            for w in word_preds:\n",
        "                next_cap, prob = s[0][:], s[1]\n",
        "                next_cap.append(w)\n",
        "                if log:\n",
        "                    prob += np.log(preds[0][w]) # assign a probability to each K words\n",
        "                else:\n",
        "                    prob += preds[0][w]\n",
        "                temp.append([next_cap, prob])\n",
        "\n",
        "        start_word = temp\n",
        "        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
        "        start_word = start_word[-K_beams:]\n",
        "\n",
        "    start_word = start_word[-1][0]\n",
        "    captions_ = [tokenizer.index_word[i] for i in start_word]\n",
        "    final_caption = []\n",
        "    for i in captions_:\n",
        "        if i != 'endseq':\n",
        "            final_caption.append(i)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    final_caption = ' '.join(final_caption[1:])\n",
        "    return final_caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-21T13:44:14.944327Z",
          "iopub.status.busy": "2024-01-21T13:44:14.943543Z",
          "iopub.status.idle": "2024-01-21T13:44:15.793991Z",
          "shell.execute_reply": "2024-01-21T13:44:15.792692Z",
          "shell.execute_reply.started": "2024-01-21T13:44:14.944293Z"
        },
        "id": "BOq5oyedEqLg"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "# validation with test data\n",
        "actual, predicted_greedy, predicted_beam = list(), list(), list()\n",
        "\n",
        "for key in tqdm(test[:10]):\n",
        "    captions = mapping[key]\n",
        "    # split into words\n",
        "    actual_captions = [caption.split() for caption in captions]\n",
        "    # append to the list\n",
        "    actual.append(actual_captions)\n",
        "    # predict the caption data\n",
        "    y_pred_beam = beam_search_generator(model, features[key], tokenizer, max_length)\n",
        "    y_pred_greedy = predict_caption(model, features[key], tokenizer, max_length)\n",
        "    predicted_greedy.append(y_pred_greedy.split())\n",
        "    predicted_beam.append(y_pred_beam.split())\n",
        "\n",
        "# calculate Bleu Scores\n",
        "print('Bleu-1 (Greedy): %f' % corpus_bleu(actual, predicted_greedy, weights=(1.0,0,0,0)))\n",
        "print('Bleu-2 (Greedy): %f' % corpus_bleu(actual, predicted_greedy, weights=(0.5,0.5,0,0)))\n",
        "print('Bleu-1 (Beam 3): %f' % corpus_bleu(actual, predicted_beam, weights=(1.0,0,0,0)))\n",
        "print('Bleu-2 (Beam 3): %f' % corpus_bleu(actual, predicted_beam, weights=(0.5,0.5,0,0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIvdDK2LEqLg"
      },
      "source": [
        "# Generate Captions For Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-21T13:41:01.343392Z",
          "iopub.status.busy": "2024-01-21T13:41:01.342782Z",
          "iopub.status.idle": "2024-01-21T13:41:01.348062Z",
          "shell.execute_reply": "2024-01-21T13:41:01.347119Z",
          "shell.execute_reply.started": "2024-01-21T13:41:01.343360Z"
        },
        "id": "Vv4teLP1EqLg"
      },
      "outputs": [],
      "source": [
        "def idx_to_word(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-21T13:41:36.984132Z",
          "iopub.status.busy": "2024-01-21T13:41:36.983764Z",
          "iopub.status.idle": "2024-01-21T13:41:36.991263Z",
          "shell.execute_reply": "2024-01-21T13:41:36.990279Z",
          "shell.execute_reply.started": "2024-01-21T13:41:36.984104Z"
        },
        "id": "OIqYn6XmEqLg"
      },
      "outputs": [],
      "source": [
        "def predict_caption(model, image, tokenizer, max_length):\n",
        "    # add start tag for generation process\n",
        "    in_text = 'startseq'\n",
        "    # iterate over the max length of sequence\n",
        "    for i in range(max_length):\n",
        "        # encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # pad the sequence\n",
        "        sequence = pad_sequences([sequence], max_length, padding='post', truncating='post')\n",
        "        # predict next word\n",
        "        yhat = model.predict([image, sequence], verbose=0)\n",
        "        # get index with high probability\n",
        "        yhat = np.argmax(yhat)\n",
        "        # convert index to word\n",
        "        word = idx_to_word(yhat, tokenizer)\n",
        "        # stop if word not found\n",
        "        if word is None:\n",
        "            break\n",
        "        # append word as input for generating next word\n",
        "        in_text += \" \" + word\n",
        "        # stop if we reach end tag\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "\n",
        "    return in_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY3lkYoWEqLg"
      },
      "source": [
        "# Visualize the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nK2q0sQMEqLh"
      },
      "outputs": [],
      "source": [
        "# check if it's a file (not a subdirectory)\n",
        "def generate_features(img_path):\n",
        "    if os.path.isfile(img_path):\n",
        "        try:\n",
        "            # load image from file\n",
        "            image = load_img(img_path, target_size=(299, 299))\n",
        "            # convert image pixels to numpy array\n",
        "            image = img_to_array(image)\n",
        "            image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "            # preprocess image\n",
        "            image = preprocess_input(image)\n",
        "            feature = modelx.predict(image, verbose=0)\n",
        "            # store feature\n",
        "            return feature\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_path}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [],
        "execution": {
          "iopub.execute_input": "2024-01-21T13:42:19.760778Z",
          "iopub.status.busy": "2024-01-21T13:42:19.760059Z",
          "iopub.status.idle": "2024-01-21T13:42:19.767014Z",
          "shell.execute_reply": "2024-01-21T13:42:19.766007Z",
          "shell.execute_reply.started": "2024-01-21T13:42:19.760737Z"
        },
        "id": "LcCdzPitEqLh"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "def generate_caption(image_name, skip_true=False):\n",
        "    # load the image\n",
        "    image_id = image_name.split('.')[0]\n",
        "    img_path = f\"{BASE_PATH}/Images/{image_name}\"\n",
        "    image = Image.open(img_path)\n",
        "\n",
        "    if skip_true == False:\n",
        "        captions = mapping[image_id]\n",
        "        print('---------------------Actual---------------------')\n",
        "        for caption in captions:\n",
        "            print(caption)\n",
        "        feature = features[image_id]\n",
        "    else:\n",
        "        feature = generate_features(image_name)\n",
        "\n",
        "    # predict the caption\n",
        "    y_pred = predict_caption(model, feature, tokenizer, max_length)\n",
        "    print('--------------------Predicted--------------------')\n",
        "    print(y_pred)\n",
        "    plt.imshow(image)\n",
        "\n",
        "def generate_caption_2(image_path):\n",
        "    image = Image.open(image_path)\n",
        "    feature = generate_features(image_path)\n",
        "    # predict the caption\n",
        "    y_pred = predict_caption(model, feature, tokenizer, max_length)\n",
        "    print('--------------------Predicted--------------------')\n",
        "    print(y_pred)\n",
        "    plt.imshow(image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agGvId4UEqLh"
      },
      "outputs": [],
      "source": [
        "np.random.shuffle(test)\n",
        "test[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UIlXgsYEqLh"
      },
      "outputs": [],
      "source": [
        "generate_caption('503090187_8758ab5680.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSj56FHpEqLi"
      },
      "outputs": [],
      "source": [
        "generate_caption_2('/tmp/test.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ref: https://www.geeksforgeeks.org/image-caption-generator-using-deep-learning-on-flickr8k-dataset/"
      ],
      "metadata": {
        "id": "b1dIPDaIfgue"
      }
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 2808179,
          "sourceId": 4845244,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4333473,
          "sourceId": 7445101,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30637,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}